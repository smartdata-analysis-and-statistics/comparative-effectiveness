[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Comparative Effectiveness and Personalized Medicine Research Using Real-World Data",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "chapter_06.html",
    "href": "chapter_06.html",
    "title": "1  Confounding adjustment using propensity score methods",
    "section": "",
    "text": "2 Goal\nThe purpose of this document is to provide example R code that demonstrates how to estimate the propensity score and implement matching, stratification, weighting, and regression adjustment for the continuous propensity score. In this example using simulated data, we have two disease modifying therapies (DMT1 and DMT0) and the outcome is the number of post-treatment multiple sclerosis relapses during follow-up. We will estimate the average treatment effect in the treated (ATT) using propensity score matching, stratification, and weighting. We will estimate the average treatment effect in the population (ATE) using regression adjustment for the continuous propensity score. The treatment effects can be interpreted as annualized relapse rate ratios (ARR).\nWe consider an example dataset with the following characteristics:\nIn this approach, a regression model is fitted to describe the observed outcome as a function of the received treatment and the estimated propensity score:\nBootstrapped confidence intervals can be obtained as follows:"
  },
  {
    "objectID": "chapter_06.html#logistic-regression",
    "href": "chapter_06.html#logistic-regression",
    "title": "1  Confounding adjustment using propensity score methods",
    "section": "4.1 Logistic regression",
    "text": "4.1 Logistic regression\nWe sought to restore balance in the distribution of baseline covariates in patients treated with DMT1 (index treatment) and DMT0 (control tratment). We fit a multivariable logistic regression model in which treatment was regressed on baseline characteristics including age, sex, previous DMT efficacy, and previous number of relapses.\n\n# Fit logistic regression model\nps.model <- glm(treatment ~ age + female + prevDMTefficacy + prerelapse_num, \n                data = dat, family = binomial())\n\n# Summary of logistic regression model\nsummary(ps.model)\n\n\nCall:\nglm(formula = treatment ~ age + female + prevDMTefficacy + prerelapse_num, \n    family = binomial(), data = dat)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.7949   0.2585   0.5220   0.7478   1.5033  \n\nCoefficients:\n                                     Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                          4.809473   0.157127  30.609  < 2e-16 ***\nage                                 -0.086708   0.002996 -28.939  < 2e-16 ***\nfemale1                              0.253611   0.057664   4.398 1.09e-05 ***\nprevDMTefficacyLow_efficacy          0.310394   0.083022   3.739 0.000185 ***\nprevDMTefficacyMedium_high_efficacy  0.660266   0.054393  12.139  < 2e-16 ***\nprerelapse_num                       0.156318   0.039288   3.979 6.93e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 10786  on 9999  degrees of freedom\nResidual deviance:  9597  on 9994  degrees of freedom\nAIC: 9609\n\nNumber of Fisher Scoring iterations: 5\n\n# Extract propensity scores\ndat$ps <- predict(ps.model, data = dat, type = \"response\")"
  },
  {
    "objectID": "chapter_06.html#assessing-overlap",
    "href": "chapter_06.html#assessing-overlap",
    "title": "1  Confounding adjustment using propensity score methods",
    "section": "4.2 Assessing overlap",
    "text": "4.2 Assessing overlap\nWe examined the degree of overlap in the distribution of propensity scores across treatment groups using histograms and side-by-side box plots.\n\n# Histogram\nggplot(dat, aes(x = ps, fill = as.factor(treatment), color = as.factor(treatment))) + \n  geom_histogram(alpha = 0.3, position='identity', bins = 15) + \n  facet_grid(as.factor(treatment) ~ .) + \n  xlab(\"Probability of Treatment\") + \n  ylab(\"Count\") +\n  ggtitle(\"Propensity Score Distribution by Treatment Group\") +\n  theme(legend.position = \"bottom\", legend.direction = \"vertical\")\n\n\n\n# Side-by-side box plots\nggplot(dat, aes(x=as.factor(treatment), y=ps, fill=as.factor(treatment))) +\n  geom_boxplot() + \n  ggtitle(\"Propensity Score Distribution by Treatment Group\") +\n  ylab(\"Probability of Treatment\") + \n  xlab(\"Treatment group\") +\n  theme(legend.position = \"none\")\n\n\n\n# Distribution of propensity scores by treatment groups\nsummary(dat$ps[dat$treatment == \"DMT1\"])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.3230  0.7214  0.8265  0.7970  0.9010  0.9854 \n\nsummary(dat$ps[dat$treatment == \"DMT0\"])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.3230  0.5730  0.6894  0.6795  0.7975  0.9799"
  },
  {
    "objectID": "chapter_06.html#optimal-full-matching-without-replacement",
    "href": "chapter_06.html#optimal-full-matching-without-replacement",
    "title": "1  Confounding adjustment using propensity score methods",
    "section": "5.1 1:1 Optimal full matching without replacement",
    "text": "5.1 1:1 Optimal full matching without replacement\n\nlibrary(MatchIt)\n\n# Use MatchIt package for PS matching\nopt <- matchit(treatment ~ age + female + prevDMTefficacy + prerelapse_num, \n               data = dat, \n               method = \"full\",\n               estimand = \"ATT\")\n\nopt\n\nA matchit object\n - method: Optimal full matching\n - distance: Propensity score\n             - estimated with logistic regression\n - number of obs.: 10000 (original), 10000 (matched)\n - target estimand: ATT\n - covariates: age, female, prevDMTefficacy, prerelapse_num"
  },
  {
    "objectID": "chapter_06.html#assess-balance-after-matching",
    "href": "chapter_06.html#assess-balance-after-matching",
    "title": "1  Confounding adjustment using propensity score methods",
    "section": "5.2 Assess balance after matching",
    "text": "5.2 Assess balance after matching\n\nsummary(opt)\n\n\nCall:\nmatchit(formula = treatment ~ age + female + prevDMTefficacy + \n    prerelapse_num, data = dat, method = \"full\", estimand = \"ATT\")\n\nSummary of Balance for All Data:\n                                    Means Treated Means Control Std. Mean Diff.\ndistance                                   0.7970        0.6795          0.8943\nage                                       44.2496       51.3883         -0.7289\nfemale0                                    0.2318        0.2735         -0.0987\nfemale1                                    0.7682        0.7265          0.0987\nprevDMTefficacyNone                        0.4118        0.5422         -0.2649\nprevDMTefficacyLow_efficacy                0.1114        0.1135         -0.0065\nprevDMTefficacyMedium_high_efficacy        0.4768        0.3443          0.2651\nprerelapse_num                             0.4595        0.3930          0.0976\n                                    Var. Ratio eCDF Mean eCDF Max\ndistance                                0.7873    0.1917   0.3379\nage                                     1.3868    0.1519   0.3085\nfemale0                                      .    0.0417   0.0417\nfemale1                                      .    0.0417   0.0417\nprevDMTefficacyNone                          .    0.1304   0.1304\nprevDMTefficacyLow_efficacy                  .    0.0020   0.0020\nprevDMTefficacyMedium_high_efficacy          .    0.1324   0.1324\nprerelapse_num                          1.1990    0.0133   0.0383\n\nSummary of Balance for Matched Data:\n                                    Means Treated Means Control Std. Mean Diff.\ndistance                                   0.7970        0.7970          0.0003\nage                                       44.2496       44.3185         -0.0070\nfemale0                                    0.2318        0.2275          0.0101\nfemale1                                    0.7682        0.7725         -0.0101\nprevDMTefficacyNone                        0.4118        0.4130         -0.0024\nprevDMTefficacyLow_efficacy                0.1114        0.0893          0.0703\nprevDMTefficacyMedium_high_efficacy        0.4768        0.4977         -0.0419\nprerelapse_num                             0.4595        0.4399          0.0288\n                                    Var. Ratio eCDF Mean eCDF Max\ndistance                                0.9976    0.0005   0.0075\nage                                     1.0392    0.0038   0.0153\nfemale0                                      .    0.0043   0.0043\nfemale1                                      .    0.0043   0.0043\nprevDMTefficacyNone                          .    0.0012   0.0012\nprevDMTefficacyLow_efficacy                  .    0.0221   0.0221\nprevDMTefficacyMedium_high_efficacy          .    0.0209   0.0209\nprerelapse_num                          1.1319    0.0060   0.0229\n                                    Std. Pair Dist.\ndistance                                     0.0008\nage                                          0.0667\nfemale0                                      0.1775\nfemale1                                      0.1775\nprevDMTefficacyNone                          0.1100\nprevDMTefficacyLow_efficacy                  0.1846\nprevDMTefficacyMedium_high_efficacy          0.1614\nprerelapse_num                               0.2170\n\nSample Sizes:\n              Control Treated\nAll           2300.      7700\nMatched (ESS)  307.06    7700\nMatched       2300.      7700\nUnmatched        0.         0\nDiscarded        0.         0\n\nplot(summary(opt))\n\n\n\n# black line is treated group, grey line is control group\nplot(opt, type = \"density\", which.xs = vars)"
  },
  {
    "objectID": "chapter_06.html#estimating-the-att",
    "href": "chapter_06.html#estimating-the-att",
    "title": "1  Confounding adjustment using propensity score methods",
    "section": "5.3 Estimating the ATT",
    "text": "5.3 Estimating the ATT\nWe can estimate the ATT in the matched sample using Poisson regression in which the number of post-treatment relapses is regressed on treatment status and follow-up time for each patient (captured by the variable years). More details are provided at .\n\n# Matched data\nmatched.data <- match.data(opt)\n\n# Poisson regression model\nopt.fit <- glm(y ~ treatment + offset(log(years)), \n            family = poisson(link = \"log\"),\n            data = matched.data, \n            weights = weights)\n\n# Treatment effect estimation\nopt.comp <- comparisons(opt.fit,\n                        variables = \"treatment\",\n                        vcov = ~subclass,\n                        newdata = subset(matched.data, treatment == \"DMT1\"),\n                        wts = \"weights\",\n                        transform_pre = \"ratio\")\n\nopt.comp |> tidy()\n\n# A tibble: 1 x 9\n  type     term      contrast   estim~1 std.e~2 stati~3  p.value conf.~4 conf.~5\n  <chr>    <chr>     <chr>        <dbl>   <dbl>   <dbl>    <dbl>   <dbl>   <dbl>\n1 response treatment mean(DMT1~   0.804   0.102    7.88 3.25e-15   0.604    1.00\n# ... with abbreviated variable names 1: estimate, 2: std.error, 3: statistic,\n#   4: conf.low, 5: conf.high\n\n\nAs indicated in the summary output above, the annualized relapse rate ratio for DMT1 vs DMT0 among patients treated with DMT0 (ATT) is given as 0.8 with a 95% confidence interval ranging from 0.6 to 1."
  },
  {
    "objectID": "chapter_06.html#divide-sample-into-quintiles-of-propensity-scores",
    "href": "chapter_06.html#divide-sample-into-quintiles-of-propensity-scores",
    "title": "1  Confounding adjustment using propensity score methods",
    "section": "6.1 Divide sample into quintiles of propensity scores",
    "text": "6.1 Divide sample into quintiles of propensity scores\nWe will form five mutually exclusive groups of the estimated propensity score.\n\n# Create five strata\ndat <- dat %>% mutate(ps.strata = cut(ps, \n                                      breaks = c(quantile(ps, probs=seq(0,1,0.2))),\n                                      labels = seq(1:5),\n                                      include.lowest = TRUE))\n\n# Number of patients in each stratum\ntable(dat$ps.strata)\n\n\n   1    2    3    4    5 \n2002 2015 1991 1997 1995"
  },
  {
    "objectID": "chapter_06.html#assess-balance-within-each-propensity-score-stratum",
    "href": "chapter_06.html#assess-balance-within-each-propensity-score-stratum",
    "title": "1  Confounding adjustment using propensity score methods",
    "section": "6.2 Assess balance within each propensity score stratum",
    "text": "6.2 Assess balance within each propensity score stratum\nWithin each propensity score stratum, treated and control patients should have similar values of the propensity score and the distribution of baseline covariates should be approximately balanced between treatment groups.\n\n6.2.1 Propensity Score Stratum #1\n\ntab1.strata1 <- CreateTableOne(vars, data = dat %>% filter(ps.strata == 1), \n                               factorVars = c(\"female\", \"prevDMTefficacy\"), \n                               strata = \"treatment\", test = FALSE)\n\ntab1.strata1.print <- print(tab1.strata1, catDigits = 2, contDigits = 2, \n                            smd = TRUE)\n\n\n\n\n\n\n\nDMT0\nDMT1\nSMD\n\n\n\n\nn\n901\n1101\n\n\n\nage (mean (SD))\n58.38 (3.67)\n57.45 (3.73)\n0.251\n\n\nfemale = 1 (%)\n605 (67.15)\n775 (70.39)\n0.070\n\n\nprevDMTefficacy (%)\n\n\n0.056\n\n\nNone\n650 (72.14)\n771 (70.03)\n\n\n\nLow_efficacy\n106 (11.76)\n130 (11.81)\n\n\n\nMedium_high_efficacy\n145 (16.09)\n200 (18.17)\n\n\n\nprerelapse_num (mean (SD))\n0.29 (0.53)\n0.33 (0.56)\n0.074\n\n\n\n\n\n\n\n6.2.2 Propensity Score Stratum #2\n\ntab1.strata2 <- CreateTableOne(vars, data = dat %>% filter(ps.strata == 2), \n                               factorVars = c(\"female\", \"prevDMTefficacy\"), \n                               strata = \"treatment\", test = FALSE)\n\ntab1.strata2.print <- print(tab1.strata2, catDigits = 2, contDigits = 2, \n                            smd = TRUE)\n\n\n\n\n\n\n\nDMT0\nDMT1\nSMD\n\n\n\n\nn\n617\n1398\n\n\n\nage (mean (SD))\n52.18 (4.35)\n51.97 (4.22)\n0.049\n\n\nfemale = 1 (%)\n458 (74.23)\n1048 (74.96)\n0.017\n\n\nprevDMTefficacy (%)\n\n\n0.054\n\n\nNone\n292 (47.33)\n624 (44.64)\n\n\n\nLow_efficacy\n69 (11.18)\n162 (11.59)\n\n\n\nMedium_high_efficacy\n256 (41.49)\n612 (43.78)\n\n\n\nprerelapse_num (mean (SD))\n0.40 (0.64)\n0.41 (0.66)\n0.004\n\n\n\n\n\n\n\n6.2.3 Propensity Score Stratum #3\n\ntab1.strata3 <- CreateTableOne(vars, data = dat %>% filter(ps.strata == 3), \n                               factorVars = c(\"female\", \"prevDMTefficacy\"), \n                               strata = \"treatment\", test = FALSE)\n\ntab1.strata3.print <- print(tab1.strata3, catDigits = 2, contDigits = 2, \n                            smd = TRUE)\n\n\n\n\n\n\n\nDMT0\nDMT1\nSMD\n\n\n\n\nn\n392\n1599\n\n\n\nage (mean (SD))\n46.73 (4.06)\n46.36 (4.08)\n0.092\n\n\nfemale = 1 (%)\n305 (77.81)\n1193 (74.61)\n0.075\n\n\nprevDMTefficacy (%)\n\n\n0.041\n\n\nNone\n168 (42.86)\n687 (42.96)\n\n\n\nLow_efficacy\n52 (13.27)\n191 (11.94)\n\n\n\nMedium_high_efficacy\n172 (43.88)\n721 (45.09)\n\n\n\nprerelapse_num (mean (SD))\n0.49 (0.68)\n0.47 (0.66)\n0.031\n\n\n\n\n\n\n\n6.2.4 Propensity Score Stratum #4\n\ntab1.strata4 <- CreateTableOne(vars, data = dat %>% filter(ps.strata == 4), \n                               factorVars = c(\"female\", \"prevDMTefficacy\"), \n                               strata = \"treatment\", test = FALSE)\n\ntab1.strata4.print <- print(tab1.strata4, catDigits = 2, contDigits = 2, \n                            smd = TRUE)\n\n\n\n\n\n\n\nDMT0\nDMT1\nSMD\n\n\n\n\nn\n269\n1728\n\n\n\nage (mean (SD))\n41.07 (4.11)\n40.88 (4.29)\n0.046\n\n\nfemale = 1 (%)\n203 (75.46)\n1356 (78.47)\n0.071\n\n\nprevDMTefficacy (%)\n\n\n0.084\n\n\nNone\n105 (39.03)\n634 (36.69)\n\n\n\nLow_efficacy\n22 ( 8.18)\n181 (10.47)\n\n\n\nMedium_high_efficacy\n142 (52.79)\n913 (52.84)\n\n\n\nprerelapse_num (mean (SD))\n0.50 (0.69)\n0.51 (0.71)\n0.012\n\n\n\n\n\n\n\n6.2.5 Propensity Score Stratum #5\n\ntab1.strata5 <- CreateTableOne(vars, data = dat %>% filter(ps.strata == 5), \n                               factorVars = c(\"female\", \"prevDMTefficacy\"), \n                               strata = \"treatment\", test = FALSE)\n\ntab1.strata5.print <- print(tab1.strata5, catDigits = 2, contDigits = 2, \n                            smd = TRUE)\n\n\n\n\n\n\n\nDMT0\nDMT1\nSMD\n\n\n\n\nn\n121\n1874\n\n\n\nage (mean (SD))\n33.26 (4.95)\n32.04 (5.58)\n0.233\n\n\nfemale = 1 (%)\n100 (82.64)\n1543 (82.34)\n0.008\n\n\nprevDMTefficacy (%)\n\n\n0.050\n\n\nNone\n32 (26.45)\n455 (24.28)\n\n\n\nLow_efficacy\n12 ( 9.92)\n194 (10.35)\n\n\n\nMedium_high_efficacy\n77 (63.64)\n1225 (65.37)\n\n\n\nprerelapse_num (mean (SD))\n0.52 (0.66)\n0.52 (0.73)\n0.004"
  },
  {
    "objectID": "chapter_06.html#estimating-and-pooling-of-stratum-specific-treatment-effects",
    "href": "chapter_06.html#estimating-and-pooling-of-stratum-specific-treatment-effects",
    "title": "1  Confounding adjustment using propensity score methods",
    "section": "6.3 Estimating and pooling of stratum-specific treatment effects",
    "text": "6.3 Estimating and pooling of stratum-specific treatment effects\nThe overall ATT across strata can be estimated by weighting stratum-specific estimates by the proportion of treated patients in each stratum over all treated patients in the sample.\nWe first define a function att.strata.function() to calculate stratum-specific estimates of the treatment effect:\n\natt.strata.function <- function(data, stratum, confint = TRUE) {\n\n  fit <- glm(\"y ~ treatment + offset(log(years))\",\n      family = poisson(link = \"log\"),\n      data = data %>% filter(ps.strata == stratum))\n\n  arr <- round(as.numeric(exp(coef(fit)[\"treatmentDMT1\"])), digits = 3)\n  ll <- ul <- NA\n  \n  if (confint) {\n    ll <- round(exp(confint(fit))[\"treatmentDMT1\",1], digits = 3)\n    ul <- round(exp(confint(fit))[\"treatmentDMT1\",2], digits = 3)\n  }\n  \n  return(c(\"stratum\" = stratum,\n           \"arr\" = arr,\n           \"ci_lower\"  = ll,\n           \"ci_upper\"  = ul))\n}\n\narr.strata <- as.data.frame(t(sapply(1:5, att.strata.function, data = dat)))\narr.strata\n\n  stratum   arr ci_lower ci_upper\n1       1 0.904    0.760    1.076\n2       2 0.822    0.696    0.975\n3       3 0.798    0.666    0.961\n4       4 0.716    0.587    0.881\n5       5 0.589    0.463    0.761\n\n\nSubsequently, we define a function weights.strata.function() to calculate the weights for each stratum. The weight is the proportion of treated patients in each stratum over all treated patients in the sample:\n\nweights.strata.function <- function(data, stratum) {\n  n_DMT1_stratum <- nrow(data %>% filter(ps.strata == stratum & treatment == \"DMT1\"))\n  n_DMT1_all <- nrow(data %>% filter(treatment == \"DMT1\"))\n  weight <- n_DMT1_stratum/n_DMT1_all\n  return(c(\"stratum\" = stratum, \"weight\" = weight))\n}\n\nweights.strata <- as.data.frame(t(sapply(1:5, weights.strata.function, data = dat)))\nweights.strata\n\n  stratum    weight\n1       1 0.1429870\n2       2 0.1815584\n3       3 0.2076623\n4       4 0.2244156\n5       5 0.2433766\n\n\n\n# Create table with ARRs and weights for each PS stratum\narr.weights.merged <- merge(arr.strata, weights.strata, by = \"stratum\")\n\n# Calculate the weighted ARR for each stratum\narr.weights.merged <- arr.weights.merged %>%\n  mutate(weighted.arr = as.numeric(arr) * weight)\n\n# Sum the weighted ARRs across strata to get the overall ATT\nsum(arr.weights.merged$weighted.arr)\n\n[1] 0.7482462\n\n\n\n\n\nWe now define a new function ps.stratification.bootstrap() that integrates estimation of the ATT and the PS weights for bootstrapping purposes:\n\nps.stratification.bootstrap <- function(data, inds) {\n  d <- data[inds,]\n  \n  d$ps.strata <- cut(d$ps, \n                       breaks = c(quantile(dat$ps, probs = seq(0, 1, by = 0.2))),\n                       labels = seq(5),\n                       include.lowest = TRUE)\n  \n  arr.strata <- as.data.frame(t(sapply(1:5, att.strata.function, \n                                       data = d, confint = FALSE)))\n  \n  weights.strata <- as.data.frame(t(sapply(1:5, weights.strata.function, data = d)))\n  \n  return(arr.strata$arr[1] * weights.strata$weight[1] + \n           arr.strata$arr[2] * weights.strata$weight[2] +\n           arr.strata$arr[3] * weights.strata$weight[3] + \n           arr.strata$arr[4] * weights.strata$weight[4] +\n           arr.strata$arr[5] * weights.strata$weight[5])                                                  \n}\n\nWe can now estimate the treatment effect and its confidence interval using the bootstrap procedure:\n\nlibrary(boot)\n\nset.seed(1854)\narr.stratification.boot <- boot(data = dat, \n                                statistic = ps.stratification.bootstrap, \n                                R = 1000)\n\n# Bootstrapped ARR\nmedian(arr.stratification.boot$t)\n\n[1] 0.7558609\n\n# Bootstrapped ARR 95% CI\nquantile(arr.stratification.boot$t[,1], c(0.025, 0.975))\n\n     2.5%     97.5% \n0.6835885 0.8362947"
  },
  {
    "objectID": "chapter_06.html#calculate-propensity-score-weights-for-att",
    "href": "chapter_06.html#calculate-propensity-score-weights-for-att",
    "title": "1  Confounding adjustment using propensity score methods",
    "section": "7.1 Calculate propensity score weights for ATT",
    "text": "7.1 Calculate propensity score weights for ATT\nPropensity score weighting reweights the study sample to generate an artificial population (i.e., pseudo-population) in which the covariates are no longer associated with treatment, thereby removing confounding by measured covariates. For the ATT, the weight for all treated patients is set to one. Conversely, the weight for patients in the control group is set to the propensity score divided by one minus the propensity score, that is, (PS/(1 − PS)). We estimated stabilized weights to address extreme weights.\n\nlibrary(WeightIt)\n\nw.out <- weightit(treatment ~ age + female + prevDMTefficacy + prerelapse_num,\n                  data = dat,\n                  method = \"ps\",\n                  estimand = \"ATT\")\n                  #stabilize = TRUE)\n\nw.out\n\nA weightit object\n - method: \"ps\" (propensity score weighting)\n - number of obs.: 10000\n - sampling weights: none\n - treatment: 2-category\n - estimand: ATT (focal: DMT1)\n - covariates: age, female, prevDMTefficacy, prerelapse_num\n\nsummary(w.out)\n\n                 Summary of weights\n\n- Weight ranges:\n\n        Min                                   Max\nDMT0 0.4772 |---------------------------| 48.6856\nDMT1 1.0000  ||                            1.0000\n\n- Units with 5 most extreme weights by group:\n                                             \n         9492    8836    6544    9610    4729\n DMT0 32.1027 32.1027 34.3126 38.1817 48.6856\n            8       7       4       2       1\n DMT1       1       1       1       1       1\n\n- Weight statistics:\n\n     Coef of Var   MAD Entropy # Zeros\nDMT0       1.098 0.673   0.383       0\nDMT1       0.000 0.000  -0.000       0\n\n- Effective Sample Sizes:\n\n              DMT0 DMT1\nUnweighted 2300.   7700\nWeighted   1043.16 7700\n\nplot(summary(w.out))"
  },
  {
    "objectID": "chapter_06.html#assess-balance-in-the-weighted-sample",
    "href": "chapter_06.html#assess-balance-in-the-weighted-sample",
    "title": "1  Confounding adjustment using propensity score methods",
    "section": "7.2 Assess balance in the weighted sample",
    "text": "7.2 Assess balance in the weighted sample\n\nbal.tab(w.out, stats = c(\"m\", \"v\"), thresholds = c(m = .05))\n\nBalance Measures\n                                         Type Diff.Adj     M.Threshold\nprop.score                           Distance  -0.0045 Balanced, <0.05\nage                                   Contin.   0.0054 Balanced, <0.05\nfemale                                 Binary   0.0005 Balanced, <0.05\nprevDMTefficacy_None                   Binary  -0.0003 Balanced, <0.05\nprevDMTefficacy_Low_efficacy           Binary   0.0023 Balanced, <0.05\nprevDMTefficacy_Medium_high_efficacy   Binary  -0.0020 Balanced, <0.05\nprerelapse_num                        Contin.  -0.0034 Balanced, <0.05\n                                     V.Ratio.Adj\nprop.score                                0.9926\nage                                       1.0102\nfemale                                         .\nprevDMTefficacy_None                           .\nprevDMTefficacy_Low_efficacy                   .\nprevDMTefficacy_Medium_high_efficacy           .\nprerelapse_num                            1.0941\n\nBalance tally for mean differences\n                    count\nBalanced, <0.05         7\nNot Balanced, >0.05     0\n\nVariable with the greatest mean difference\n Variable Diff.Adj     M.Threshold\n      age   0.0054 Balanced, <0.05\n\nEffective sample sizes\n              DMT0 DMT1\nUnadjusted 2300.   7700\nAdjusted   1043.16 7700"
  },
  {
    "objectID": "chapter_06.html#estimate-the-att",
    "href": "chapter_06.html#estimate-the-att",
    "title": "1  Confounding adjustment using propensity score methods",
    "section": "7.3 Estimate the ATT",
    "text": "7.3 Estimate the ATT\nOne way to estimate the ATT is to use the survey package. The function svyglm() generates model-robust (Horvitz-Thompson-type) standard errors by default, and thus does not require additional adjustments.\n\nlibrary(survey)\n\nweighted.data <- svydesign(ids = ~1, data = dat, weights = ~w.out$weights)\n\nweighted.fit <- svyglm(y ~ treatment + offset(log(years)),\n                       family = poisson(link = \"log\"),\n                       design = weighted.data)\n\nexp(coef(weighted.fit)[\"treatmentDMT1\"])\n\ntreatmentDMT1 \n    0.7083381 \n\nexp(confint(weighted.fit))[\"treatmentDMT1\",] \n\n    2.5 %    97.5 % \n0.6245507 0.8033662 \n\n\n\n\n\nAs indicated above, propensity score weighting yielded an ATT estimate of 0.71 (95% CI: 0.66; 0.76).\nAn alternative approach is to use glm() to estimate the treatment effect and calculate robust standard errors.\n\n# Alternative way to estimate treatment effect\nweighted.fit2 <- glm(y ~ treatment + offset(log(years)),\n              family = poisson(link = \"log\"),\n              data = dat,\n              weights = w.out$weights)\n\n# Extract the estimated ARR\nexp(coef(weighted.fit2))[\"treatmentDMT1\"]\n\ntreatmentDMT1 \n    0.7083381 \n\n# Calculate robust standard error and p-value of the log ARR\ncoeftest(weighted.fit2, vcov. = vcovHC)[\"treatmentDMT1\",]\n\n     Estimate    Std. Error       z value      Pr(>|z|) \n-3.448337e-01  6.442745e-02 -5.352280e+00  8.685284e-08 \n\n# Derive 95% confidence interval of the ARR\nexp(lmtest::coefci(weighted.fit2, \n       level = 0.95, # 95% confidence interval\n       vcov. = vcovHC)[\"treatmentDMT1\",])\n\n    2.5 %    97.5 % \n0.6243094 0.8036767 \n\n\n\n\n\nUsing this approach, the ATT estimate was 0.71 (95% CI: 0.62; 0.8)."
  },
  {
    "objectID": "chapter_12.html",
    "href": "chapter_12.html",
    "title": "2  Dealing with irregular and informative visits",
    "section": "",
    "text": "3 Introduction\nWe first load the required packages\nIn this example dataset, we have a discrete outcome y that is affected by its baseline value edss, age, sex, and the treatment duration time.\nWe remove y according to the informative visit process that depends on the received treatment, gender, and age.\nIn the censored data, a total of 17 out of 5000 patients have a visit at time=60.\nWe will estimate the marginal treatment effect at time time=60."
  },
  {
    "objectID": "chapter_12.html#original-data",
    "href": "chapter_12.html#original-data",
    "title": "2  Dealing with irregular and informative visits",
    "section": "5.1 Original data",
    "text": "5.1 Original data\n\norigdat60 <- dataset %>% filter(time == 60)\n\n# Predict probability of treatment allocation\nfitps <- glm(x ~ age + sex + edss, family = 'binomial', \n             data = origdat60)\n\n# Derive the propensity score\norigdat60 <- origdat60 %>% mutate(ipt = ifelse(x == 1, 1/predict(fitps, type = 'response'),\n                                               1/(1-predict(fitps, type = 'response'))))\n\n# Estimate \nfit_ref_m <- tidy(lm(y ~ x, weight = ipt, data = origdat60), conf.int = TRUE)"
  },
  {
    "objectID": "chapter_12.html#doubly-weighted-marginal-treatment-effect",
    "href": "chapter_12.html#doubly-weighted-marginal-treatment-effect",
    "title": "2  Dealing with irregular and informative visits",
    "section": "5.2 doubly-weighted marginal treatment effect",
    "text": "5.2 doubly-weighted marginal treatment effect\n\nobsdat60 <- dataset_visit %>% mutate(visit = ifelse(is.na(y_obs),0,1)) %>% filter(time == 60)\n\ngamma <- glm(visit ~ x + sex + age + edss, family = 'binomial', data = obsdat60)$coef   \n\nobsdat60 <- obsdat60 %>% mutate(rho_i = 1/exp(gamma[\"(Intercept)\"] +\n                                                          gamma[\"x\"]*x +\n                                                          gamma[\"sex\"]*sex +\n                                                          gamma[\"age\"]*age))\n\n# Predict probability of treatment allocation\nfitps <- glm(x ~ age + sex + edss, family='binomial', data = obsdat60)\n\n# Derive the propensity score\nobsdat60 <- obsdat60 %>% mutate(ipt = ifelse(x==1, 1/predict(fitps, type='response'),\n                                            1/(1-predict(fitps, type='response'))))\n\n\nfit_w <- tidy(lm(y_obs ~ x, weights = ipt*rho_i, data = obsdat60), conf.int = TRUE)"
  },
  {
    "objectID": "chapter_12.html#multilevel-multiple-imputation",
    "href": "chapter_12.html#multilevel-multiple-imputation",
    "title": "2  Dealing with irregular and informative visits",
    "section": "5.3 Multilevel multiple imputation",
    "text": "5.3 Multilevel multiple imputation\nWe impute the entire vector of y_obs for all 61 potential visits and generate 10 imputed datasets. Note: mlmi currently does not support imputation of treatment-covariate interaction terms.\n\nimp <- impute_y_mice_3l(dataset_visit, seed = 12345)\n\n\n\n\nWe can now estimate the treatment effect in each imputed dataset\n\n# Predict probability of treatment allocation\nfitps <- glm(x ~ age + sex + edss, family='binomial', data = dataset_visit)\n  \n# Derive the propensity score\ndataset_visit <- dataset_visit %>% mutate(ipt = ifelse(x==1, 1/predict(fitps, type='response'),\n                                                       1/(1-predict(fitps, type='response'))))\n  \nQ <- U <- rep(NA, 10) # Error variances\n\nfor (i in seq(10)) {\n  dati <- cbind(dataset_visit[,c(\"x\",\"ipt\",\"time\")], y_imp = imp[,i]) %>% filter(time == 60)\n  \n  # Estimate \n  fit <- tidy(lm(y_imp ~ x, weight = ipt, data = dati), conf.int = TRUE) \n  \n  Q[i] <- fit %>% filter(term == \"x\") %>% pull(estimate)\n  U[i] <- (fit %>% filter(term == \"x\") %>% pull(std.error))**2\n}\n\nfit_mlmi <- pool.scalar(Q = Q, U = U)"
  },
  {
    "objectID": "chapter_12.html#doubly--weighted-marginal-treatment-effect-total",
    "href": "chapter_12.html#doubly--weighted-marginal-treatment-effect-total",
    "title": "2  Dealing with irregular and informative visits",
    "section": "6.1 doubly -weighted marginal treatment effect total",
    "text": "6.1 doubly -weighted marginal treatment effect total\n\nobsdatall <- dataset_visit %>% mutate(visit = ifelse(is.na(y_obs),0,1))  \ngamma <- glm(visit ~ x + sex + age + edss, family = 'binomial', data = obsdatall)$coef   \nobsdatall <- obsdatall %>% mutate(rho_i = 1/exp(gamma[\"(Intercept)\"] +\n                                                gamma[\"x\"]*x +\n                                                gamma[\"sex\"]*sex +\n                                                gamma[\"age\"]*age))\n# Predict probability of treatment allocation\nfitps <- glm(x ~ age + sex + edss, family='binomial', data = obsdatall)\n# Derive the propensity score\nobsdatall <- obsdatall %>% mutate(ipt = ifelse(x==1, 1/predict(fitps, type='response'),\n                                             1/(1-predict(fitps, type='response'))))\nfit_w <- tidy(lm(y_obs ~ x, weights = ipt*rho_i, data = obsdatall), conf.int = TRUE)"
  }
]