[
  {
    "objectID": "chapter_09.html#main-analysis",
    "href": "chapter_09.html#main-analysis",
    "title": "5  Dealing with missing data",
    "section": "5.1 Main Analysis",
    "text": "5.1 Main Analysis\nThe main objective of this analysis is to assess whether the number of episodes (y) occurring within specific time periods (years) differs between the treatment groups (1: DMF and 0: TERI). To address potential confounding factors, the researchers consider variables such as patient age, the log of premedical cost (logPremedicalcost), previous DMT efficacy (prevDMTefficacy), and the number of episodes in previous relapses (prerelapseNum).\nWhen estimating treatment effects from observational data, an assumption is made that the patient populations in both treatment groups are as similar as possible. Various methods for balancing data across treatment groups are proposed, including matching, inverse propensity weighting, stratification, and regression adjustment.\nIn this case, the focus is specifically on the matching method, which offers advantages over regression adjustment by potentially alleviating issues related to model mis-specification. This includes addressing non-linear relationships between certain confounders and the outcome variable and accounting for treatment effects that may depend on specific confounders (treatment-confounder interaction terms). Propensity scores are used to match subjects in the treatment groups.\nMoreover, intentionally introducing incomplete covariate variables in this example adds complexity to the propensity score estimation. Depending on the propensity score estimation technique employed, it may be necessary to incorporate an imputation step. For instance, logistic regression estimation requires complete data for all observations, while XGBoost is robust to missing data .\nTo estimate marginal treatment effects, the g-computation method is employed . This method involves specifying a model for the outcome dependent on the treatment and covariates. The potential outcomes, i.e., the predicted values of the outcome on treatment (\\(y_i^1\\)) and control (\\(y_i^0\\)) for each sample unit \\(i\\), are estimated. The marginal treatment effect is then calculated by contrasting the averaged estimated potential outcomes.\nIn this example, we consider the estimation of comparative treatment effects in the absence of treatment-effect heterogeneity."
  },
  {
    "objectID": "chapter_09.html#estimation-workflow",
    "href": "chapter_09.html#estimation-workflow",
    "title": "5  Dealing with missing data",
    "section": "5.2 Estimation workflow",
    "text": "5.2 Estimation workflow\nThe proposed workflow consists of the following steps:\n\n\n\nEstimation Workflow\n\n\n\nData Exploration: In this step, we examine the observed data to comprehend the variables within the dataset. Our primary focus lies on identifying missing patterns and relationships among observed variables, including missing indicator variables and others. This exploration aids in discerning the most plausible missing mechanisms and suitable imputation techniques. Additionally, field experts’ insights may be incorporated to enhance understanding of the missing process, potentially considering MNAR assumptions.\nImputation: It is essential to evaluate whether the imputation procedure is necessary or if simpler methods, such as complete case analysis, are more suitable. In case imputation procedures are required, selecting plausible imputation methods that align with the main model analysis is crucial. This involves choosing individual imputation methods for each incomplete variable, determining the predictor variables on the imputation model. Pre_imputation (where imputation values can be deterministically derived from other variables) and Post-imputation (e.g.ensuring imputed values fall within a reasonable range) steps may also considered.\nData Balancing: Several methods, including PS matching or inverse weighting propensity score, can be utilized. It is required to evaluate the balance, which could be done via visual inspection.(eg.cobalt package). In this example, we estimate propensity scores using logistic regression. For most balancing procedures in R, counterparts specifically designed for imputed datasets are available, such as those in the matchthem R package, which includes PS matching and IPW as done in the matchit R package.\nModel Fit: : It is fit a model to predict the outcomes for each sample unit under each possible treatment value (DMF and TERI), as predictors include the treatment and optionally the baseline covariates and also the propensity score.\nTreatment Estimation & Pooling: For simplicity in this tutorial, we will use the comparison functions from the R matchingmethods package , which can be used for completed data and also from outputs from the imputation process. In the last case, internally the functions calculate the treatment effects on each imputed dataset and pool the estimates using Rubin’s Rules.\n\nLet’s start by preparing the R environment. All the functions used in this tutorial can be found in the resource file functions.r.\n\n# Load the required packages and additional functions\nsource(\"resources/chapter 09/functions.r\")"
  },
  {
    "objectID": "chapter_09.html#homogeneous-treatment-effect",
    "href": "chapter_09.html#homogeneous-treatment-effect",
    "title": "5  Dealing with missing data",
    "section": "5.3 Homogeneous Treatment Effect",
    "text": "5.3 Homogeneous Treatment Effect\nIn this example, we focus on estimating comparative treatment effects in the absence of heterogeneous treatment effects (HTE).\n\n5.3.1 Generating an Observational Dataset\nWe can simulate an observational dataset of \\(N = 3000\\) patients as follows:\n\ndata_hom &lt;- generate_data(n = 3000, seed = 1234) \n\nThe generate_data() function allows the specification of various treatment effect options, easily adjustable by modifying the beta parameter. In this instance, we assume a consistent treatment effect across the entire target population. This dataset currently contains no missing values.\nThe simulated dataset comprises two treatment groups with variations in baseline characteristics. For example, the figure below illustrates baseline imbalances in covariates such as age.\n\n\n\n\n\nFigure 1:Distribution of confounders and outcome variable\n\n\n\n\nWe can calculate the treatment effect on the complete observed dataset. To do this, we start by balancing the dataset using Propensity Score matching. In this case, the propensity score model uses confounder variables only: age, gender, prevDMTefficacy, logPremedicalcost, and prerelapseNum.\n\n## Apply Matching on the PS for the ATE\nmF &lt;- matchit(  treatment ~ age + gender + prevDMTefficacy + logPremedicalcost + prerelapseNum, \n                data = data_hom,\n                family = binomial,\n                method = \"full\",\n                caliper = 0.2,\n                estimand = \"ATE\",\n                replace = FALSE) \n\n## Extract matched data\nmdata &lt;- match.data(mF)\n\nThen, we proceed to model estimation. In this case, a Poisson model is used with the form:\n\\[\\begin{eqnarray}count_i\\sim Poisson(\\lambda_i)\\end{eqnarray}\\]\n\\[\\begin{eqnarray} log(\\lambda_i) &=& \\beta_0 + \\beta_1treatment_i + \\beta_2age + \\beta_3gender \\\\ &+& \\beta_4prevDMTefficacy + \\beta_5logPremedicalcost\n\\\\ &+& \\beta_6prerelapseNum + \\beta_7numSymptoms + offset(log(years))\\end{eqnarray}\\]\nSince patient measurements were recorded over varying time frames, the natural logarithm of the years of evaluation is incorporated as an offset in the model. The model is fitted with a glm function, and we include the treatment and the baseline covariates as predictors, which are optional if the data is well balanced. Additionally, it is necessary to specify the matching weights in the glm function.\n\n# Model fit\nfit_mod &lt;- glm(as.formula(\"y ~ treatment + gender + age + logPremedicalcost + prerelapseNum + prevDMTefficacy + numSymptoms + offset(log(years))\"),\n                 family = poisson(link = \"log\"),\n                 data = mdata,\n                 weights = weights)\n\nTypically, Poisson models adjust standard errors using robust standard errors to accommodate small values arising from the equidispersion assumption. This correction can be directly applied to the model using the vcovCL() function . However, given that we will calculate the treatment effect using the functions of the marginaleffects package, this step becomes redundant. This package allows specifying HC3 sandwich standard errors during treatment estimation.\nFinally, we calculate the Average Treatment Effect (ATE). The ATE is defined as \\[\\tau_{ATE}=E(y_i^1-y_i^0)\\]\nBut this cannot be directly extracted from the \\(\\beta_1\\) parameter, as the model has \\(log(\\lambda)\\) as the response. We estimate it as: \\[\\tau_{ATE}=E(\\lambda^1_i-\\lambda^0_i)\\] This can be done with the function avg_comparisons(), from the R package marginaleffect, that calculates the potential outcomes for each unit sample and then combines them to summarize the average effect.\n\n# Estimation of treatment effects with robust standard errors\nATE &lt;- avg_comparisons(fit_mod, \n                       variables = list(treatment = c(\"TERI\",\"DMF\")),\n                       vcov = \"HC3\",\n                       newdata = mdata,\n                       wts = \"weights\")\n\nresult_ATE &lt;- data.frame( ATE,\n                          analysis = \"Full Data\")\n\nHenceforth, for ease of explanation, we will use the function TE_estimation() attached to the function code that performs all the previous estimation steps at once.\n\n\n5.3.2 Generating Missing Values\nIn this example, we focus on the case where confounder variables are incomplete.\nMissing values can be generated using the getmissdata() function, considering the following patterns of missingness for the log previous medical cost (logPremedicalcost):\n\nMAR: missingness depends on age and sex\nMART: missingness depends on age, sex and the treatment variable treatment\nMARTY: missingness depends on age, sex, treatment and the outcome variable y\nMNAR: missingness depends on age, sex and logPremedicalcost\n\nLets select the missing data pattern “MART”\n\nm_data_hom &lt;- get_missdata(data = data_hom, scenario = \"MART\")\n\nAfter introducing missing values, we only have complete data for \\(N=\\) 1015 patients.\n\n\n\n\n\nBaseline characteristics of the incomplete dataset.\n\n\n\n\n\n\n\n\n\nDMF\n(N=2265)\nTERI\n(N=735)\nOverall\n(N=3000)\n\n\n\n\nAge (years)\n\n\n\n\n\nMean (SD)\n44.4 (9.95)\n51.5 (8.59)\n46.2 (10.1)\n\n\nMedian [Min, Max]\n45.0 [18.0, 64.0]\n53.0 [23.0, 64.0]\n47.0 [18.0, 64.0]\n\n\nMissing\n303 (13.4%)\n54 (7.3%)\n357 (11.9%)\n\n\nGender\n\n\n\n\n\nFemale\n1740 (76.8%)\n526 (71.6%)\n2266 (75.5%)\n\n\nMale\n525 (23.2%)\n209 (28.4%)\n734 (24.5%)\n\n\nEfficacy of previous DMT\n\n\n\n\n\nNone\n725 (32.0%)\n318 (43.3%)\n1043 (34.8%)\n\n\nLow_efficacy\n190 (8.4%)\n52 (7.1%)\n242 (8.1%)\n\n\nMedium_high_efficacy\n800 (35.3%)\n225 (30.6%)\n1025 (34.2%)\n\n\nMissing\n550 (24.3%)\n140 (19.0%)\n690 (23.0%)\n\n\nLog prior medical costs\n\n\n\n\n\nMean (SD)\n8.71 (1.03)\n9.45 (1.20)\n8.98 (1.15)\n\n\nMedian [Min, Max]\n8.75 [5.10, 11.3]\n9.48 [5.56, 12.7]\n8.98 [5.10, 12.7]\n\n\nMissing\n1145 (50.6%)\n109 (14.8%)\n1254 (41.8%)\n\n\nNumber of prior symptoms\n\n\n\n\n\n0\n158 (7.0%)\n53 (7.2%)\n211 (7.0%)\n\n\n1\n1142 (50.4%)\n401 (54.6%)\n1543 (51.4%)\n\n\n&gt;=2\n432 (19.1%)\n151 (20.5%)\n583 (19.4%)\n\n\nMissing\n533 (23.5%)\n130 (17.7%)\n663 (22.1%)\n\n\nNumber of prior relapses\n\n\n\n\n\nMean (SD)\n0.438 (0.650)\n0.414 (0.653)\n0.432 (0.650)\n\n\nMedian [Min, Max]\n0 [0, 4.00]\n0 [0, 3.00]\n0 [0, 4.00]\n\n\nMissing\n305 (13.5%)\n71 (9.7%)\n376 (12.5%)\n\n\n\n\n\n\n\n\n\n5.3.3 Data Exploration\nFirst, let’s examine the missing patterns in our dataset. This will help us better understand the missing data, including the proportion of missing values and the underlying causes, which may be related to other observable variables. Various R packages, such as ggmice, vim, naniar, and factorminer, can assist in visualizing the data.\nUpon initial examination, we’ve observed that the outcome variable y, treatment status, and gender are fully observed, while other covariate variables, including age, logPremedicalcost, prevDMTefficacy, numSymptoms, and prerelapseNum, have incomplete data. In total, approximately 11% of the observations in the dataset have missing data. These missing values follow a non-monotonic pattern, requiring the use of MCMC imputation techniques, therefore we mainly use the Full Conditional Specification approach given in the mice package.\nWhen assessing missingness based on treatment groups, we find that there are more patients receiving DMF treatment. However, the percentage of missing values is higher for the DMF treatment group compared to the TERI treatment group, indicating that data is unlikely to be MCAR when the proportion of missing differs by treatment allocation.\n\nlibrary(naniar)\nnaniar::gg_miss_upset(m_data_hom, nsets = 10)\n\n\n\nnaniar::gg_miss_var(m_data_hom, facet = treatment, show_pct = T)\n\n\n\n\nAdditionally, it could be explored whether associations exist between the missingness of variables and other observed variables, indicating if a MAR assumption is more plausible than a MCAR assumption. The plausibility of MNAR vs. MAR assumptions cannot be evidenced by observable data, and the possibility of a MNAR scenario is contemplated based on expert input.\n\n\n5.3.4 Methods for Handling Missing Data\n\n5.3.4.1 Complete Case Analysis\nTo estimate the ATE using propensity score matching, we employ complete case analysis. Initially, we filter out all units with incomplete data and then apply the estimation process as before.\n\n# Filter out the complete case data\nccdata &lt;- m_data_hom[complete.cases(m_data_hom), ]\n\n# Estimation procedure\nresult_CC &lt;- ATE_estimation( data = ccdata,\n                             analysis = \"Complete Case Analysis\")$ATE\nresult_ATE &lt;- result_ATE %&gt;% add_row(result_CC)\n\n\n\n5.3.4.2 Missing Indicator\nIn this method, it is essential to create missing indicators and deterministic imputed variables for each incomplete variable. For example, for the “age” variable, we calculate a missing indicator, denoted as “age.mind,” and a deterministic imputed value of age where missing values are replaced by an arbitrary value (in this case, the missing values were replaced by zero).\n\ndat$age.mind &lt;- ifelse( is.na(dat$age),1,0)  # missing indicator of age\ndat$age &lt;-  ifelse(is.na(dat$age), 0, dat$age) # deterministic imputed age, \n\nSubsequently, the Propensity Score (PS) model is estimated for all the confounding variables, including their missing indicators. In this case, the propensity score model is given by:\n\nPS.formula &lt;- treatment ~ gender + age.mind + age + lpmc.mind + \n  logPremedicalcost + pde.mind + prevDMTefficacy + prn.mind + prerelapseNum\n\nThen, the estimation process follows the same structure as before:\n\nresult_mind &lt;- ATE_estimation(data = m_data_hom, PSform = \"Mind\", \n                              analysis = \"Missing indicator\")$ATE\nresult_ATE &lt;- result_ATE %&gt;% add_row(result_mind)\n\n\n\n5.3.4.3 Multiple Imputation\nIn this section, we will generate \\(m=10\\) imputed datasets and perform matching within each imputed dataset. We first need to specify the imputation model for prevDMTefficacy, premedicalcost, numSymptoms, prerelapseNum, and age, i.e., the predictors for each incomplete variable. This can be done in mice via the prediction matrix or by using the form parameter where the models are specified for each variable and saved in a list.\n\n# We add a covariate for log(years)\nimpdata &lt;-  m_data_hom %&gt;% mutate(logyears = log(years))\n\n# Specify the conditional imputation models\nform_y &lt;- list(prevDMTefficacy ~ age + gender + logyears + logPremedicalcost + \n                 numSymptoms + treatment + prerelapseNum + y,\n               logPremedicalcost ~ age + gender + logyears + prevDMTefficacy + \n                 numSymptoms + treatment + prerelapseNum + y,\n               numSymptoms ~ age + gender + logPremedicalcost + logyears + \n                 prevDMTefficacy + prerelapseNum + treatment + y,\n               prerelapseNum ~ age + gender + logPremedicalcost + logyears + \n                 prevDMTefficacy + numSymptoms + treatment + y,\n               age ~ prerelapseNum + gender + logPremedicalcost + logyears + \n                 prevDMTefficacy + numSymptoms + treatment + y)\nform_y &lt;- name.formulas(form_y)\n\nNext, we need to set the individual imputation model for each variable. We call the mice function, which automatically proposes certain imputation methods according to the type of variable. Here, we decide to modify the imputation method for the numSymptoms and prevDMTefficacy variables to the predictive mean matching method “pmm”. After this, we run the mice() function to generate 10 imputed datasets.\n\n# Adopt predictive mean matching for imputing the incomplete variables\nimp0 &lt;- mice(impdata, form = form_y, maxit = 0)\nmethod &lt;- imp0$method\nmethod[\"numSymptoms\"] &lt;- \"pmm\"\nmethod[\"prevDMTefficacy\"] &lt;- \"pmm\"\n\n# Generate 10 imputed datasets\nimp &lt;- mice(impdata, form = form_y, method = method, m = 10, maxit = 10,  \n            printFlag = FALSE)\n\nBefore proceeding with the estimation procedure, we inspect the convergence of all the imputed variables using a trace plot:\n\nplot(imp)\n\n\n\n\n\n\n\nAs there don’t seem to be any problems in the trace plot (i.e., no marked tendency & well-dispersed plots), we can now proceed with the PS Matching in each of the imputed datasets using the MatchedThem package functions. Here, we adopt full matching without replacement and use a logistic model. The function allows the pooling of PS estimates, which can be done through the within or across approach.\n\n5.3.4.3.1 Multiple Imputation (within approach)\nThe within approach it is specified with the approach parameter in the matchthem function as follows:\n\n# Matching based on PS model\nmdata &lt;- matchthem(formula = treatment ~ age + gender+ prevDMTefficacy + \n                     logPremedicalcost + prerelapseNum,\n                   datasets = imp,\n                   approach = \"within\",\n                   method = \"full\",\n                   caliper = 0.1,\n                   family = binomial,\n                   estimand = \"ATE\",\n                   distance = \"glm\",\n                   link = \"logit\",\n                   replace = FALSE) \n\nThen we proceed to fit a main model on the outcome Y.\n\n# Get a list of the imputed datasets\ndat &lt;- complete( mdata, action = \"all\")\n\n# Fit the model on each imputed dataset\nmod &lt;- lapply(dat, \\(i) \n                glm(formula = as.formula(\"y ~ treatment + gender + age + logPremedicalcost + prerelapseNum + prevDMTefficacy + numSymptoms + offset(log(years))\"),\n                family = poisson(link = \"log\"), \n                weights = weights,\n                data = i))\n\nFinally, we utilize the marginal effects package to estimate the treatment effect. As before, we use the avg_comparisons function, which allows us to specify the variance correction. While most of the marginal effects functions are designed to handle imputed datasets, since we need to evaluate the model on each imputed dataset, we manually pass each imputed dataset into the parameter newdata using the lapply function, as follows:\n\n\n5.3.4.3.2 Multiple Imputation (across aproach)\nWe proceed similarly as before; the only difference is specifying the approach “across” in the matchthem function. To simplify the steps, use the built-up function ATE_estimation.\n\nresult_micea &lt;- ATE_estimation( data = imp,\n                                approach = \"across\",\n                                analysis = \"MICE (across)\")$ATE\nresult_ATE &lt;- bind_rows(result_ATE, result_micea)\n\n\n\n\n\n5.3.5 Results\nAnalysis methods:\n\nFull Data: The treatment effect is estimated in the original data of \\(N=3000\\) patients where no missing values are present. This estimate can be used as a benchmark to compare the missing data methods.\nComplete Case Analysis: The treatment effect is estimated using all data from \\(N=\\) 1015 patients that do not have any missing values.\nMissing Indicator: The treatment effect is estimated in the incomplete dataset of \\(N=3000\\) patients. The propensity score model includes a missing indicator variable for each incomplete covariate.\nMICE: A treatment effect is estimated within each imputed dataset using propensity score analysis. Using Rubin’s rule, the ten treatment effects are combined into a single treatment effect.\n\n\nlibrary(ggplot2)\nresult_ATE$analysis = factor(result_ATE$analysis,\n                         levels = c(\"Full Data\",\n                                    \"Complete Case Analysis\",\n                                    \"Missing indicator\",\n                                    \"MICE (within)\",\n                                    \"MICE (across)\"))\nggplot(result_ATE,aes(x = analysis, y = estimate, col = analysis)) +\n  geom_point(shape = 1,\n             size  = 1) +\n  geom_errorbar(aes(ymin  = conf.low ,\n                    ymax  = conf.high),\n                width = 0.2,\n                size  = 0.5) +\n  see::scale_color_flat() + theme_light() + \n  theme(axis.title.x = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank(),\n        legend.position = \"bottom\")\n\n\n\n\nIn this example, we observe that there is no significant difference across the methods. The Complete Case Analysis yields a wider confidence interval and leads to more biased estimates. It appears that the MICE method with the within approach and Missing Indicator methods provide less biased estimates. However, the formal leads to a non-significant treatment estimate similar to the results from the Full Dataset."
  },
  {
    "objectID": "chapter_09.html#version-info",
    "href": "chapter_09.html#version-info",
    "title": "5  Dealing with missing data",
    "section": "Version info",
    "text": "Version info\nThis chapter was rendered using the following version of R and its packages:\n\n\nR version 4.2.3 (2023-03-15 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=Dutch_Netherlands.utf8  LC_CTYPE=Dutch_Netherlands.utf8   \n[3] LC_MONETARY=Dutch_Netherlands.utf8 LC_NUMERIC=C                      \n[5] LC_TIME=Dutch_Netherlands.utf8    \n\nattached base packages:\n[1] grid      stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] naniar_1.0.0           ggplot2_3.4.3          ggmice_0.1.0          \n [4] missForest_1.5         mice_3.16.0            marginaleffects_0.15.0\n [7] survey_4.2-1           survival_3.5-5         Matrix_1.5-4.1        \n[10] cobalt_4.5.1           sandwich_3.0-2         PSweight_1.1.8        \n[13] WeightIt_0.14.2        MatchThem_1.1.0        MatchIt_4.5.4         \n[16] optmatch_0.10.6        truncnorm_1.0-9        MASS_7.3-60           \n[19] tidyr_1.3.0            dplyr_1.1.2            data.table_1.14.8     \n[22] table1_1.4.3           kableExtra_1.3.4      \n\nloaded via a namespace (and not attached):\n  [1] uuid_1.1-0              backports_1.4.1         systemfonts_1.0.4      \n  [4] plyr_1.8.8              splines_4.2.3           TH.data_1.1-2          \n  [7] chk_0.9.0               digest_0.6.31           foreach_1.5.2          \n [10] htmltools_0.5.5         fansi_1.0.4             magrittr_2.0.3         \n [13] checkmate_2.2.0         see_0.8.0               officer_0.6.3          \n [16] svglite_2.1.1           askpass_1.2.0           gfonts_0.2.0           \n [19] colorspace_2.1-0        rvest_1.0.3             mitools_2.4            \n [22] textshaping_0.3.6       pan_1.6                 xfun_0.39              \n [25] crayon_1.5.2            jsonlite_1.8.5          lme4_1.1-33            \n [28] zoo_1.8-12              iterators_1.0.14        glue_1.6.2             \n [31] gtable_0.3.4            emmeans_1.8.8           nnls_1.5               \n [34] webshot_0.5.5           UpSetR_1.4.0            car_3.1-2              \n [37] shape_1.4.6             jomo_2.7-6              abind_1.4-5            \n [40] scales_1.2.1            fontquiver_0.2.1        mvtnorm_1.2-3          \n [43] DBI_1.1.3               rngtools_1.5.2          rstatix_0.7.2          \n [46] Rcpp_1.0.10             performance_0.10.8      viridisLite_0.4.2      \n [49] xtable_1.8-4            Formula_1.2-5           DT_0.29                \n [52] fontLiberation_0.1.0    glmnet_4.1-7            SuperLearner_2.0-28.1  \n [55] datawizard_0.9.0        htmlwidgets_1.6.2       httr_1.4.7             \n [58] RColorBrewer_1.1-3      ellipsis_0.3.2          pkgconfig_2.0.3        \n [61] farver_2.1.1            nnet_7.3-19             utf8_1.2.3             \n [64] crul_1.4.0              effectsize_0.8.5        tidyselect_1.2.0       \n [67] labeling_0.4.3          rlang_1.1.1             later_1.3.1            \n [70] munsell_0.5.0           tools_4.2.3             cli_3.6.1              \n [73] generics_0.1.3          broom_1.0.5             evaluate_0.21          \n [76] stringr_1.5.0           fastmap_1.1.1           ragg_1.2.5             \n [79] yaml_2.3.7              tables_0.9.17           knitr_1.44             \n [82] zip_2.3.0               purrr_1.0.1             randomForest_4.7-1.1   \n [85] mitml_0.4-5             visdat_0.6.0            nlme_3.1-162           \n [88] doRNG_1.8.6             mime_0.12               gam_1.22-2             \n [91] xml2_1.3.4              compiler_4.2.3          rstudioapi_0.15.0      \n [94] curl_5.0.1              ggsignif_0.6.4          tibble_3.2.1           \n [97] modelsummary_1.4.3      stringi_1.7.12          parameters_0.21.3      \n[100] gdtools_0.3.3           lattice_0.21-8          fontBitstreamVera_0.1.1\n[103] nloptr_2.0.3            gbm_2.1.8.1             vctrs_0.6.3            \n[106] pillar_1.9.0            lifecycle_1.0.3         lmtest_0.9-40          \n[109] estimability_1.4.1      rlemon_0.2.1            cowplot_1.1.1          \n[112] insight_0.19.6          flextable_0.9.2         httpuv_1.6.11          \n[115] R6_2.5.1                promises_1.2.0.1        gridExtra_2.3          \n[118] codetools_0.2-19        boot_1.3-28.1           openssl_2.0.6          \n[121] withr_2.5.0             httpcode_0.3.0          multcomp_1.4-25        \n[124] bayestestR_0.13.1       parallel_4.2.3          rpart_4.1.19           \n[127] coda_0.19-4             minqa_1.2.5             rmarkdown_2.24         \n[130] carData_3.0-5           ggpubr_0.6.0            itertools_0.1-3        \n[133] numDeriv_2016.8-1.1     shiny_1.7.5"
  },
  {
    "objectID": "chapter_09.html#heterogeneous-treatment-effect",
    "href": "chapter_09.html#heterogeneous-treatment-effect",
    "title": "5  Dealing with missing data",
    "section": "5.4 Heterogeneous Treatment Effect",
    "text": "5.4 Heterogeneous Treatment Effect\nIn practice, the treatment effect is often not homogeneous among individuals. In this section, we will consider a heterogeneous treatment effect where there is effect modification by the confounding covariates. The way we generate the heterogeneous dataset is to segment the population according to the efficacy of each treatment (Iscore). We can see in the graph below, that for the previous dataset, there was no difference in treatment effect across the groups, but in this new dataset, we observe a marked difference across groups.\nLet’s start by simulating a new heterogeneous treatment dataset, using the same missing data generation process as before.\n\n\n\n\n\nWe use also the G computation to calculate the treatment effect, but this time in the main model, we take into account the interaction of the treatment and the covariates. In addition, we will estimate the conditional average treatment effect given the Iscore groups. \\[\\tau_{CATE}(x) =E(y^1-y^0|X=x)\\]\n\nhet.model &lt;- \"y ~ treatment*(gender + age + logPremedicalcost + prerelapseNum + prevDMTefficacy  + numSymptoms) + offset(log(years))\"\nresult_het &lt;- ATE_estimation (data = data_het, \n                              model = het.model,\n                              analysis = \"Full data\",\n                              variable = \"Iscore\")$ATE_var\n\n\n5.4.1 Methods for Dealing with Missing Data\n\n5.4.1.1 Complete Case Analysis\nAs before, we also proceed to calculate the ATE by employing the Complete Case Analysis.\n\nresult_CC &lt;- ATE_estimation (data = m_data_het[complete.cases(m_data_het),],\n                              model = het.model,\n                              analysis = \"Complete Case Analysis\",\n                              variable = \"Iscore\")$ATE_var\nresult_het &lt;- bind_rows(result_het,result_CC)\n\n\n\n5.4.1.2 Multiple Imputation by Treatment Group\nAs there is a treatment effect, the first imputation approach that we consider is to perform a separate imputation procedure on each of the treatment groups. This is an option that can be implemented in this case as the treatment group is complete, and the sample size is large enough in each treatment group . Here we use the same imputation models that we used before but removing the treatment variable.\n\nimp_het &lt;-  m_data_het %&gt;% mutate(logyears = log(years))\ndata_DMF  &lt;- subset(imp_het, treatment == \"DMF\")\ndata_TERI  &lt;- subset(imp_het, treatment == \"TERI\")\nimp_DMF &lt;- mice(data_DMF,  form = form_nt,method = method, m = 10, \n                maxit = 10, printFlag = FALSE)\nimp_TERI &lt;- mice(data_TERI,form = form_nt,method = method, m = 10, \n                 maxit = 10, printFlag = FALSE)\nimp_sep &lt;-  rbind(imp_DMF, imp_TERI)\n\n\n\n5.4.1.3 Parametric Multiple Imputation\nAs the main model includes treatment interaction terms, we need to include them also in the imputation model to avoid congeniality issues. So we modify the imputation models for each variable, including the treatment interaction terms and also the interaction of treatment and outcome as follows:\n\nform_iy &lt;- list(prevDMTefficacy ~ (y + treatment)*(age + gender + logPremedicalcost + numSymptoms + prerelapseNum + logyears) + y*treatment,\n                logPremedicalcost ~ (y + treatment)*(age + gender + prevDMTefficacy + numSymptoms + prerelapseNum + logyears) + y*treatment,\n                numSymptoms ~ (y + treatment)*(age + gender + logPremedicalcost + prevDMTefficacy + prerelapseNum + logyears) + y*treatment,\n                prerelapseNum ~ (y + treatment)*(age + gender + logPremedicalcost + prevDMTefficacy + numSymptoms + logyears) + y*treatment,\n                age ~ (y + treatment)*(prerelapseNum + gender + logPremedicalcost + prevDMTefficacy + numSymptoms + logyears) + y*treatment)\nform_iy &lt;- name.formulas(form_iy)\n\nThen we proceed to impute the dataset with mice with a parametric model using and not the interaction terms.\n\nimpdata_het &lt;-  m_data_het %&gt;% mutate(logyears = log(years))\nimp_y &lt;- mice(impdata_het, form = form_y, method = method, m = 10, \n              maxit = 10,  printFlag = FALSE)\nimp_iy &lt;- mice(impdata_het, form = form_iy, method = method, m = 10, \n               maxit = 10,  printFlag = FALSE)\n\n\n\n5.4.1.4 Non-parametric Multiple Imputation\nAnother option is to use imputation methods based on non-parametric approaches such as random forest, which are robust to the inclusion of interaction and quadratic terms. Here we use the “rf” method included in mice, but there are other available options as discussed by .\n\nimp_rf &lt;- mice(impdata_het, method = \"rf\", m = 10, maxit = 10, \n               ntree = 10, printFlag = FALSE)\n#plot(imp_rf)\n\nIt has also been proposed a new method based on XGBoost that seems also an option for data with interaction terms . Here it is required to calibrate the parameters to be included in the function and do an extra job to put it in the mice package format.\n\nlibrary(mixgb)\nparams &lt;- list(max_depth = 3, subsample = 0.7, nthread = 2)\ncv.results &lt;- mixgb_cv(data = impdata_het, nrounds = 100,\n                       xgb.params = params, verbose = FALSE)\n\nimp_gb &lt;- mixgb(data = impdata_het, m = 10, maxit = 10, nrounds = cv.results$best.nrounds)\ndata_gb &lt;- bind_rows(impdata_het,imp_gb, .id = '.imp')\ndata_gb$'.imp' &lt;- as.numeric(data_gb$'.imp') - 1\nimp_gb &lt;- mice::as.mids(data_gb)\n\nAfter checking the convergence of all the imputation methods,via traceplots, we proceed to estimate the treatment effect with the ATE_estimation() function, were it is required to specify the variable Iscore to evaluate the treatment effect on each group.\n\nimp_datasets &lt;- list(imp_sep,imp_y,imp_iy,imp_rf,imp_gb)\nn_analysis &lt;- c(\"MICE (separated)\",\"MICE (no interaction)\", \"MICE (interaction)\", \"Random forest\", \"MixGb\")\nresponse_imp &lt;- lapply(seq_along(imp_datasets), \\(i) \n                     ATE_estimation( data = imp_datasets[[i]],\n                                     model = het.model,\n                                     approach = \"within\",\n                                     variable = \"Iscore\",\n                                     analysis = n_analysis[[i]])$ATE_var)\nresponse_imp &lt;- do.call(rbind,response_imp)\nresult_het &lt;- bind_rows(result_het,response_imp)\n\n\n\n\n5.4.2 Results\n\nlibrary(ggplot2)\nresult_het$Iscore &lt;- as.factor(result_het$Iscore)\nlevels(result_het$Iscore) &lt;- c(\"High DMF\",\"Moderate DMF\", \"Neutral\", \n                               \"Moderate TERI\", \"High TERI\")\nresult_het$analysis = factor(result_het$analysis,\n                         levels = c(\"Full data\",\n                                    \"Complete Case Analysis\",\n                                    \"MICE (separated)\",\n                                    \"MICE (no interaction)\",\n                                    \"MICE (interaction)\",\n                                    \"Random forest\",\n                                    \"MixGb\"))\n\nggplot(result_het,aes(x = analysis, y = estimate, col = analysis)) +\n  geom_point(shape = 1,\n             size  = 1) +\n  geom_errorbar(aes(ymin  = conf.low,\n                    ymax  = conf.high),\n                width = 0.2,\n                size  = 0.5) +\n  see::scale_color_flat() + theme_light() + \n  theme(axis.title.x = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank(),\n        legend.position = \"bottom\") +\n  facet_wrap(\"Iscore\",ncol = 2, scales = \"free\")\n\n\n\n\nWe found that except for the complete case analysis, all the methods lead to unbiased results of the treatment effect across all the Iscore groups. However, it seems that the estimation of the MixGb method leads to estimations closer to the Full dataset ones."
  },
  {
    "objectID": "chapter_09.html#references",
    "href": "chapter_09.html#references",
    "title": "5  Dealing with missing data",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "chapter_07.html#simulation",
    "href": "chapter_07.html#simulation",
    "title": "4  Effect Modification Analysis within the Propensity score Framework",
    "section": "4.1 Simulation",
    "text": "4.1 Simulation\nFirst, we need to install the R package simcausal, which can be obtained from GitHub:\n\ndevtools::install_github('osofr/simcausal', build_vignettes = FALSE)\n\nWe will use the following data-generation model:\n\nrequire(simcausal)\nD &lt;- DAG.empty()\nD &lt;- D + \n  node(\"age\", distr = \"rnorm\", \n       mean = 2, sd = 4) + \n  node(\"gender\", distr = \"rbern\", \n       prob = plogis(4)) +\n  node(\"education\", distr = \"rbern\", \n       prob = plogis(3 + 5 * age)) +\n  node(\"diet\", distr = \"rbern\", \n       prob = plogis(1 - 3 * education)) +\n  node(\"income\", distr = \"rbern\", \n       prob = plogis(2 - 5 * education - 4 * age)) +\n  node(\"smoking\", distr = \"rbern\", \n       prob = plogis(1 + 1.2 * gender + 2 * age)) +\n  node(\"hypertension\", distr = \"rbern\", \n       prob = plogis(1 + log(3) * diet + \n                       log(1.3) * age + \n                       log(3.5) * smoking + \n                       log(0.5) * gender))\nDset &lt;- set.DAG(D)\n\nBelow is the diagram, with pink lines representing open backdoor path.\n\n\nusing the following vertex attributes: \n\n\nNAdarkbluenone100.50\n\n\nusing the following edge attributes: \n\n\nblack0.210.60.5\n\n\n\n\n\nWe can now generate an example dataset:\n\nObs.Data &lt;- sim(DAG = Dset, n = 50000, rndseed = 123)\nObs.Data$smoking &lt;- as.character(Obs.Data$smoking)\nObs.Data$income &lt;- as.factor(Obs.Data$income)\nObs.Data$income &lt;- relevel(Obs.Data$income, ref = \"1\")\n\nSample data from the hypothetical example of association between hypertension and smoking, where other variables such as income, age [centered], gender, education and diet also plays a role in the data generation process.\n\n\n\n\n\n\nage\ngender\neducation\ndiet\nincome\nsmoking\nhypertension\n\n\n\n\n34901\n12.29\n1\n1\n1\n0\n1\n1\n\n\n149\n10.40\n1\n1\n0\n0\n1\n1\n\n\n10060\n2.99\n1\n1\n0\n0\n1\n0\n\n\n22220\n-4.31\n0\n0\n0\n1\n0\n1\n\n\n9979\n-6.44\n0\n0\n0\n1\n0\n1"
  },
  {
    "objectID": "chapter_07.html#covariate-adjustment",
    "href": "chapter_07.html#covariate-adjustment",
    "title": "4  Effect Modification Analysis within the Propensity score Framework",
    "section": "4.2 Covariate adjustment",
    "text": "4.2 Covariate adjustment\n\n4.2.1 Interaction approach\nBelow, we estimate a logistic regression model to assess whether the effect of smoking (the exposure) on hypertension is modified by income levels. This model considers the following variables:\n\nOutcome: hypertension\nExposure variables: smoking and income\nConfounders: age and gender\n\n\nrequire(jtools)\n\nfit.w.em &lt;- glm(hypertension ~ smoking * income + age + gender, \n            family = binomial(link = \"logit\"), data = Obs.Data)\n\nresults.model &lt;- summ(fit.w.em, exp = TRUE)\n\n\n\n\n\n\n\nexp(Est.)\n2.5%\n97.5%\nz val.\np\n\n\n\n\n(Intercept)\n5.46\n4.37\n6.82\n14.97\n0.00\n\n\nsmoking1\n2.93\n2.60\n3.30\n17.69\n0.00\n\n\nincome0\n0.48\n0.41\n0.57\n-8.28\n0.00\n\n\nage\n1.29\n1.27\n1.31\n36.77\n0.00\n\n\ngender\n0.54\n0.43\n0.67\n-5.55\n0.00\n\n\nsmoking1:income0\n1.27\n1.04\n1.56\n2.33\n0.02\n\n\n\n\n\n\n\nResults indicate that the interaction between smoking status and income level is statistically significant (p = 0.02).\nIf we expand previous model to adjust for an additional confounder education, we have:\n\nfit.w.int &lt;- glm(hypertension ~ smoking * income + age + gender + education, \n                 family = binomial(link = \"logit\"), \n                 data = Obs.Data)\n\nresults.int.model &lt;- summ(fit.w.int, exp = TRUE)\n\n\n\n\n\n\n\nexp(Est.)\n2.5%\n97.5%\nz val.\np\n\n\n\n\n(Intercept)\n5.69\n4.56\n7.11\n15.31\n0.00\n\n\nsmoking1\n3.35\n2.95\n3.79\n18.85\n0.00\n\n\nincome0\n1.09\n0.85\n1.40\n0.68\n0.49\n\n\nage\n1.30\n1.28\n1.32\n37.32\n0.00\n\n\ngender\n0.54\n0.43\n0.67\n-5.58\n0.00\n\n\neducation\n0.42\n0.35\n0.51\n-8.87\n0.00\n\n\nsmoking1:income0\n1.10\n0.90\n1.35\n0.93\n0.35\n\n\n\n\n\n\n\nThe interaction term between income and smoking is no longer statistically significant (p = 0.35).\nWe can generate a summary report from aforementioned effect modification analysis.\n\nrequire(interactionR)\n\nem.object &lt;- interactionR(fit.w.em, \n                          exposure_names = c(\"income0\", \"smoking1\"), \n                          ci.type = \"mover\", ci.level = 0.95, \n                          em = TRUE, recode = FALSE)\n\nThe table below depicts the adjusted odds ratios for income levels (high = 0, and low = 1). The variables CI.ll and CI.ul depict the lower and upper limits of the 95 percent confidence intervals, OR11 = \\(OR_{A = 1, M = 1}\\) , OR10 = \\(OR_{A = 1}\\), OR01 = \\(OR_{M = 1}\\) and OR00 captures the reference.\n\n\n\n\nTable 4.1: Summary report from an interaction analysis when investigating association between two exposure variables (smoking and income) and hypertension.\n\n\nMeasures\nEstimates\nCI.ll\nCI.ul\n\n\n\n\nOR00\n1.00\nNA\nNA\n\n\nOR01\n2.93\n2.60\n3.30\n\n\nOR10\n0.48\n0.41\n0.57\n\n\nOR11\n1.80\n1.63\n1.98\n\n\nOR(smoking1 on outcome [income0==0]\n2.93\n2.60\n3.30\n\n\nOR(smoking1 on outcome [income0==1]\n3.72\n3.14\n4.41\n\n\nMultiplicative scale\n1.27\n1.04\n1.56\n\n\nRERI\n-0.61\n-0.98\n-0.29\n\n\n\n\n\n\n\n\nSimilarly, for the analysis adjusting for an additional confounder education, we have:\n\n\n\n\nTable 4.2: Summary report from an interaction analysis when investigating association between two exposure variables (smoking and income) and hypertension.\n\n\nMeasures\nEstimates\nCI.ll\nCI.ul\n\n\n\n\nOR00\n1.00\nNA\nNA\n\n\nOR01\n1.09\n0.85\n1.40\n\n\nOR10\n3.35\n2.95\n3.79\n\n\nOR11\n4.02\n3.29\n4.92\n\n\nOR(income0 on outcome [smoking1==0]\n1.09\n0.85\n1.40\n\n\nOR(income0 on outcome [smoking1==1]\n1.20\n1.00\n1.45\n\n\nOR(smoking1 on outcome [income0==0]\n3.35\n2.95\n3.79\n\n\nOR(smoking1 on outcome [income0==1]\n3.69\n3.11\n4.37\n\n\nMultiplicative scale\n1.10\n0.90\n1.35\n\n\nRERI\n0.59\n0.03\n1.27\n\n\nAP\n0.15\n0.00\n0.26\n\n\nSI\n1.24\n1.01\n1.53\n\n\n\n\n\n\n\n\n\n# test run with additive model\nObs.Data$smoking &lt;- as.numeric(as.character(Obs.Data$smoking))\nObs.Data$income &lt;- as.numeric(as.character(Obs.Data$income))\nfit.w.int.add &lt;- glm(hypertension ~ smoking * income + age + gender + education, \n                     family = gaussian(link = \"identity\"), data = Obs.Data)\nsim_slopes(fit.w.int.add, pred = smoking, modx = income,\n           exp = TRUE, robust = TRUE,\n           confint = TRUE, data = Obs.Dat)\n\nJOHNSON-NEYMAN INTERVAL \n\nWhen income is INSIDE the interval [-3.27, 16.87], the slope of smoking is\np &lt; .05.\n\nNote: The range of observed values of income is [0.00, 1.00]\n\nSIMPLE SLOPES ANALYSIS \n\nSlope of smoking when income = 0.00 (0): \n\n  Est.   S.E.   2.5%   97.5%   t val.      p\n------ ------ ------ ------- -------- ------\n  0.25   0.02   1.24    1.34    12.76   0.00\n\nSlope of smoking when income = 1.00 (1): \n\n  Est.   S.E.   2.5%   97.5%   t val.      p\n------ ------ ------ ------- -------- ------\n  0.28   0.01   1.30    1.34    34.53   0.00\n\n\n\n\n4.2.2 Stratification\nThis approach involves estimating a regression model in different strata of the discrete effect modifier income:\n\n# Estimate the prognostic effect of smoking in low income individuals\nfit.income1 &lt;- glm(hypertension ~ smoking + age + gender, \n            family = binomial(link = \"logit\"), \n            data = subset(Obs.Data, income == 1))\n\n# Estimate the prognostic effect of smoking in high income individuals\nfit.income0 &lt;- glm(hypertension ~ smoking + age + gender, \n            family = binomial(link = \"logit\"), \n            data = subset(Obs.Data, income == 0))\n\nThe table below summarizes the adjusted odds ratios for smoking across the different income levels (low = 1, and high = 0) as obtained using the stratified approach.\n\n\n\n\n\nValue of income\nEstimate\n2.5 %\n97.5 %\nz value\np value\n\n\n\n\n1\n3.07\n2.71\n3.47\n17.65\n0\n\n\n0\n3.59\n3.02\n4.26\n14.57\n0\n\n\n\n\n\n\n\nNote that we can obtain the same results by estimating a regression model with an interaction term between the modifier and all covariates:\n\nfit.all.int &lt;- glm(hypertension ~ income * (smoking + age + gender), \n                   family = binomial(link = \"logit\"), data = Obs.Data)\n\n# Odds ratio for smoking in individuals with low income \nexp(coef(fit.all.int)[\"smoking\"])\n\nsmoking \n3.59026 \n\n# Odds ratio for smoking in individuals with high income\nexp(coef(fit.all.int)[\"smoking\"] + coef(fit.all.int)[\"income:smoking\"])\n\n smoking \n3.066878"
  },
  {
    "objectID": "chapter_07.html#propensity-score-matching",
    "href": "chapter_07.html#propensity-score-matching",
    "title": "4  Effect Modification Analysis within the Propensity score Framework",
    "section": "4.3 Propensity score matching",
    "text": "4.3 Propensity score matching\n\n4.3.1 Stratification with exact matching within subgroups\nWe simulate another example dataset using aforementioned DAG, but restrict the sample size to 5000 individuals to reduce computational burden.\n\nset.seed(123)\nObs.Data &lt;- sim(DAG = Dset, n = 5000, rndseed = 123)\n\nWe first estimate the propensity of smoking in the high-income group (income == 0):\n\nrequire(MatchIt)\n\nmatch.income.0 &lt;- matchit(smoking ~ age + gender, \n                          data = subset(Obs.Data, income == 0),\n                          method = \"full\", distance = \"glm\", link = \"logit\")\ndata.income.0 &lt;- match.data(match.income.0)\n\nBelow, we draw a sample from the high-income group based on the hypothetical example of an association between hypertension and smoking. Here age [centered], gender, education, and diet are covariates.\n\n\n            age gender education diet income smoking hypertension  distance\n657   6.0810120      0         1    1      0       1            1 0.9999874\n4932  1.6109860      1         1    0      0       1            0 0.9943155\n252  -0.2475055      1         1    1      0       0            1 0.8525107\n2693 -0.2511048      1         1    0      0       1            1 0.8516785\n1646 -0.2836155      1         0    1      0       1            1 0.8439843\n        weights subclass\n657  1.00000000       36\n4932 1.00000000       50\n252  0.03296089       25\n2693 1.00000000       25\n1646 1.00000000        4\n\n\nNow, we do the same for the low-income group (income == 1):\n\nmatch.income.1 &lt;- matchit(smoking ~ age + gender, \n                          data = subset(Obs.Data, income == 1),\n                          method = \"full\", distance = \"glm\", link = \"logit\")\ndata.income.1 &lt;- match.data(match.income.1)\n\nWe estimated the exposure effect from a weighted outcome model for the matched data. While the weights are essential for estimating the point estimate from the outcome model, the subclass variable assists in calculating the robust variance of the exposure effect estimate.\n\n# Treatment effect estimation\nfit.income.0 &lt;- glm(hypertension ~ smoking + age + gender, \n                   data = data.income.0, weights = weights,\n                   family = binomial(\"logit\"))\nfit.income.1 &lt;- glm(hypertension ~ smoking + age + gender, \n                   data = data.income.1, weights = weights,\n                   family = binomial(\"logit\"))\n# Robust variance calculation\nfit.nexp.adj.res1 &lt;- summ(fit.income.1,  \n                          robust = TRUE,\n                          cluster = \"subclass\",\n                          confint = TRUE)\nfit.nexp.adj.res0 &lt;- summ(fit.income.0, \n                          robust = TRUE,\n                          cluster = \"subclass\",\n                          confint = TRUE)\n\n\n\n\n\nTable 4.3: Subgroup-specific treatment effect estimates (expressed in log-OR) from the hypothetical example using the stratified approach.\n\n\nValue of income\nEst.\n2.5%\n97.5%\nz val.\np\n\n\n\n\n0\n3.74\n-37.58\n45.06\n0.18\n0.86\n\n\n1\n1.39\n0.94\n1.85\n6.04\n0.00\n\n\n\n\n\n\n\n\n\n\n4.3.2 Joint approach without exact matching within subgroups\nHere, entire cohort data is used to estimate the propensity scores, and the effect modifier income is considered as a covariate in the propensity score model:\n\nps.formula &lt;- as.formula(\"smoking ~ age + gender + income\")\nmatch.obj.j &lt;- matchit(ps.formula, data = Obs.Data,\n                      method = \"full\", \n                      distance = \"glm\",\n                      link = \"logit\")\nmatch.data.j &lt;- match.data(match.obj.j)\n\n\nfit.joint.no.exact &lt;- glm(hypertension ~ smoking*income + age + gender, \n                          data = match.data.j, \n                          weights = weights,\n                          family = binomial(\"logit\"))\nrequire(interactions)\nnem.nexp.adj.res &lt;- sim_slopes(fit.joint.no.exact, \n                               pred = smoking, \n                               modx = income,\n                               robust = \"HC1\", \n                               cluster = \"subclass\",\n                               johnson_neyman = TRUE, \n                               confint = TRUE,\n                               data = match.data.j)\n\n\n\n\n\nTable 4.4: Subgroup-specific treatment effect estimates (expressed in log-OR) from the hypothetical example using the joint approach.\n\n\nValue of income\nEst.\nS.E.\n2.5%\n97.5%\nz val.\np\n\n\n\n\n0\n3.85\n1.00\n1.89\n5.82\n3.84\n0\n\n\n1\n1.40\n0.28\n0.85\n1.95\n4.99\n0\n\n\n\n\n\n\n\n\n\n\n4.3.3 Joint approach with exact matching within subgroups\nWe specify the moderator variable’s name in the exact argument of the matchit function.\n\nps.formula.no.mod &lt;- as.formula(\"smoking ~ age + gender\")\nmatch.obj.js &lt;- matchit(ps.formula.no.mod, data = Obs.Data,\n                        method = \"full\", distance = \"glm\",link = \"logit\",\n                        exact = \"income\")\nmatch.data.js &lt;- match.data(match.obj.js)\nfit.joint.exact &lt;- glm(hypertension ~ smoking*income + age + gender, \n                       data = match.data.js, weights = weights,\n                       family = binomial(\"logit\"))\njs.nexp.adj.res &lt;- sim_slopes(fit.joint.exact, \n                              pred = smoking, modx = income,\n                              robust = \"HC1\", cluster = \"subclass\",\n                              johnson_neyman = FALSE, confint = TRUE,\n                              data = match.data.js)\n\n\n\n\n\nTable 4.5: Subgroup-specific exposure effect estimates (expressed in log-OR) from the hypothetical example using the Joint model, separate matching approach.\n\n\nValue of income\nEst.\nS.E.\n2.5%\n97.5%\nz val.\np\n\n\n\n\n0\n3.89\n1.01\n1.92\n5.87\n3.87\n0\n\n\n1\n1.38\n0.28\n0.84\n1.93\n4.95\n0\n\n\n\n\n\n\n\n\n\n\n4.3.4 Interaction approach without exact matching within subgroups\nAnalysts incorporate relevant moderator-covariate interactions into the propensity score model that align with biological plausibility. For instance, in the case study we considered an interaction between age (a covariate) and income (a moderator), but did not include other interactions terms.\n\nps.formula.with.int &lt;- formula(\"smoking ~ age*income + gender\")\nmatch.obj.i &lt;- matchit(ps.formula.with.int, data = Obs.Data,\n                       method = \"full\", distance = \"glm\",link = \"logit\")\nmatch.data.i &lt;- match.data(match.obj.i)\nfit.int.no.exact &lt;- glm(hypertension ~ smoking*income + age + gender, \n                        data = match.data.i, weights = weights,\n                        family = binomial(\"logit\"))\ni.nexp.adj.res &lt;- sim_slopes(fit.int.no.exact, \n                             pred = smoking, modx = income,\n                             robust = \"HC1\", cluster = \"subclass\",\n                             johnson_neyman = FALSE, confint = TRUE,\n                             data = match.data.i)\n\n\n\n\n\nTable 4.6: Subgroup-specific exposure effect estimates (expressed in log-OR) from the hypothetical example using the interaction approach.\n\n\nValue of income\nEst.\nS.E.\n2.5%\n97.5%\nz val.\np\n\n\n\n\n0\n3.87\n1.00\n1.90\n5.83\n3.86\n0\n\n\n1\n1.39\n0.28\n0.84\n1.94\n4.95\n0\n\n\n\n\n\n\n\n\n\n\n4.3.5 Interaction approach with exact matching within subgroups\nThis method bears resemblance to the interaction approach for propensity score estimation. However, when it comes to matching, researchers match within each moderator subgroup.\n\nmatch.obj.is &lt;- matchit(ps.formula.with.int, data = Obs.Data,\n                      method = \"full\", distance = \"glm\",link = \"logit\",\n                      exact = \"income\")\nmatch.data.is &lt;- match.data(match.obj.is)\nfit.int.exact &lt;- glm(hypertension ~ smoking*income + age + gender, \n                     data = match.data.is, weights = weights,\n                     family = binomial(\"logit\"))\nis.nexp.adj.res &lt;- sim_slopes(fit.int.exact, \n                              pred = smoking, modx = income,\n                              robust = \"HC1\", cluster = \"subclass\",\n                              johnson_neyman = FALSE, confint = TRUE,\n                              data = match.data.is)\n\n\n\n\n\nTable 4.7: Subgroup-specific exposure effect estimates (expressed in log-OR) from the hypothetical example using the interaction model, separate matching approach.\n\n\nValue of income\nEst.\nS.E.\n2.5%\n97.5%\nz val.\np\n\n\n\n\n0\n3.86\n1.00\n1.90\n5.83\n3.85\n0\n\n\n1\n1.40\n0.28\n0.85\n1.95\n4.99\n0"
  },
  {
    "objectID": "chapter_07.html#propensity-score-weighting",
    "href": "chapter_07.html#propensity-score-weighting",
    "title": "4  Effect Modification Analysis within the Propensity score Framework",
    "section": "4.4 Propensity Score Weighting",
    "text": "4.4 Propensity Score Weighting\n\n4.4.1 Common model\nThis approach adds confounder-moderator interactions in the common weight model.\n\nrequire(WeightIt)\nW.out &lt;- weightit(ps.formula.with.int, \n                  data = Obs.Data,\n                  method = \"ps\", \n                  estimand = \"ATT\")\nrequire(survey)\nd.w &lt;- svydesign(~1, weights = W.out$weights, data = Obs.Data)\nfit2w &lt;- svyglm(hypertension ~ smoking*income, design = d.w,\n                family = binomial(\"logit\"))\nw.nexp.adj.res &lt;- sim_slopes(fit2w, pred = smoking, modx = income, \n                             confint = TRUE)\n\n\n\n\n\nTable 4.8: Subgroup-specific exposure effect estimates (expressed in log-OR) from the hypothetical example using the weighting approach.\n\n\nValue of income\nEst.\nS.E.\n2.5%\n97.5%\nt val.\np\n\n\n\n\n0\n2.66\n0.63\n1.42\n3.89\n4.23\n0\n\n\n1\n1.32\n0.25\n0.83\n1.82\n5.24\n0\n\n\n\n\n\n\n\n\nWe can adjust previous analysis model to adopt stabilized weights for the propensity score (stabilize = TRUE):\n\nW.out.st &lt;- weightit(ps.formula.with.int, data = Obs.Data,\n                     method = \"ps\", \n                     estimand = \"ATT\", \n                     stabilize = TRUE)\nd.sw &lt;- svydesign(~1, weights = W.out.st$weights, data = Obs.Data)\nfit2sw &lt;- svyglm(hypertension ~ smoking*income + age + gender, \n                  design = d.sw,\n                  family = binomial(\"logit\"))\nws.nexp.adj.res &lt;- sim_slopes(fit2sw, \n                              pred = smoking, modx = income, \n                              confint = TRUE)\n\n\n\n\n\nTable 4.9: Subgroup-specific exposure effect estimates (expressed in log-OR) from the hypothetical example using stabilized propensity score weights.\n\n\nValue of income\nEst.\nS.E.\n2.5%\n97.5%\nt val.\np\n\n\n\n\n0\n2.27\n0.73\n0.84\n3.69\n3.12\n0\n\n\n1\n1.32\n0.25\n0.83\n1.82\n5.23\n0\n\n\n\n\n\n\n\n\n\n\n4.4.2 Separate models\nPropensity score weighting approach with weights estimated separately from each subgroup:\n\nps.formula.with.no.int &lt;- formula(\"smoking ~ age + gender\")\nW.out1 &lt;- weightit(ps.formula.with.no.int, \n                   data = subset(Obs.Data, income == 1),\n                   method = \"ps\", \n                   estimand = \"ATT\")\ntrimmed.weight.1.percent1 &lt;- trim(W.out1$weights, \n                                  at = 1, lower = TRUE)\n\n\n\n\n\nTable 4.10: Weight summaries before and after truncation.\n\n\nWeight\nMin.\n1st Qu.\nMedian\nMean\n3rd Qu.\nMax.\n\n\n\n\nRaw weights\n0\n0.01\n0.11\n0.45\n1\n11.69\n\n\n1% truncated weights\n0\n0.01\n0.11\n0.44\n1\n7.61\n\n\n\n\n\n\n\n\n\n# Outcome model for income = 1\nd.w1 &lt;- svydesign(~1, weights = trimmed.weight.1.percent1, \n                  data = subset(Obs.Data, income == 1))\nfit2unadj1 &lt;- svyglm(hypertension ~ smoking, design = d.w1,\n                     family = binomial(\"logit\"))\n\n# weight model for income = 0\nW.out0 &lt;- weightit(ps.formula, data = subset(Obs.Data, income == 0),\n                  method = \"ps\", estimand = \"ATT\")\ntrimmed.weight.1.percent0 &lt;- trim(W.out0$weights, at = 1, lower = TRUE)\n\n# Outcome model for income = 0\nd.w0 &lt;- svydesign(~1, weights = trimmed.weight.1.percent0, \n                  data = subset(Obs.Data, income == 0))\nfit2unadj0 &lt;- svyglm(hypertension ~ smoking, design = d.w0,\n                     family = binomial(\"logit\"))\n\nfit.exp.adj.res1 &lt;- summ(fit2unadj1, confint = TRUE)\nfit.exp.adj.res0 &lt;- summ(fit2unadj0, confint = TRUE)\n\n\n\n\n\nTable 4.11: Subgroup-specific exposure effect estimates (expressed in log-OR) from the hypothetical example using the propensity score weighting approach (Separate weight models).\n\n\nValue of income\nEst.\n2.5%\n97.5%\nt val.\np\n\n\n\n\n0\n2.21\n1.27\n3.15\n4.60\n0\n\n\n1\n1.34\n0.85\n1.83\n5.36\n0\n\n\n\n\n\n\n\n\n\n\n4.4.3 Weights from the subgroup balancing propensity scores\nSubgroup balancing propensity scores for propensity score weighting:\n\nw.out &lt;- weightit(smoking ~ age + gender + income, \n                data = Obs.Data,\n                method = \"ps\", estimand = \"ATT\")\nw.out.sb &lt;- sbps(w.out, moderator = \"income\")\nd.w.sb &lt;- svydesign(~1, weights = w.out.sb$weights, data = Obs.Data)\nfit2unadj.sb &lt;- svyglm(hypertension ~ smoking*income, design = d.w.sb,\n                       family = binomial(\"logit\"))\nsb.w.nexp.adj.res &lt;- sim_slopes(fit2unadj.sb, \n                              pred = smoking, \n                              modx = income, \n                              confint = TRUE,\n                              johnson_neyman = FALSE,)\n\n\n\n\n\nTable 4.12: Subgroup-specific exposure effect estimates (expressed in log-OR) from the hypothetical example using the subgroup balancing weighting approach.\n\n\nValue of income\nEst.\nS.E.\n2.5%\n97.5%\nt val.\np\n\n\n\n\n0\n2.68\n0.64\n1.44\n3.92\n4.22\n0\n\n\n1\n1.32\n0.25\n0.82\n1.82\n5.22\n0"
  },
  {
    "objectID": "chapter_07.html#covariate-adjustment-for-the-propensity-score",
    "href": "chapter_07.html#covariate-adjustment-for-the-propensity-score",
    "title": "4  Effect Modification Analysis within the Propensity score Framework",
    "section": "4.5 Covariate adjustment for the propensity score",
    "text": "4.5 Covariate adjustment for the propensity score\n\n4.5.1 As continuous covariate\nAn implementation of propensity scores as a continuous covariate in the outcome model:\n\n# Separate models for each subgroup\n\n# For subgroup income = 1 \nObs.Data$ps[Obs.Data$income == 1] &lt;- glm(ps.formula, \n                                         data = subset(Obs.Data, income == 1), \n                                         family = \"binomial\")$fitted.values\nfit2adj1 &lt;- glm(hypertension ~ smoking + age + gender, \n                family = binomial(\"logit\"), \n                data = subset(Obs.Data, income == 1))\n\n# For subgroup income = 0\nObs.Data$ps[Obs.Data$income == 0] &lt;- glm(ps.formula, \n                                         data = subset(Obs.Data, income == 0), \n                                         family = \"binomial\")$fitted.values\nfit2adj0 &lt;- glm(hypertension ~ smoking + age + gender, \n                family = binomial(\"logit\"), \n                data = subset(Obs.Data, income == 0))\n\nfit.nexp.adj.res1 &lt;- summ(fit2adj1, robust = TRUE, confint = TRUE)\nfit.nexp.adj.res0 &lt;- summ(fit2adj0, robust = TRUE, confint = TRUE)\n\n\n\n\n\nTable 4.13: Subgroup-specific exposure effect estimates (expressed in log-OR) from the hypothetical example using Propensity Score as a covariate adjustment approach (considering separate models for each subgroup).\n\n\nValue of income\nEst.\n2.5%\n97.5%\nz val.\np\n\n\n\n\n0\n1.16\n0.56\n1.75\n3.83\n0\n\n\n1\n1.37\n0.96\n1.77\n6.61\n0\n\n\n\n\n\n\n\n\n\n# Common model\nObs.Data$ps &lt;- glm(ps.formula.with.int, data = Obs.Data,\n                       family = \"binomial\")$fitted.values\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nfit2adjc &lt;- glm(hypertension ~ smoking*income + age + gender + ps, \n                family = binomial(\"logit\"), \n                data = Obs.Data)\nc.nexp.adj.res &lt;- sim_slopes(fit2adjc,\n                             pred = smoking, modx = income,\n                             confint = TRUE,\n                             data = Obs.Data)\n\n\n\n\n\nTable 4.14: Subgroup-specific exposure effect estimates (expressed in log-OR) from the hypothetical example using Propensity Score as a covariate adjustment approach (considering a common model).\n\n\nValue of income\nEst.\nS.E.\n2.5%\n97.5%\nz val.\np\n\n\n\n\n0\n1.17\n0.29\n0.61\n1.74\n4.07\n0\n\n\n1\n1.43\n0.23\n0.98\n1.87\n6.30\n0\n\n\n\n\n\n\n\n\n\n\n4.5.2 As quantiles\nThe propensity scores as a categorical covariate, broken by quintiles, in the outcome model.\n\nObs.Data$ps &lt;- glm(ps.formula.with.int, \n                   data = Obs.Data, \n                   family = \"binomial\")$fitted.values\nquintiles &lt;- quantile(Obs.Data$ps, \n                      prob = seq(from = 0, to = 1, by = 0.2), \n                      na.rm = T)\nObs.Data$psq &lt;- cut(Obs.Data$ps, breaks = quintiles, \n                   labels = seq(1,5), include.lowest = T)\nObs.Data$psq &lt;- as.factor(Obs.Data$psq)\n\nfit2adjq &lt;- glm(hypertension ~ (smoking*psq)*income, \n                family = binomial(\"logit\"),\n                data = Obs.Data)\ncq.nexp.adj.res &lt;- sim_slopes(fit2adjq, \n                              pred = smoking, \n                              modx = income, \n                              confint = TRUE,\n                              data = Obs.Data)\n\n\n\n\n\nTable 4.15: Subgroup-specific exposure effect estimates (expressed in log-OR) from the hypothetical example using Propensity Score as a covariate adjustment approach (as quintiles).\n\n\nValue of income\nEst.\nS.E.\n2.5%\n97.5%\nz val.\np\n\n\n\n\n0\n3.08\n0.63\n1.85\n4.32\n4.91\n0\n\n\n1\n2.60\n0.47\n1.68\n3.51\n5.56\n0"
  },
  {
    "objectID": "chapter_07.html#propensity-score-stratification",
    "href": "chapter_07.html#propensity-score-stratification",
    "title": "4  Effect Modification Analysis within the Propensity score Framework",
    "section": "4.6 Propensity Score Stratification",
    "text": "4.6 Propensity Score Stratification\nHere is an implementation of propensity score stratification approach by using the marginal mean weighting through stratification (MMWS):\n\nmatch.obj &lt;- matchit(ps.formula, data = Obs.Data,\n                      method = \"subclass\", subclass = 3, \n                      estimand = \"ATT\", min.n = 10)\ndata.subclass &lt;- match.data(match.obj)\nsubclass.fit &lt;- glm(hypertension ~ smoking*income, family = binomial(\"logit\"),\n              data = data.subclass,\n              weights = weights)\nsubclass.nexp.adj.res &lt;- sim_slopes(subclass.fit, \n                                    pred = smoking, \n                                    modx = income, \n                                    confint = TRUE,\n                                    robust = \"HC3\",\n                                    johnson_neyman = FALSE,\n                                    data = data.subclass)\n\n\n\n\n\nTable 4.16: Subgroup-specific exposure effect estimates (expressed in log-OR) from the hypothetical example using propensity score stratification approach.\n\n\nValue of income\nEst.\nS.E.\n2.5%\n97.5%\nz val.\np\n\n\n\n\n0\n2.21\n0.47\n1.29\n3.13\n4.71\n0\n\n\n1\n1.89\n0.19\n1.51\n2.26\n9.78\n0"
  },
  {
    "objectID": "chapter_07.html#summary",
    "href": "chapter_07.html#summary",
    "title": "4  Effect Modification Analysis within the Propensity score Framework",
    "section": "4.7 Summary",
    "text": "4.7 Summary\nThe marginal odds ratios for smoking are summarized below"
  },
  {
    "objectID": "chapter_07.html#version-info",
    "href": "chapter_07.html#version-info",
    "title": "4  Effect Modification Analysis within the Propensity score Framework",
    "section": "Version info",
    "text": "Version info\nThis chapter was rendered using the following version of R and its packages:\n\n\nR version 4.2.3 (2023-03-15 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=Dutch_Netherlands.utf8  LC_CTYPE=Dutch_Netherlands.utf8   \n[3] LC_MONETARY=Dutch_Netherlands.utf8 LC_NUMERIC=C                      \n[5] LC_TIME=Dutch_Netherlands.utf8    \n\nattached base packages:\n[1] grid      stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] interactionR_0.1.6 simcausal_0.5.6    scales_1.2.1       ggplot2_3.4.2     \n [5] xtable_1.8-4       dplyr_1.1.2        kableExtra_1.3.4   knitr_1.43        \n [9] cowplot_1.1.1      readstata13_0.10.1 survey_4.2-1       survival_3.5-5    \n[13] Matrix_1.5-4.1     broom_1.0.5        MatchIt_4.5.4      interactions_1.1.5\n[17] jtools_2.2.1       sandwich_3.0-2     lmtest_0.9-40      zoo_1.8-12        \n[21] optmatch_0.10.6    WeightIt_0.14.2    cobalt_4.5.1       table1_1.4.3      \n\nloaded via a namespace (and not attached):\n [1] fontquiver_0.2.1        webshot_0.5.4           httr_1.4.6             \n [4] tools_4.2.3             backports_1.4.1         utf8_1.2.3             \n [7] R6_2.5.1                DBI_1.1.3               colorspace_2.1-0       \n[10] withr_2.5.0             tidyselect_1.2.0        curl_5.0.1             \n[13] compiler_4.2.3          textshaping_0.3.6       cli_3.6.1              \n[16] rvest_1.0.3             expm_0.999-7            flextable_0.9.2        \n[19] xml2_1.3.4              officer_0.6.2           fontBitstreamVera_0.1.1\n[22] labeling_0.4.2          mvtnorm_1.2-2           askpass_1.1            \n[25] systemfonts_1.0.4       stringr_1.5.0           digest_0.6.31          \n[28] rmarkdown_2.22          svglite_2.1.1           gfonts_0.2.0           \n[31] pkgconfig_2.0.3         htmltools_0.5.5         fastmap_1.1.1          \n[34] highr_0.10              htmlwidgets_1.6.2       rlang_1.1.1            \n[37] rstudioapi_0.14         httpcode_0.3.0          shiny_1.7.4            \n[40] farver_2.1.1            generics_0.1.3          jsonlite_1.8.5         \n[43] car_3.1-2               zip_2.3.0               magrittr_2.0.3         \n[46] Formula_1.2-5           Rcpp_1.0.10             munsell_0.5.0          \n[49] fansi_1.0.4             abind_1.4-5             gdtools_0.3.3          \n[52] lifecycle_1.0.3         chk_0.9.0               stringi_1.7.12         \n[55] yaml_2.3.7              carData_3.0-5           promises_1.2.0.1       \n[58] crayon_1.5.2            lattice_0.21-8          splines_4.2.3          \n[61] pander_0.6.5            pillar_1.9.0            uuid_1.1-0             \n[64] igraph_1.5.0            codetools_0.2-19        crul_1.4.0             \n[67] glue_1.6.2              evaluate_0.21           msm_1.7                \n[70] mitools_2.4             fontLiberation_0.1.0    data.table_1.14.8      \n[73] vctrs_0.6.3             httpuv_1.6.11           openssl_2.0.6          \n[76] gtable_0.3.3            purrr_1.0.1             tidyr_1.3.0            \n[79] assertthat_0.2.1        xfun_0.39               mime_0.12              \n[82] later_1.3.1             ragg_1.2.5              viridisLite_0.4.2      \n[85] tibble_3.2.1            ellipsis_0.3.2          rlemon_0.2.1"
  },
  {
    "objectID": "chapter_10.html#introduction",
    "href": "chapter_10.html#introduction",
    "title": "6  Systematic review and meta-analysis of Real-World Evidence",
    "section": "6.1 Introduction",
    "text": "6.1 Introduction\nWe first load the required packages\n\nlibrary(dplyr)\nlibrary(gemtc)\nlibrary(netmeta)"
  },
  {
    "objectID": "chapter_10.html#pairwise-meta-analysis-of-clinical-trials",
    "href": "chapter_10.html#pairwise-meta-analysis-of-clinical-trials",
    "title": "6  Systematic review and meta-analysis of Real-World Evidence",
    "section": "6.2 Pairwise meta-analysis of clinical trials",
    "text": "6.2 Pairwise meta-analysis of clinical trials\n\n6.2.1 Toculizumab for coronavirus disease 2019\nIn this example, we consider the results from a systematic literature review of clinical trials investigating any pharmacological in hosptialized patients with coronavirus disease 2019 (Selvarajan et al. 2022). A total of 23 randomized controlled trials were included and studied seven different interventions: dexamethasone, remdesivir, tocilizumab, hydroxychloroquine, combination of lopinavir/ritonavir, favipiravir and interferon-β. We here focus on the synthesis of 7 trials that comparted toculizumab (TOCI) to standard care (STD) and collected mortality data.\n\n\n\n\n\nstudlab\ntreat1\ntreat2\nevent1\nn1\nevent2\nn2\n\n\n\n\nHermine et al\nTOCI\nSTD\n7\n63\n8\n67\n\n\nRosas et al\nTOCI\nSTD\n58\n294\n28\n144\n\n\nSalama et al\nTOCI\nSTD\n26\n249\n11\n128\n\n\nSalvarini et al\nTOCI\nSTD\n2\n60\n1\n66\n\n\nStone et al\nTOCI\nSTD\n9\n161\n3\n82\n\n\nVeiga et al\nTOCI\nSTD\n14\n65\n6\n64\n\n\n\n\n\n\n\nWe now conduct a pairwise meta-analysis to assess the pooled effect of tocilizumab versus standard care. For each study, the log odds ratio and corresponding standard error is derived after which the corresponding estimates are pooled using the Mantel-Haenszel method.\n\nresults.TOCI &lt;- metabin(event1,n1,event2,n2,studlab,data=tocilizumab,\n                        sm=\"OR\",main=\"tocilizumab vs standard care\", \n                        prediction=TRUE)\nforest(results.TOCI, leftcols = \"studlab\", rightcols = \"effect.ci\")\n\n\n\n\nAltough a random effects meta-analysis was conducted, no heterogeneity was found (\\(\\tau\\)=0, with a 95% confidence interval ranging from 0 to 0.85).\n\n\n6.2.2 Remdesivir for coronavirus disease 2019\nIn aforementioned example, a total of 4 trials compared remdesivir to standard care:"
  },
  {
    "objectID": "chapter_10.html#network-meta-analysis-of-clinical-trials",
    "href": "chapter_10.html#network-meta-analysis-of-clinical-trials",
    "title": "6  Systematic review and meta-analysis of Real-World Evidence",
    "section": "6.3 Network meta-analysis of clinical trials",
    "text": "6.3 Network meta-analysis of clinical trials\nWe here use the R packages netmeta for conducting a frequentist network meta-analysis. A detailed tutorial on the use of netmeta is available from the book Doing Meta-Analysis with R: A Hands-On Guide.\n\n6.3.1 Interventions for coronavirus disease 2019\nWe here consider data from a study which aimed to assess the comparative effectiveness of remdesivir and tocilizumab for reducing mortality in hospitalised COVID-19 patients. 80 trials were identified from two published network meta-analyses (Selvarajan et al. 2022), (Siemieniuk et al. 2020), a living COVID-19 trial database (COVID-NMA Initiative) [Covid-NMA.com], and a clinical trial database [clinicaltrials.gov]. Trials were included in this study if the patient population included hospitalized COVID-19 patients, active treatment was remdesivir or tocilizumab, comparator treatment was placebo or standard care, short-term mortality data was available, and the trial was published. 21 trials were included. For included trials, a risk of bias score was extracted from the COVID-NMA Initiative.\n\n\n\n\n\nstudlab\ntreat1\ntreat2\nevent1\nn1\nevent2\nn2\n\n\n\n\nAder\nREM\nSTD\n34\n414\n37\n418\n\n\nBeigel (ACTT-1)\nREM\nSTD\n59\n541\n77\n521\n\n\nBroman\nTOCI\nSTD\n1\n57\n0\n29\n\n\nCriner\nREM\nSTD\n4\n384\n4\n200\n\n\nDeclerq (COV-AID)\nTOCI\nSTD\n10\n81\n9\n74\n\n\nGordon (REMAP-CAP)\nTOCI\nSTD\n83\n353\n116\n358\n\n\nHermine (CORIMUNO)\nTOCI\nSTD\n7\n63\n8\n67\n\n\nHorby (RECOVERY)\nTOCI\nSTD\n621\n2022\n729\n2094\n\n\nIslam\nREM\nSTD\n0\n30\n0\n30\n\n\nMahajan\nREM\nSTD\n5\n34\n3\n36\n\n\nPan (WHO Solidarity)\nREM\nSTD\n602\n4146\n643\n4129\n\n\nRosas (COVACTA)\nTOCI\nSTD\n58\n294\n28\n144\n\n\nRutgers\nTOCI\nSTD\n21\n174\n34\n180\n\n\nSalama (EMPACTA)\nTOCI\nSTD\n26\n249\n11\n128\n\n\nSalvarani\nTOCI\nSTD\n2\n60\n1\n63\n\n\nSoin (COVINTOC)\nTOCI\nSTD\n11\n92\n15\n88\n\n\nSpinner\nREM\nSTD\n5\n384\n4\n200\n\n\nStone (BACC-BAY)\nTOCI\nSTD\n9\n161\n4\n82\n\n\nTalaschian\nTOCI\nSTD\n5\n17\n4\n19\n\n\nVeiga (TOCIBRAS)\nTOCI\nSTD\n14\n65\n6\n64\n\n\nWang\nREM\nSTD\n22\n158\n10\n78\n\n\n\n\n\n\n\nThe corresponding network is displayed below:\n\n\n\n\n\nEvidence network of the 21 coronavirus-19 trials\n\n\n\n\nWe use the following command to calculate the log odds ratios and corresponding standard errors for each study:\n\ncovid &lt;- pairwise(treat = treat, event = event, n = n, studlab = studlab, sm = \"OR\")\nhead(covid)\n\n\n\n\n\n\nTE\nseTE\nstudlab\ntreat1\ntreat2\nevent1\nn1\nevent2\nn2\nincr\nallstudies\n\n\n\n\n-0.0819293\n0.2483849\nAder\nREM\nSTD\n34\n414\n37\n418\n0.0\nFALSE\n\n\n-0.3483875\n0.1851030\nBeigel (ACTT-1)\nREM\nSTD\n59\n541\n77\n521\n0.0\nFALSE\n\n\n0.4487619\n1.6487159\nBroman\nTOCI\nSTD\n1\n57\n0\n29\n0.5\nFALSE\n\n\n-0.6620566\n0.7125543\nCriner\nREM\nSTD\n4\n384\n4\n200\n0.0\nFALSE\n\n\n0.0170679\n0.4904898\nDeclerq (COV-AID)\nTOCI\nSTD\n10\n81\n9\n74\n0.0\nFALSE\n\n\n-0.4442338\n0.1688337\nGordon (REMAP-CAP)\nTOCI\nSTD\n83\n353\n116\n358\n0.0\nFALSE\n\n\n\n\n\n\n\nBelow, we conduct a random effects network meta-analysis where we consider standard care (STD) as the control treatment. Note that we have one study where zero cell counts occur, this study will not contribute to the NMA as the log odds ratio and its standard error cannot be determined.\n\nNMA.covid &lt;- netmeta(TE = TE, seTE = seTE, treat1 = treat1, treat2 = treat2,\n                     studlab = studlab, data = covid, sm = \"OR\", ref = \"STD\",\n                     comb.random = TRUE, common = FALSE, warn = FALSE)\nNMA.covid \n\nNumber of studies: k = 20\nNumber of pairwise comparisons: m = 20\nNumber of treatments: n = 3\nNumber of designs: d = 2\n\nRandom effects model\n\nTreatment estimate (sm = 'OR', comparison: other treatments vs 'STD'):\n         OR           95%-CI     z p-value\nREM  0.8999 [0.8067; 1.0039] -1.89  0.0588\nSTD       .                .     .       .\nTOCI 0.8301 [0.7434; 0.9268] -3.31  0.0009\n\nQuantifying heterogeneity / inconsistency:\ntau^2 = 0; tau = 0; I^2 = 0% [0.0%; 48.9%]\n\nTests of heterogeneity (within designs) and inconsistency (between designs):\n                    Q d.f. p-value\nTotal           16.38   18  0.5663\nWithin designs  16.38   18  0.5663\nBetween designs  0.00    0      --\n\n\nA league table of the treatment effect estimates is given below:\n\nnetleague(NMA.covid)\n\nLeague table (random effects model):\n                                                                        \n                     REM 0.8999 [0.8067; 1.0039]                       .\n 0.8999 [0.8067; 1.0039]                     STD 1.2047 [1.0789; 1.3451]\n 1.0842 [0.9282; 1.2663] 1.2047 [1.0789; 1.3451]                    TOCI\n\n\nWe can also present the results in a forest plot:\n\n\n\n\n\nThe figure below shows the percentage of direct and indirect evidence used for each estimated comparison.\n\n\n\n\n\nWe now consider a Bayesian random effects network meta-analysis that analyzes the observed event counts using a binomial link function.\n\nbdata &lt;- data.frame(study = studlab,\n                    treatment = treat,\n                    responders = event,\n                    sampleSize = n)\n\nnetwork &lt;- mtc.network(data.ab  = bdata)\n\nmodel &lt;- mtc.model(network,\n                   likelihood = \"binom\",\n                   link = \"log\",\n                   linearModel = \"random\",\n                   n.chain = 3)\n\n\n# Adaptation\nmcmc1 &lt;- mtc.run(model, n.adapt = 1000, n.iter = 1000, thin = 10)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 42\n   Unobserved stochastic nodes: 45\n   Total graph size: 930\n\nInitializing model\n\n# Sampling\nmcmc2 &lt;- mtc.run(model, n.adapt = 10000, n.iter = 100000, thin = 10)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 42\n   Unobserved stochastic nodes: 45\n   Total graph size: 930\n\nInitializing model\n\n\nWe can extract the pooled treatment effect estimates from the posterior distribution. When using STD as control group, we have:\n\nsummary(relative.effect(mcmc2, t1 = \"STD\"))\n\n\nResults on the Log Risk Ratio scale\n\nIterations = 10010:110000\nThinning interval = 10 \nNumber of chains = 3 \nSample size per chain = 10000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n              Mean      SD  Naive SE Time-series SE\nd.STD.REM  -0.1064 0.09646 0.0005569      0.0008355\nd.STD.TOCI -0.1131 0.08223 0.0004748      0.0008386\nsd.d        0.1115 0.08791 0.0005075      0.0018066\n\n2. Quantiles for each variable:\n\n                2.5%      25%      50%      75%   97.5%\nd.STD.REM  -0.314600 -0.15974 -0.10224 -0.05014 0.08589\nd.STD.TOCI -0.258568 -0.16444 -0.12017 -0.07070 0.07745\nsd.d        0.003872  0.04383  0.09309  0.15850 0.32971\n\n\nThe corresponding odds ratios are as follows:\n\n\n\n\n\nComparison\n95% CrI\n\n\n\n\nREM vs. STD\n0.9 (0.73; 1.09)\n\n\nTOCI vs. STD\n0.89 (0.77; 1.08)\n\n\nREM vs. TOCI\n1.02 (0.75; 1.27)\n\n\n\n\n\n\n\nFinally, we expand the COVID-19 network with trials investigating the effectiveness of hydroxychloroquine (HCQ), lopinavir/ritonavir (LOPI), dexamethasone (DEXA) or interferon-\\(\\beta\\) (INTB) (Selvarajan et al. 2022). The corresponding network is displayed below:\n\n\n\n\n\nEvidence network of the 33 coronavirus-19 trials\n\n\n\n\nWe conducted a random effects network meta-analysis, results are depicted below:\n\n\nNumber of studies: k = 33\nNumber of pairwise comparisons: m = 33\nNumber of treatments: n = 7\nNumber of designs: d = 6\n\nRandom effects model\n\nTreatment estimate (sm = 'OR', comparison: other treatments vs 'STD'):\n         OR           95%-CI     z p-value            95%-PI\nDEXA 0.8557 [0.7558; 0.9688] -2.46  0.0139  [0.7463; 0.9812]\nHCQ  1.1809 [0.8934; 1.5610]  1.17  0.2428  [0.8786; 1.5872]\nINTB 1.1606 [0.9732; 1.3841]  1.66  0.0973  [0.9604; 1.4026]\nLOPI 1.0072 [0.8906; 1.1392]  0.11  0.9085  [0.8794; 1.1537]\nREM  0.8983 [0.8014; 1.0070] -1.84  0.0658  [0.7913; 1.0199]\nSTD       .                .     .       .                 .\nTOCI 0.8304 [0.7410; 0.9306] -3.20  0.0014  [0.7316; 0.9426]\n\nQuantifying heterogeneity / inconsistency:\ntau^2 = 0.0004; tau = 0.0205; I^2 = 0.6% [0.0%; 42.3%]\n\nTests of heterogeneity (within designs) and inconsistency (between designs):\n                    Q d.f. p-value\nTotal           27.18   27  0.4543\nWithin designs  27.18   27  0.4543\nBetween designs  0.00    0      --\n\n\nWe can calculate the P score for each treatment as follows:\n\nnetrank(NMA.covidf)\n\n     P-score\nTOCI  0.9070\nDEXA  0.8357\nREM   0.7143\nSTD   0.4027\nLOPI  0.3899\nHCQ   0.1336\nINTB  0.1166\n\n\n\n\n6.3.2 Pharmacologic treatments for chronic obstructive pulmonary disease\nIn this example, we consider the resuls from a systematic review of randomized controlled trials on pharmacologic treatments for chronic obstructive pulmonary disease (Baker, Baker, and Coleman 2009). The primary outcome, occurrence of one or more episodes of COPD exacerbation, is binary (yes / no). For this outcome, five drug treatments (fluticasone, budesonide, salmeterol, formoterol, tiotropium) and two combinations (fluticasone + salmeterol, budesonide + formoterol) were compared to placebo. The authors considered the two combinations as separate treatments instead of evaluating the individual components.\n\ndata(Baker2009)\n\n\n\n\n\n\nstudy\nyear\nid\ntreatment\nexac\ntotal\n\n\n\n\nLlewellyn-Jones 1996\n1996\n1\nFluticasone\n0\n8\n\n\nLlewellyn-Jones 1996\n1996\n1\nPlacebo\n3\n8\n\n\nBoyd 1997\n1997\n2\nSalmeterol\n47\n229\n\n\nBoyd 1997\n1997\n2\nPlacebo\n59\n227\n\n\nPaggiaro 1998\n1998\n3\nFluticasone\n45\n142\n\n\nPaggiaro 1998\n1998\n3\nPlacebo\n51\n139\n\n\n\n\n\n\n\n\nBaker &lt;- pairwise(treat = treatment,\n                  event = exac,\n                  n = total,\n                  studlab = id,\n                  sm = \"OR\",\n                  data = Baker2009)\n\nNMA.COPD &lt;- netmeta(TE = TE, seTE = seTE, treat1 = treat1, treat2 = treat2,\n                    studlab = studlab, data = Baker, sm = \"OR\", ref = \"Placebo\",\n                    comb.random = TRUE)\n\nWarning: Comparisons with missing TE / seTE or zero seTE not considered in\nnetwork meta-analysis.\n\n\nComparisons not considered in network meta-analysis:\n studlab                 treat1     treat2 TE seTE\n      39 Fluticasone+Salmeterol    Placebo NA   NA\n      39 Fluticasone+Salmeterol Salmeterol NA   NA\n      39             Salmeterol    Placebo NA   NA\n\nnetgraph(NMA.COPD)\n\n\n\n\n\n\n6.3.3 Advanced Therapies for Ulcerative Colitis\nIn this example, we consider a systematic literature review of Phase 3 randomized controlled trials investigating the following advanced therapies: infliximab, adalimumab, vedolizumab, golimumab, tofacitinib, ustekinumab, filgotinib, ozanimod, and upadacitinib (Panaccione et al. 2023). This review included 48 RCTs, from which 23 were found eligible for inclusion in a network meta-analysis. The included RCT populations were largely comparable in their baseline characteristics, though some heterogeneity was noted in weight, disease duration, extent of disease, and concomitant medications. A risk of bias assessment showed a low risk of bias for all included RCTs, which were all industry sponsored.\nWe here focus on the synthesis of 18 trials that contributed efficacy data for induction in bio-naive populations. The following FDA- and/or EMA-approved biologic or SMD doses were investigated:\n\nAdalimumab subcutaneous 160 mg at week 0, 80 mg at week 2, and 40 mg at week 4 (ADA160/80)\nInfliximab intravenous 5 mg/kg (INF5) at weeks 0, 2, and 6 then every 8 weeks\nInfliximab intravenous 10 mg/kg (INF10) at weeks 0, 2, and 6 then every 8 weeks\nFilgotinib oral 100 mg once daily (FIL100)\nFilgotinib oral 200 mg once daily (FIL200)\nGolimumab subcutaneous 200 mg at week 0 and 100 mg at week 2 (GOL200/100)\nOzanimod oral 0.23 mg once daily for 4 days, 0.46 mg once daily for 3 days, then 0.92 mg once daily (OZA0.92)\nTofacitinib oral 10 mg twice daily for 8 weeks (TOF10)\nUpadacitinib oral 45 mg once daily for 8 weeks (UPA45)\nUstekinumab intravenous 6 mg/kg at week 0 (UST6)\nVedolizumab intravenous 300 mg at weeks 0, 2, and 6 (VED300)\n\nThe reference treatment is placebo (PBO).\n\n\n\nEfficacy outcomes (i.e., clinical remission) data of induction bio-naïve populations \n\n\nstudlab\ntreat1\ntreat2\nevent1\nn1\nevent2\nn2\n\n\n\n\nACT-1\nINF10\nINF5\n39\n122\n47\n121\n\n\nACT-1\nINF10\nPBO\n39\n122\n18\n121\n\n\nACT-1\nINF5\nPBO\n47\n121\n18\n121\n\n\nACT-2\nINF10\nINF5\n33\n120\n41\n121\n\n\nACT-2\nINF10\nPBO\n33\n120\n7\n123\n\n\nACT-2\nINF5\nPBO\n41\n121\n7\n123\n\n\nGEMINI 1\nVED300\nPBO\n30\n130\n5\n76\n\n\nJapic CTI-060298\nINF5\nPBO\n21\n104\n11\n104\n\n\nJiang 2015\nINF5\nPBO\n22\n41\n9\n41\n\n\nM10-447\nADA160/80\nPBO\n9\n90\n11\n96\n\n\nNCT01551290\nINF5\nPBO\n11\n50\n5\n49\n\n\nNCT02039505\nVED300\nPBO\n22\n79\n6\n41\n\n\nOCTAVE 1\nTOF10\nPBO\n56\n222\n9\n57\n\n\nOCTAVE 2\nTOF10\nPBO\n43\n195\n4\n47\n\n\nPURSUIT-SC\nGOL200/100\nPBO\n45\n253\n16\n251\n\n\nSELECTION\nFIL100\nFIL200\n47\n277\n60\n245\n\n\nSELECTION\nFIL100\nPBO\n47\n277\n17\n137\n\n\nSELECTION\nFIL200\nPBO\n60\n245\n17\n137\n\n\nTRUE NORTH\nOZA0.92\nPBO\n66\n299\n10\n151\n\n\nU-ACCOMPLISH\nUPA45\nPBO\n54\n166\n3\n81\n\n\nU-ACHIEVE Study 2\nUPA45\nPBO\n41\n145\n4\n72\n\n\nULTRA-1\nADA160/80\nPBO\n24\n130\n12\n130\n\n\nULTRA-2\nADA160/80\nPBO\n32\n150\n16\n145\n\n\nUNIFI\nUST6\nPBO\n27\n147\n15\n151\n\n\n\n\n\n\n\nThe corresponding network is displayed below:\n\n\n\n\n\nEvidence network of 18 trials that contributed efficacy data for induction in bio-naive populations\n\n\n\n\nBelow, we conduct a random effects network meta-analysis of the reported study effects (expressed as odds ratio) and consider placebo (treat = \"PBO\") as the control treatment.\n\nNMA.uc &lt;- netmeta(TE = TE, seTE = seTE, treat1 = treat1, treat2 = treat2,\n                  studlab = studlab, data = UlcerativeColitis, sm = \"OR\", \n                  ref = \"PBO\", common = FALSE, comb.random = TRUE)\nNMA.uc\n\nAll treatments except FIL100 and UST6 are significantly more efficacious than PBO at inducing clinical remission. We can now estimate the probabilities of each treatment being at each possible rank and the SUCRAs (Surface Under the Cumulative RAnking curve):\n\nsucra.uc &lt;- rankogram(NMA.uc, nsim = 100, random = TRUE, common = FALSE, \n                      small.values = \"undesirable\")\n\n# Exctract the SUCRA values\nsucra.uc$ranking.random\n\n ADA160/80     FIL100     FIL200 GOL200/100      INF10       INF5    OZA0.92 \n0.23818182 0.19454545 0.39909091 0.62363636 0.59727273 0.73818182 0.78727273 \n       PBO      TOF10      UPA45       UST6     VED300 \n0.01636364 0.39545455 0.98454545 0.36272727 0.66272727 \n\n\nThese results indicate that 98.5% of the evaluated treatments are worse than UPA45."
  },
  {
    "objectID": "chapter_10.html#version-info",
    "href": "chapter_10.html#version-info",
    "title": "6  Systematic review and meta-analysis of Real-World Evidence",
    "section": "Version info",
    "text": "Version info\nThis chapter was rendered using the following version of R and its packages:\n\n\nR version 4.2.3 (2023-03-15 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=Dutch_Netherlands.utf8  LC_CTYPE=Dutch_Netherlands.utf8   \n[3] LC_MONETARY=Dutch_Netherlands.utf8 LC_NUMERIC=C                      \n[5] LC_TIME=Dutch_Netherlands.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] dmetar_0.0.9000  netmeta_2.8-2    meta_6.5-0       gemtc_1.0-1     \n[5] coda_0.19-4      dplyr_1.1.2      kableExtra_1.3.4\n\nloaded via a namespace (and not attached):\n [1] httr_1.4.6          magic_1.6-1         jsonlite_1.8.5     \n [4] viridisLite_0.4.2   splines_4.2.3       highr_0.10         \n [7] stats4_4.2.3        metafor_4.2-0       slam_0.1-50        \n[10] yaml_2.3.7          robustbase_0.99-0   ggrepel_0.9.3      \n[13] numDeriv_2016.8-1.1 pillar_1.9.0        lattice_0.21-8     \n[16] glue_1.6.2          digest_0.6.31       rvest_1.0.3        \n[19] minqa_1.2.5         colorspace_2.1-0    MuMIn_1.47.5       \n[22] htmltools_0.5.5     Matrix_1.5-4.1      plyr_1.8.8         \n[25] pkgconfig_2.0.3     mvtnorm_1.2-2       Rglpk_0.6-5        \n[28] scales_1.2.1        webshot_0.5.4       svglite_2.1.1      \n[31] rjags_4-14          metadat_1.2-0       lme4_1.1-33        \n[34] tibble_3.2.1        farver_2.1.1        generics_0.1.3     \n[37] ggplot2_3.4.2       withr_2.5.0         nnet_7.3-19        \n[40] cli_3.6.1           magrittr_2.0.3      mclust_6.0.0       \n[43] evaluate_0.21       fansi_1.0.4         nlme_3.1-162       \n[46] MASS_7.3-60         truncnorm_1.0-9     forcats_1.0.0      \n[49] xml2_1.3.4          class_7.3-22        tools_4.2.3        \n[52] lifecycle_1.0.3     stringr_1.5.0       kernlab_0.9-32     \n[55] munsell_0.5.0       cluster_2.1.4       fpc_2.2-10         \n[58] compiler_4.2.3      systemfonts_1.0.4   rlang_1.1.1        \n[61] grid_4.2.3          nloptr_2.0.3        rstudioapi_0.14    \n[64] CompQuadForm_1.4.3  htmlwidgets_1.6.2   igraph_1.5.0       \n[67] labeling_0.4.2      rmarkdown_2.22      boot_1.3-28.1      \n[70] gtable_0.3.3        codetools_0.2-19    abind_1.4-5        \n[73] flexmix_2.3-19      R6_2.5.1            gridExtra_2.3      \n[76] knitr_1.43          prabclus_2.3-2      fastmap_1.1.1      \n[79] utf8_1.2.3          mathjaxr_1.6-0      poibin_1.5         \n[82] modeltools_0.2-23   stringi_1.7.12      parallel_4.2.3     \n[85] Rcpp_1.0.10         vctrs_0.6.3         DEoptimR_1.0-14    \n[88] tidyselect_1.2.0    xfun_0.39           diptest_0.76-0"
  },
  {
    "objectID": "chapter_10.html#references",
    "href": "chapter_10.html#references",
    "title": "6  Systematic review and meta-analysis of Real-World Evidence",
    "section": "References",
    "text": "References\n\n\n\n\nBaker, William L, Erica L Baker, and Craig I Coleman. 2009. “Pharmacologic Treatments for Chronic Obstructive Pulmonary Disease: A Mixed-Treatment Comparison Meta-Analysis.” Pharmacotherapy 29 (8): 891–905. https://doi.org/10.1592/phco.29.8.891.\n\n\nPanaccione, Remo, Eric B Collins, Gil Y Melmed, Severine Vermeire, Silvio Danese, Peter D R Higgins, Christina S Kwon, et al. 2023. “Efficacy and Safety of Advanced Therapies for Moderately to Severely Active Ulcerative Colitis at Induction and Maintenance: An Indirect Treatment Comparison Using Bayesian Network Meta-Analysis.” Crohn’s & Colitis 360 5 (2). https://doi.org/10.1093/crocol/otad009.\n\n\nSelvarajan, Sandhiya, Annuja Anandaradje, Santhosh Shivabasappa, Deepthy Melepurakkal Sadanandan, N. Sreekumaran Nair, and Melvin George. 2022. “Efficacy of Pharmacological Interventions in COVID-19: A Network Meta-Analysis.” British Journal of Clinical Pharmacology 88 (9): 4080–91. https://doi.org/10.1111/bcp.15338.\n\n\nSiemieniuk, Reed AC, Jessica J Bartoszko, Dena Zeraatkar, Elena Kum, Anila Qasim, Juan Pablo Dı́az Martinez, Ariel Izcovich, et al. 2020. “Drug Treatments for Covid-19: Living Systematic Review and Network Meta-Analysis.” BMJ, July, m2980. https://doi.org/10.1136/bmj.m2980."
  },
  {
    "objectID": "chapter_12.html#introduction",
    "href": "chapter_12.html#introduction",
    "title": "7  Dealing with irregular and informative visits",
    "section": "7.1 Introduction",
    "text": "7.1 Introduction\nWe first load the relevant R scripts:\n\nsource(\"resources/chapter 12/sim.r\")\nsource(\"resources/chapter 12/fig_functions.r\")\nsource(\"resources/chapter 12/mlmi.r\")"
  },
  {
    "objectID": "chapter_12.html#example-dataset",
    "href": "chapter_12.html#example-dataset",
    "title": "7  Dealing with irregular and informative visits",
    "section": "7.2 Example dataset",
    "text": "7.2 Example dataset\nBelow, we generate an example dataset that contains information on the treatment allocation x and three baseline covariates age, sex and edss (EDSS at treatment start). The discrete outcome y represents the Expanded Disability Status Scale (EDSS) score after time months of treatment exposure. Briefly, the EDSS is a semi-continuous measure that varies from 0 (no disability) to 10 (death).\n\nset.seed(9843626)\n\ndataset  &lt;- sim_data_EDSS(npatients = 500,\n                          ncenters = 10,\n                          follow_up = 12*5, # Total follow-up (number of months)\n                          sd_a_t = 0.5,   # DGM - Within-visit variation in EDSS scores\n                          baseline_EDSS = 1.3295,    # DGM - Mean baseline EDDS score\n                          sd_alpha_ij = 1.46,    # DGM - Between-subject variation in baseline EDSS\n                          sd_beta1_j = 0.20,    # DGM - Between-site variation in baseline EDSS\n                          mean_age = 42.41,\n                          sd_age = 10.53,\n                          min_age = 18,\n                          beta_age = 0.05, # DGM - prognostic effect of age\n                          beta_t = 0.014,  # DGM - prognostic effect of time\n                          beta_t2 = 0,    # DGM - prognostic effect of time squared\n                          delta_xt = 0, # DGM - interaction treatment time\n                          delta_xt2 = 0, # 0.0005    # DGM - interaction treatment time2\n                          p_female = 0.75, \n                          beta_female = -0.2 ,  ## DGM - prognostic effect of male sex\n                          delta_xf = 0,      ## DGM - interaction sex treatment       \n                          rho = 0.8,             # DGM - autocorrelation of between alpha_tij\n                          corFUN = corAR1,       # DGM - correlation structure of the latent EDSS scores\n                          tx_alloc_FUN = treatment_alloc_confounding_v2 ) ## or treatment_alloc_randomized\n\n\n\n\n\n\nDistribution of the EDSS score at each time point\n\n\n\n\nWe remove the outcome y according to the informative visit process that depends on the received treatment, gender, and age.\n\ndataset_visit &lt;- censor_visits_a5(dataset, seed = 12345) %&gt;% \n  dplyr::select(-y) %&gt;%\n  mutate(time_x = time*x)\n\nIn the censored data, a total of 17 out of 5000 patients have a visit at time=60."
  },
  {
    "objectID": "chapter_12.html#estimation-of-treatment-effect",
    "href": "chapter_12.html#estimation-of-treatment-effect",
    "title": "7  Dealing with irregular and informative visits",
    "section": "7.3 Estimation of treatment effect",
    "text": "7.3 Estimation of treatment effect\nWe will estimate the marginal treatment effect at time time=60.\n\n7.3.1 Original data\n\norigdat60 &lt;- dataset %&gt;% filter(time == 60)\n\n# Predict probability of treatment allocation\nfitps &lt;- glm(x ~ age + sex + edss, family = 'binomial', \n             data = origdat60)\n\n# Derive the propensity score\norigdat60 &lt;- origdat60 %&gt;% mutate(ipt = ifelse(x == 1, 1/predict(fitps, type = 'response'),\n                                               1/(1-predict(fitps, type = 'response'))))\n\n# Estimate \nfit_ref_m &lt;- tidy(lm(y ~ x, weight = ipt, data = origdat60), conf.int = TRUE) \n\n\n\n7.3.2 Doubly-weighted marginal treatment effect\nWe here implement inverse probability of response weights into the estimating equations to adjust for nonrandom missingness Coulombe, Moodie, and Platt (2020).\n\nobsdat60 &lt;- dataset_visit %&gt;% mutate(visit = ifelse(is.na(y_obs),0,1)) %&gt;% filter(time == 60)\n\ngamma &lt;- glm(visit ~ x + sex + age + edss, family = 'binomial', data = obsdat60)$coef   \n\nobsdat60 &lt;- obsdat60 %&gt;% mutate(rho_i = 1/exp(gamma[\"(Intercept)\"] +\n                                                          gamma[\"x\"]*x +\n                                                          gamma[\"sex\"]*sex +\n                                                          gamma[\"age\"]*age))\n\n# Predict probability of treatment allocation\nfitps &lt;- glm(x ~ age + sex + edss, family='binomial', data = obsdat60)\n\n# Derive the propensity score\nobsdat60 &lt;- obsdat60 %&gt;% mutate(ipt = ifelse(x==1, 1/predict(fitps, type='response'),\n                                            1/(1-predict(fitps, type='response'))))\n\n\nfit_w &lt;- tidy(lm(y_obs ~ x, weights = ipt*rho_i, data = obsdat60), conf.int = TRUE)\n\n\n\n7.3.3 Multilevel multiple imputation\nWe adopt the imputation approach proposed by Debray et al. (2023). Briefly, we impute the entire vector of y_obs for all 61 potential visits and generate 10 imputed datasets. Note: mlmi currently does not support imputation of treatment-covariate interaction terms.\n\nimp &lt;- impute_y_mice_3l(dataset_visit, seed = 12345)\n\nWe can now estimate the treatment effect in each imputed dataset\n\n# Predict probability of treatment allocation\nfitps &lt;- glm(x ~ age + sex + edss, family='binomial', data = dataset_visit)\n  \n# Derive the propensity score\ndataset_visit &lt;- dataset_visit %&gt;% mutate(ipt = ifelse(x==1, 1/predict(fitps, type='response'),\n                                                       1/(1-predict(fitps, type='response'))))\n  \nQ &lt;- U &lt;- rep(NA, 10) # Error variances\n\nfor (i in seq(10)) {\n  dati &lt;- cbind(dataset_visit[,c(\"x\",\"ipt\",\"time\")], y_imp = imp[,i]) %&gt;% filter(time == 60)\n  \n  # Estimate \n  fit &lt;- tidy(lm(y_imp ~ x, weight = ipt, data = dati), conf.int = TRUE) \n  \n  Q[i] &lt;- fit %&gt;% filter(term == \"x\") %&gt;% pull(estimate)\n  U[i] &lt;- (fit %&gt;% filter(term == \"x\") %&gt;% pull(std.error))**2\n}\n\nfit_mlmi &lt;- pool.scalar(Q = Q, U = U)"
  },
  {
    "objectID": "chapter_12.html#reproduce-the-results-using-all-data-to-compute-the-marginal-effect-with-iiv-weighted",
    "href": "chapter_12.html#reproduce-the-results-using-all-data-to-compute-the-marginal-effect-with-iiv-weighted",
    "title": "7  Dealing with irregular and informative visits",
    "section": "7.4 Reproduce the results using all data to compute the marginal effect with IIV-weighted",
    "text": "7.4 Reproduce the results using all data to compute the marginal effect with IIV-weighted\n\n7.4.1 Doubly -weighted marginal treatment effect total\n\nobsdatall &lt;- dataset_visit %&gt;% mutate(visit = ifelse(is.na(y_obs),0,1))  \ngamma &lt;- glm(visit ~ x + sex + age + edss, family = 'binomial', data = obsdatall)$coef   \nobsdatall &lt;- obsdatall %&gt;% mutate(rho_i = 1/exp(gamma[\"(Intercept)\"] +\n                                                gamma[\"x\"]*x +\n                                                gamma[\"sex\"]*sex +\n                                                gamma[\"age\"]*age))\n# Predict probability of treatment allocation\nfitps &lt;- glm(x ~ age + sex + edss, family='binomial', data = obsdatall)\n# Derive the propensity score\nobsdatall &lt;- obsdatall %&gt;% mutate(ipt = ifelse(x==1, 1/predict(fitps, type='response'),\n                                             1/(1-predict(fitps, type='response'))))\nfit_w &lt;- tidy(lm(y_obs ~ x, weights = ipt*rho_i, data = obsdatall), conf.int = TRUE)"
  },
  {
    "objectID": "chapter_12.html#results",
    "href": "chapter_12.html#results",
    "title": "7  Dealing with irregular and informative visits",
    "section": "7.5 Results",
    "text": "7.5 Results"
  },
  {
    "objectID": "chapter_12.html#version-info",
    "href": "chapter_12.html#version-info",
    "title": "7  Dealing with irregular and informative visits",
    "section": "Version info",
    "text": "Version info\nThis chapter was rendered using the following version of R and its packages:\n\n\nR version 4.2.3 (2023-03-15 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=Dutch_Netherlands.utf8  LC_CTYPE=Dutch_Netherlands.utf8   \n[3] LC_MONETARY=Dutch_Netherlands.utf8 LC_NUMERIC=C                      \n[5] LC_TIME=Dutch_Netherlands.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] sparseMVN_0.2.2 truncnorm_1.0-9 MASS_7.3-60     nlme_3.1-162   \n[5] mice_3.16.0     ggplot2_3.4.3   broom_1.0.5     dplyr_1.1.2    \n\nloaded via a namespace (and not attached):\n [1] shape_1.4.6       tidyselect_1.2.0  xfun_0.39         purrr_1.0.1      \n [5] splines_4.2.3     lattice_0.21-8    colorspace_2.1-0  vctrs_0.6.3      \n [9] generics_0.1.3    htmltools_0.5.5   yaml_2.3.7        pan_1.6          \n[13] utf8_1.2.3        survival_3.5-5    rlang_1.1.1       jomo_2.7-6       \n[17] pillar_1.9.0      nloptr_2.0.3      glue_1.6.2        withr_2.5.0      \n[21] foreach_1.5.2     lifecycle_1.0.3   munsell_0.5.0     gtable_0.3.4     \n[25] htmlwidgets_1.6.2 codetools_0.2-19  evaluate_0.21     labeling_0.4.3   \n[29] knitr_1.44        fastmap_1.1.1     fansi_1.0.4       Rcpp_1.0.10      \n[33] scales_1.2.1      backports_1.4.1   jsonlite_1.8.5    farver_2.1.1     \n[37] lme4_1.1-33       digest_0.6.31     grid_4.2.3        cli_3.6.1        \n[41] tools_4.2.3       magrittr_2.0.3    glmnet_4.1-7      tibble_3.2.1     \n[45] tidyr_1.3.0       pkgconfig_2.0.3   ellipsis_0.3.2    Matrix_1.5-4.1   \n[49] minqa_1.2.5       rmarkdown_2.24    rstudioapi_0.15.0 iterators_1.0.14 \n[53] rpart_4.1.19      mitml_0.4-5       R6_2.5.1          boot_1.3-28.1    \n[57] nnet_7.3-19       compiler_4.2.3"
  },
  {
    "objectID": "chapter_12.html#references",
    "href": "chapter_12.html#references",
    "title": "7  Dealing with irregular and informative visits",
    "section": "References",
    "text": "References\n\n\n\n\nCoulombe, Janie, Erica E. M. Moodie, and Robert W. Platt. 2020. “Weighted Regression Analysis to Correct for Informative Monitoring Times and Confounders in Longitudinal Studies.” Biometrics 77 (1): 162–74. https://doi.org/10.1111/biom.13285.\n\n\nCoulombe, Janie, Erica E. M. Moodie, Robert W. Platt, and Christel Renoux. 2022. “Estimation of the Marginal Effect of Antidepressants on Body Mass Index Under Confounding and Endogenous Covariate-Driven Monitoring Times.” The Annals of Applied Statistics 16 (3). https://doi.org/10.1214/21-aoas1570.\n\n\nDebray, Thomas PA, Gabrielle Simoneau, Massimiliano Copetti, Robert W Platt, Changyu Shen, Fabio Pellegrini, and Carl de Moor. 2023. “Methods for Comparative Effectiveness Based on Time to Confirmed Disability Progression with Irregular Observations in Multiple Sclerosis.” Statistical Methods in Medical Research, June, 096228022311720. https://doi.org/10.1177/09622802231172032."
  }
]