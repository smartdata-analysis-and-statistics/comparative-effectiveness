[
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "Comparative Effectiveness and Personalized Medicine Research Using Real-World Data",
    "section": "About this book",
    "text": "About this book\nThis book provides practical guidance for estimating the effectiveness of treatments in real-world populations. It explains how real-world data can directly be used or combined with other data sources to derive overall and individualized estimates of treatment effect. The book explains statistical methods for implementing bias adjustments, conducting evidence synthesis and individualizing treatment effect, whilst also providing illustrative examples and supporting software. The chapters and contents of the book are written by leading experts, with a track record in the generation and/or evaluation of real-world evidence.\nThis book is intended as a pivotal textbook for statisticians, epidemiologists, methodologists, regulators and/or regulatory scientists considering, undertaking or appraising the real-world evidence of treatment effectiveness. It covers key concepts and stages to derive and evaluate treatment effect estimates for entire populations and specific individuals. The book offers a conceptual framework towards estimating treatment effects at both the population and individualized level, where modelling methods may include traditional regression-based and machine learning methods."
  },
  {
    "objectID": "index.html#motivation",
    "href": "index.html#motivation",
    "title": "Comparative Effectiveness and Personalized Medicine Research Using Real-World Data",
    "section": "Motivation",
    "text": "Motivation\nAlthough randomized clinical trials traditionally form the cornerstone of comparative effectiveness research, there is a growing demand to consider evidence from “real-world data” (RWD) in clinical decision-making. These data are often available from observational cohort studies, administrative databases, and patient registries, and may offer additional insights into the comparative effectiveness and safety of treatments. Yet, the analysis of RWD and the evaluation of real-world evidence face many operational and methodological challenges.\nIn this book, we aim to address three current needs. First, this book will offer the guidance that is currently lacking on assessing the quality of RWD and on implementing appropriate statistical methods to reduce bias of single study estimates of treatment effects. Second, this book will provide researchers with advanced approaches to pooling estimates from multiple non-randomized studies for which traditional evidence synthesis methods are not suitable. Finally, to answer the growing need to translate average estimates of treatment effects to individualized clinical decision-making, this book will present recent methods for more tailored approaches where patient characteristics are used to derive their individualized prognosis and treatment benefit.\nThis book aims to explain key principles and state-of-the-art methods for deriving treatment effects in entire populations and specific individuals using RWD. It will not only discuss statistical theory by key experts in the field; it will also provide illustrative examples and practical guidance for implementation in R. In short, the book aims to prepare a new generation of researchers who wish to generate and integrate evidence from both randomized and non-randomized data sources to investigate the real-world effectiveness of treatments in populations and individual patients."
  },
  {
    "objectID": "index.html#contents",
    "href": "index.html#contents",
    "title": "Comparative Effectiveness and Personalized Medicine Research Using Real-World Data",
    "section": "Contents",
    "text": "Contents\nThe book is divided into six sections:\n\nIntroduction. This section introduces the relevance of real-world data for conducting comparative effectiveness research, and discusses various concerns regarding their use.\nPrinciples of treatment effect estimation using real-world data. In this section, we discuss key principles of treatment effect estimation in non-randomized data sources. We explain methods to adjust for confounding (including propensity score analysis and disease risk score analysis) and missing data when estimating the treatment effect for a specific (sub)population.\nPrinciples of evidence synthesis. In this section, we discuss statistical methods for estimating the treatment effect using (individual participant and/or aggregate) data from multiple studies. To this purpose, key principles of meta-analysis are introduced and explained, including the standard fixed effect and random effects meta-analysis models, methods for individual patient data (IPD) meta-analysis, methods for network meta-analysis, and methods for data-driven and tailored bias adjustment.\nAdvanced modelling issues for dealing with additional bias in both randomized and non-randomized data sources. In this section, we discuss advanced statistical and machine learning methods for dealing with time-varying confounding, informative visit schedules, and measurement error.\nIndividualizing treatment effects for personalized medicine. In this section, we discuss statistical methods to estimate and evaluate individualized treatment effects.\nClosing"
  },
  {
    "objectID": "chapter_03.html#example-code",
    "href": "chapter_03.html#example-code",
    "title": "2  Validity control and quality assessment of real-world data and real-world evidence",
    "section": "2.1 Example code",
    "text": "2.1 Example code\nA risk of bias assessment was conducted in the COVID-NMA review. We can create a summary table of risk of bias assessment and produce a traffic light plot as follows:\n\nRisk_of_Bias <- read_excel(\"resources/RoB-covid.xlsx\")\n\n#creation of traffic light plot\ntrafficlight_rob <- rob_traffic_light(data = Risk_of_Bias, tool = \"ROB2\")\ntrafficlight_rob"
  },
  {
    "objectID": "chapter_03.html#version-info",
    "href": "chapter_03.html#version-info",
    "title": "2  Validity control and quality assessment of real-world data and real-world evidence",
    "section": "Version info",
    "text": "Version info\nThis chapter was rendered using the following version of R and its packages:\n\n\nR version 4.2.3 (2023-03-15 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=Dutch_Netherlands.utf8  LC_CTYPE=Dutch_Netherlands.utf8   \n[3] LC_MONETARY=Dutch_Netherlands.utf8 LC_NUMERIC=C                      \n[5] LC_TIME=Dutch_Netherlands.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] robvis_0.3.0.900 devtools_2.4.5   usethis_2.1.6    readxl_1.4.2    \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.10       urlchecker_1.0.1  cellranger_1.1.0  compiler_4.2.3   \n [5] later_1.3.0       remotes_2.4.2     prettyunits_1.1.1 profvis_0.3.8    \n [9] tools_4.2.3       digest_0.6.31     pkgbuild_1.4.0    pkgload_1.3.2    \n[13] jsonlite_1.8.4    evaluate_0.21     memoise_2.0.1     lifecycle_1.0.3  \n[17] rlang_1.1.0       shiny_1.7.4       cli_3.6.1         rstudioapi_0.14  \n[21] yaml_2.3.7        xfun_0.39         fastmap_1.1.1     stringr_1.5.0    \n[25] knitr_1.42        fs_1.6.1          vctrs_0.6.1       htmlwidgets_1.6.2\n[29] glue_1.6.2        R6_2.5.1          processx_3.8.0    rmarkdown_2.21   \n[33] sessioninfo_1.2.2 purrr_1.0.1       callr_3.7.3       magrittr_2.0.3   \n[37] promises_1.2.0.1  ps_1.7.4          ellipsis_0.3.2    htmltools_0.5.5  \n[41] mime_0.12         xtable_1.8-4      httpuv_1.6.9      stringi_1.7.12   \n[45] miniUI_0.1.1.1    cachem_1.0.7      crayon_1.5.2"
  },
  {
    "objectID": "chapter_06.html#introduction",
    "href": "chapter_06.html#introduction",
    "title": "3  Confounding adjustment using propensity score methods",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\nThe purpose of this document is to provide example R code that demonstrates how to estimate the propensity score and implement matching, stratification, weighting, and regression adjustment for the continuous propensity score. In this example using simulated data, we have two disease modifying therapies (DMT1 and DMT0) and the outcome is the number of post-treatment multiple sclerosis relapses during follow-up. We will estimate the average treatment effect in the treated (ATT) using propensity score matching, stratification, and weighting. We will estimate the average treatment effect in the population (ATE) using regression adjustment for the continuous propensity score. The treatment effects can be interpreted as annualized relapse rate ratios (ARR).\nWe consider an example dataset with the following characteristics:\n\nhead(dat)\n\n   age female prevDMTefficacy premedicalcost numSymptoms prerelapse_num\n1:  50      1            None        3899.61           1              1\n2:  51      0            None        9580.51           1              0\n3:  56      0            None        4785.89           1              0\n4:  44      1            None        8696.80           1              1\n5:  63      0            None        2588.03           1              0\n6:  28      1            None        5435.57           1              0\n   treatment y      years      Iscore\n1:      DMT1 0 1.78507871 Moderate A1\n2:      DMT1 0 0.01368925     High A1\n3:      DMT1 2 3.25530459     High A1\n4:      DMT1 2 5.73853525     Neutral\n5:      DMT1 0 1.31143053     High A1\n6:      DMT1 0 0.59137577 Moderate A0"
  },
  {
    "objectID": "chapter_06.html#comparing-baseline-characteristics",
    "href": "chapter_06.html#comparing-baseline-characteristics",
    "title": "3  Confounding adjustment using propensity score methods",
    "section": "3.2 Comparing baseline characteristics",
    "text": "3.2 Comparing baseline characteristics\n\nDMT1 is the treatment group and DMT0 is the control group\nprevDMTefficacy is previous DMT efficacy (none, low efficacy, and medium/high efficacy)\nprerelapse_num is the number of previous MS relapses\n\n\n\n\n\n\n\nDMT0\nDMT1\n\n\n\n\nn\n2300\n7700\n\n\nage (mean (SD))\n51.39 (8.32)\n44.25 (9.79)\n\n\nfemale = 1 (%)\n1671 (72.65)\n5915 (76.82)\n\n\nprevDMTefficacy (%)\n\n\n\n\nNone\n1247 (54.22)\n3171 (41.18)\n\n\nLow_efficacy\n261 (11.35)\n858 (11.14)\n\n\nMedium_high_efficacy\n792 (34.43)\n3671 (47.68)\n\n\nprerelapse_num (mean (SD))\n0.39 (0.62)\n0.46 (0.68)"
  },
  {
    "objectID": "chapter_06.html#estimating-the-propensity-score",
    "href": "chapter_06.html#estimating-the-propensity-score",
    "title": "3  Confounding adjustment using propensity score methods",
    "section": "3.3 Estimating the propensity score",
    "text": "3.3 Estimating the propensity score\n\n3.3.1 Logistic regression\nWe sought to restore balance in the distribution of baseline covariates in patients treated with DMT1 (index treatment) and DMT0 (control tratment). We fit a multivariable logistic regression model in which treatment was regressed on baseline characteristics including age, sex, previous DMT efficacy, and previous number of relapses.\n\n# Fit logistic regression model\nps.model <- glm(treatment ~ age + female + prevDMTefficacy + prerelapse_num, \n                data = dat, family = binomial())\n\n# Summary of logistic regression model\nsummary(ps.model)\n\n\nCall:\nglm(formula = treatment ~ age + female + prevDMTefficacy + prerelapse_num, \n    family = binomial(), data = dat)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.7949   0.2585   0.5220   0.7478   1.5033  \n\nCoefficients:\n                                     Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                          4.809473   0.157127  30.609  < 2e-16 ***\nage                                 -0.086708   0.002996 -28.939  < 2e-16 ***\nfemale1                              0.253611   0.057664   4.398 1.09e-05 ***\nprevDMTefficacyLow_efficacy          0.310394   0.083022   3.739 0.000185 ***\nprevDMTefficacyMedium_high_efficacy  0.660266   0.054393  12.139  < 2e-16 ***\nprerelapse_num                       0.156318   0.039288   3.979 6.93e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 10786  on 9999  degrees of freedom\nResidual deviance:  9597  on 9994  degrees of freedom\nAIC: 9609\n\nNumber of Fisher Scoring iterations: 5\n\n# Extract propensity scores\ndat$ps <- predict(ps.model, data = dat, type = \"response\")\n\n\n\n3.3.2 Assessing overlap\nWe examined the degree of overlap in the distribution of propensity scores across treatment groups using histograms and side-by-side box plots.\n\n# Histogram\nggplot(dat, aes(x = ps, fill = as.factor(treatment), color = as.factor(treatment))) + \n  geom_histogram(alpha = 0.3, position='identity', bins = 15) + \n  facet_grid(as.factor(treatment) ~ .) + \n  xlab(\"Probability of Treatment\") + \n  ylab(\"Count\") +\n  ggtitle(\"Propensity Score Distribution by Treatment Group\") +\n  theme(legend.position = \"bottom\", legend.direction = \"vertical\")\n\n\n\n# Side-by-side box plots\nggplot(dat, aes(x=as.factor(treatment), y=ps, fill=as.factor(treatment))) +\n  geom_boxplot() + \n  ggtitle(\"Propensity Score Distribution by Treatment Group\") +\n  ylab(\"Probability of Treatment\") + \n  xlab(\"Treatment group\") +\n  theme(legend.position = \"none\")\n\n\n\n# Distribution of propensity scores by treatment groups\nsummary(dat$ps[dat$treatment == \"DMT1\"])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.3230  0.7214  0.8265  0.7970  0.9010  0.9854 \n\nsummary(dat$ps[dat$treatment == \"DMT0\"])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.3230  0.5730  0.6894  0.6795  0.7975  0.9799"
  },
  {
    "objectID": "chapter_06.html#propensity-score-matching",
    "href": "chapter_06.html#propensity-score-matching",
    "title": "3  Confounding adjustment using propensity score methods",
    "section": "3.4 Propensity score matching",
    "text": "3.4 Propensity score matching\n\n\n\n\n3.4.1 1:1 Optimal full matching without replacement\n\nlibrary(MatchIt)\n\n# Use MatchIt package for PS matching\nopt <- matchit(treatment ~ age + female + prevDMTefficacy + prerelapse_num, \n               data = dat, \n               method = \"full\",\n               estimand = \"ATT\")\n\nopt\n\nA matchit object\n - method: Optimal full matching\n - distance: Propensity score\n             - estimated with logistic regression\n - number of obs.: 10000 (original), 10000 (matched)\n - target estimand: ATT\n - covariates: age, female, prevDMTefficacy, prerelapse_num\n\n\n\n\n3.4.2 Assess balance after matching\n\nsummary(opt)\n\n\nCall:\nmatchit(formula = treatment ~ age + female + prevDMTefficacy + \n    prerelapse_num, data = dat, method = \"full\", estimand = \"ATT\")\n\nSummary of Balance for All Data:\n                                    Means Treated Means Control Std. Mean Diff.\ndistance                                   0.7970        0.6795          0.8943\nage                                       44.2496       51.3883         -0.7289\nfemale0                                    0.2318        0.2735         -0.0987\nfemale1                                    0.7682        0.7265          0.0987\nprevDMTefficacyNone                        0.4118        0.5422         -0.2649\nprevDMTefficacyLow_efficacy                0.1114        0.1135         -0.0065\nprevDMTefficacyMedium_high_efficacy        0.4768        0.3443          0.2651\nprerelapse_num                             0.4595        0.3930          0.0976\n                                    Var. Ratio eCDF Mean eCDF Max\ndistance                                0.7873    0.1917   0.3379\nage                                     1.3868    0.1519   0.3085\nfemale0                                      .    0.0417   0.0417\nfemale1                                      .    0.0417   0.0417\nprevDMTefficacyNone                          .    0.1304   0.1304\nprevDMTefficacyLow_efficacy                  .    0.0020   0.0020\nprevDMTefficacyMedium_high_efficacy          .    0.1324   0.1324\nprerelapse_num                          1.1990    0.0133   0.0383\n\nSummary of Balance for Matched Data:\n                                    Means Treated Means Control Std. Mean Diff.\ndistance                                   0.7970        0.7970          0.0001\nage                                       44.2496       44.1364          0.0116\nfemale0                                    0.2318        0.2517         -0.0470\nfemale1                                    0.7682        0.7483          0.0470\nprevDMTefficacyNone                        0.4118        0.4157         -0.0079\nprevDMTefficacyLow_efficacy                0.1114        0.1224         -0.0347\nprevDMTefficacyMedium_high_efficacy        0.4768        0.4619          0.0297\nprerelapse_num                             0.4595        0.4654         -0.0087\n                                    Var. Ratio eCDF Mean eCDF Max\ndistance                                0.9955    0.0012   0.0116\nage                                     1.0161    0.0076   0.0260\nfemale0                                      .    0.0199   0.0199\nfemale1                                      .    0.0199   0.0199\nprevDMTefficacyNone                          .    0.0039   0.0039\nprevDMTefficacyLow_efficacy                  .    0.0109   0.0109\nprevDMTefficacyMedium_high_efficacy          .    0.0148   0.0148\nprerelapse_num                          0.9530    0.0057   0.0110\n                                    Std. Pair Dist.\ndistance                                     0.0022\nage                                          0.1688\nfemale0                                      0.5149\nfemale1                                      0.5149\nprevDMTefficacyNone                          0.1816\nprevDMTefficacyLow_efficacy                  0.5944\nprevDMTefficacyMedium_high_efficacy          0.4731\nprerelapse_num                               0.3893\n\nSample Sizes:\n              Control Treated\nAll           2300.      7700\nMatched (ESS)  198.89    7700\nMatched       2300.      7700\nUnmatched        0.         0\nDiscarded        0.         0\n\nplot(summary(opt))\n\n\n\n# black line is treated group, grey line is control group\nplot(opt, type = \"density\", which.xs = vars) \n\n\n\n\n\n\n\n\n\n3.4.3 Estimating the ATT\nWe can estimate the ATT in the matched sample using Poisson regression in which the number of post-treatment relapses is regressed on treatment status and follow-up time for each patient (captured by the variable years). More details are provided at .\n\n# Matched data\nmatched.data <- match.data(opt)\n\n# Poisson regression model\nopt.fit <- glm(y ~ treatment + offset(log(years)), \n            family = poisson(link = \"log\"),\n            data = matched.data, \n            weights = weights)\n\n# Treatment effect estimation\nopt.comp <- comparisons(opt.fit,\n                        variables = \"treatment\",\n                        vcov = ~subclass,\n                        newdata = subset(matched.data, treatment == \"DMT1\"),\n                        wts = \"weights\",\n                        transform_pre = \"ratio\")\n\nopt.comp |> tidy()\n\n# A tibble: 1 × 9\n  type     term      contrast   estim…¹ std.e…² stati…³  p.value conf.…⁴ conf.…⁵\n  <chr>    <chr>     <chr>        <dbl>   <dbl>   <dbl>    <dbl>   <dbl>   <dbl>\n1 response treatment mean(DMT1…   0.761   0.100    7.59 3.21e-14   0.564   0.958\n# … with abbreviated variable names ¹​estimate, ²​std.error, ³​statistic,\n#   ⁴​conf.low, ⁵​conf.high\n\n\nAs indicated in the summary output above, the annualized relapse rate ratio for DMT1 vs DMT0 among patients treated with DMT0 (ATT) is given as 0.76 with a 95% confidence interval ranging from 0.56 to 0.96."
  },
  {
    "objectID": "chapter_06.html#propensity-score-stratification",
    "href": "chapter_06.html#propensity-score-stratification",
    "title": "3  Confounding adjustment using propensity score methods",
    "section": "3.5 Propensity score stratification",
    "text": "3.5 Propensity score stratification\n\n3.5.1 Divide sample into quintiles of propensity scores\nWe will form five mutually exclusive groups of the estimated propensity score.\n\n# Create five strata\ndat <- dat %>% mutate(ps.strata = cut(ps, \n                                      breaks = c(quantile(ps, probs=seq(0,1,0.2))),\n                                      labels = seq(1:5),\n                                      include.lowest = TRUE))\n\n# Number of patients in each stratum\ntable(dat$ps.strata)\n\n\n   1    2    3    4    5 \n2002 2015 1991 1997 1995 \n\n\n\n\n3.5.2 Assess balance within each propensity score stratum\nWithin each propensity score stratum, treated and control patients should have similar values of the propensity score and the distribution of baseline covariates should be approximately balanced between treatment groups.\n\n3.5.2.1 Propensity Score Stratum #1\n\ntab1.strata1 <- CreateTableOne(vars, data = dat %>% filter(ps.strata == 1), \n                               factorVars = c(\"female\", \"prevDMTefficacy\"), \n                               strata = \"treatment\", test = FALSE)\n\ntab1.strata1.print <- print(tab1.strata1, catDigits = 2, contDigits = 2, \n                            smd = TRUE)\n\n\n\n\n\n\n\nDMT0\nDMT1\nSMD\n\n\n\n\nn\n901\n1101\n\n\n\nage (mean (SD))\n58.38 (3.67)\n57.45 (3.73)\n0.251\n\n\nfemale = 1 (%)\n605 (67.15)\n775 (70.39)\n0.070\n\n\nprevDMTefficacy (%)\n\n\n0.056\n\n\nNone\n650 (72.14)\n771 (70.03)\n\n\n\nLow_efficacy\n106 (11.76)\n130 (11.81)\n\n\n\nMedium_high_efficacy\n145 (16.09)\n200 (18.17)\n\n\n\nprerelapse_num (mean (SD))\n0.29 (0.53)\n0.33 (0.56)\n0.074\n\n\n\n\n\n\n\n3.5.2.2 Propensity Score Stratum #2\n\ntab1.strata2 <- CreateTableOne(vars, data = dat %>% filter(ps.strata == 2), \n                               factorVars = c(\"female\", \"prevDMTefficacy\"), \n                               strata = \"treatment\", test = FALSE)\n\ntab1.strata2.print <- print(tab1.strata2, catDigits = 2, contDigits = 2, \n                            smd = TRUE)\n\n\n\n\n\n\n\nDMT0\nDMT1\nSMD\n\n\n\n\nn\n617\n1398\n\n\n\nage (mean (SD))\n52.18 (4.35)\n51.97 (4.22)\n0.049\n\n\nfemale = 1 (%)\n458 (74.23)\n1048 (74.96)\n0.017\n\n\nprevDMTefficacy (%)\n\n\n0.054\n\n\nNone\n292 (47.33)\n624 (44.64)\n\n\n\nLow_efficacy\n69 (11.18)\n162 (11.59)\n\n\n\nMedium_high_efficacy\n256 (41.49)\n612 (43.78)\n\n\n\nprerelapse_num (mean (SD))\n0.40 (0.64)\n0.41 (0.66)\n0.004\n\n\n\n\n\n\n\n3.5.2.3 Propensity Score Stratum #3\n\ntab1.strata3 <- CreateTableOne(vars, data = dat %>% filter(ps.strata == 3), \n                               factorVars = c(\"female\", \"prevDMTefficacy\"), \n                               strata = \"treatment\", test = FALSE)\n\ntab1.strata3.print <- print(tab1.strata3, catDigits = 2, contDigits = 2, \n                            smd = TRUE)\n\n\n\n\n\n\n\nDMT0\nDMT1\nSMD\n\n\n\n\nn\n392\n1599\n\n\n\nage (mean (SD))\n46.73 (4.06)\n46.36 (4.08)\n0.092\n\n\nfemale = 1 (%)\n305 (77.81)\n1193 (74.61)\n0.075\n\n\nprevDMTefficacy (%)\n\n\n0.041\n\n\nNone\n168 (42.86)\n687 (42.96)\n\n\n\nLow_efficacy\n52 (13.27)\n191 (11.94)\n\n\n\nMedium_high_efficacy\n172 (43.88)\n721 (45.09)\n\n\n\nprerelapse_num (mean (SD))\n0.49 (0.68)\n0.47 (0.66)\n0.031\n\n\n\n\n\n\n\n3.5.2.4 Propensity Score Stratum #4\n\ntab1.strata4 <- CreateTableOne(vars, data = dat %>% filter(ps.strata == 4), \n                               factorVars = c(\"female\", \"prevDMTefficacy\"), \n                               strata = \"treatment\", test = FALSE)\n\ntab1.strata4.print <- print(tab1.strata4, catDigits = 2, contDigits = 2, \n                            smd = TRUE)\n\n\n\n\n\n\n\nDMT0\nDMT1\nSMD\n\n\n\n\nn\n269\n1728\n\n\n\nage (mean (SD))\n41.07 (4.11)\n40.88 (4.29)\n0.046\n\n\nfemale = 1 (%)\n203 (75.46)\n1356 (78.47)\n0.071\n\n\nprevDMTefficacy (%)\n\n\n0.084\n\n\nNone\n105 (39.03)\n634 (36.69)\n\n\n\nLow_efficacy\n22 ( 8.18)\n181 (10.47)\n\n\n\nMedium_high_efficacy\n142 (52.79)\n913 (52.84)\n\n\n\nprerelapse_num (mean (SD))\n0.50 (0.69)\n0.51 (0.71)\n0.012\n\n\n\n\n\n\n\n3.5.2.5 Propensity Score Stratum #5\n\ntab1.strata5 <- CreateTableOne(vars, data = dat %>% filter(ps.strata == 5), \n                               factorVars = c(\"female\", \"prevDMTefficacy\"), \n                               strata = \"treatment\", test = FALSE)\n\ntab1.strata5.print <- print(tab1.strata5, catDigits = 2, contDigits = 2, \n                            smd = TRUE)\n\n\n\n\n\n\n\nDMT0\nDMT1\nSMD\n\n\n\n\nn\n121\n1874\n\n\n\nage (mean (SD))\n33.26 (4.95)\n32.04 (5.58)\n0.233\n\n\nfemale = 1 (%)\n100 (82.64)\n1543 (82.34)\n0.008\n\n\nprevDMTefficacy (%)\n\n\n0.050\n\n\nNone\n32 (26.45)\n455 (24.28)\n\n\n\nLow_efficacy\n12 ( 9.92)\n194 (10.35)\n\n\n\nMedium_high_efficacy\n77 (63.64)\n1225 (65.37)\n\n\n\nprerelapse_num (mean (SD))\n0.52 (0.66)\n0.52 (0.73)\n0.004\n\n\n\n\n\n\n\n\n3.5.3 Estimating and pooling of stratum-specific treatment effects\nThe overall ATT across strata can be estimated by weighting stratum-specific estimates by the proportion of treated patients in each stratum over all treated patients in the sample.\nWe first define a function att.strata.function() to calculate stratum-specific estimates of the treatment effect:\n\natt.strata.function <- function(data, stratum, confint = TRUE) {\n\n  fit <- glm(\"y ~ treatment + offset(log(years))\",\n      family = poisson(link = \"log\"),\n      data = data %>% filter(ps.strata == stratum))\n\n  arr <- round(as.numeric(exp(coef(fit)[\"treatmentDMT1\"])), digits = 3)\n  ll <- ul <- NA\n  \n  if (confint) {\n    ll <- round(exp(confint(fit))[\"treatmentDMT1\",1], digits = 3)\n    ul <- round(exp(confint(fit))[\"treatmentDMT1\",2], digits = 3)\n  }\n  \n  return(c(\"stratum\" = stratum,\n           \"arr\" = arr,\n           \"ci_lower\"  = ll,\n           \"ci_upper\"  = ul))\n}\n\narr.strata <- as.data.frame(t(sapply(1:5, att.strata.function, data = dat)))\narr.strata\n\n  stratum   arr ci_lower ci_upper\n1       1 0.904    0.760    1.076\n2       2 0.822    0.696    0.975\n3       3 0.798    0.666    0.961\n4       4 0.716    0.587    0.881\n5       5 0.589    0.463    0.761\n\n\nSubsequently, we define a function weights.strata.function() to calculate the weights for each stratum. The weight is the proportion of treated patients in each stratum over all treated patients in the sample:\n\nweights.strata.function <- function(data, stratum) {\n  n_DMT1_stratum <- nrow(data %>% filter(ps.strata == stratum & treatment == \"DMT1\"))\n  n_DMT1_all <- nrow(data %>% filter(treatment == \"DMT1\"))\n  weight <- n_DMT1_stratum/n_DMT1_all\n  return(c(\"stratum\" = stratum, \"weight\" = weight))\n}\n\nweights.strata <- as.data.frame(t(sapply(1:5, weights.strata.function, data = dat)))\nweights.strata\n\n  stratum    weight\n1       1 0.1429870\n2       2 0.1815584\n3       3 0.2076623\n4       4 0.2244156\n5       5 0.2433766\n\n\n\n# Create table with ARRs and weights for each PS stratum\narr.weights.merged <- merge(arr.strata, weights.strata, by = \"stratum\")\n\n# Calculate the weighted ARR for each stratum\narr.weights.merged <- arr.weights.merged %>%\n  mutate(weighted.arr = as.numeric(arr) * weight)\n\n# Sum the weighted ARRs across strata to get the overall ATT\nsum(arr.weights.merged$weighted.arr)\n\n[1] 0.7482462\n\n\n\n\n\nWe now define a new function ps.stratification.bootstrap() that integrates estimation of the ATT and the PS weights for bootstrapping purposes:\n\nps.stratification.bootstrap <- function(data, inds) {\n  d <- data[inds,]\n  \n  d$ps.strata <- cut(d$ps, \n                       breaks = c(quantile(dat$ps, probs = seq(0, 1, by = 0.2))),\n                       labels = seq(5),\n                       include.lowest = TRUE)\n  \n  arr.strata <- as.data.frame(t(sapply(1:5, att.strata.function, \n                                       data = d, confint = FALSE)))\n  \n  weights.strata <- as.data.frame(t(sapply(1:5, weights.strata.function, data = d)))\n  \n  return(arr.strata$arr[1] * weights.strata$weight[1] + \n           arr.strata$arr[2] * weights.strata$weight[2] +\n           arr.strata$arr[3] * weights.strata$weight[3] + \n           arr.strata$arr[4] * weights.strata$weight[4] +\n           arr.strata$arr[5] * weights.strata$weight[5])                                                  \n}\n\nWe can now estimate the treatment effect and its confidence interval using the bootstrap procedure:\n\nlibrary(boot)\n\n\nAttaching package: 'boot'\n\n\nThe following object is masked from 'package:survival':\n\n    aml\n\nset.seed(1854)\narr.stratification.boot <- boot(data = dat, \n                                statistic = ps.stratification.bootstrap, \n                                R = 1000)\n\n# Bootstrapped ARR\nmedian(arr.stratification.boot$t)\n\n[1] 0.7558609\n\n# Bootstrapped ARR 95% CI\nquantile(arr.stratification.boot$t[,1], c(0.025, 0.975))\n\n     2.5%     97.5% \n0.6835885 0.8362947"
  },
  {
    "objectID": "chapter_06.html#propensity-score-weighting",
    "href": "chapter_06.html#propensity-score-weighting",
    "title": "3  Confounding adjustment using propensity score methods",
    "section": "3.6 Propensity score weighting",
    "text": "3.6 Propensity score weighting\n\n3.6.1 Calculate propensity score weights for ATT\nPropensity score weighting reweights the study sample to generate an artificial population (i.e., pseudo-population) in which the covariates are no longer associated with treatment, thereby removing confounding by measured covariates. For the ATT, the weight for all treated patients is set to one. Conversely, the weight for patients in the control group is set to the propensity score divided by one minus the propensity score, that is, (PS/(1 − PS)). We estimated stabilized weights to address extreme weights.\n\nlibrary(WeightIt)\n\nw.out <- weightit(treatment ~ age + female + prevDMTefficacy + prerelapse_num,\n                  data = dat,\n                  method = \"ps\",\n                  estimand = \"ATT\")\n                  #stabilize = TRUE)\n\nw.out\n\nA weightit object\n - method: \"ps\" (propensity score weighting)\n - number of obs.: 10000\n - sampling weights: none\n - treatment: 2-category\n - estimand: ATT (focal: DMT1)\n - covariates: age, female, prevDMTefficacy, prerelapse_num\n\nsummary(w.out)\n\n                 Summary of weights\n\n- Weight ranges:\n\n        Min                                   Max\nDMT0 0.4772 |---------------------------| 48.6856\nDMT1 1.0000  ||                            1.0000\n\n- Units with 5 most extreme weights by group:\n                                             \n         9492    8836    6544    9610    4729\n DMT0 32.1027 32.1027 34.3126 38.1817 48.6856\n            6       4       3       2       1\n DMT1       1       1       1       1       1\n\n- Weight statistics:\n\n     Coef of Var   MAD Entropy # Zeros\nDMT0       1.098 0.673   0.383       0\nDMT1       0.000 0.000  -0.000       0\n\n- Effective Sample Sizes:\n\n              DMT0 DMT1\nUnweighted 2300.   7700\nWeighted   1043.16 7700\n\nplot(summary(w.out))\n\n\n\n\n\n\n3.6.2 Assess balance in the weighted sample\n\nbal.tab(w.out, stats = c(\"m\", \"v\"), thresholds = c(m = .05))\n\nBalance Measures\n                                         Type Diff.Adj     M.Threshold\nprop.score                           Distance  -0.0045 Balanced, <0.05\nage                                   Contin.   0.0054 Balanced, <0.05\nfemale                                 Binary   0.0005 Balanced, <0.05\nprevDMTefficacy_None                   Binary  -0.0003 Balanced, <0.05\nprevDMTefficacy_Low_efficacy           Binary   0.0023 Balanced, <0.05\nprevDMTefficacy_Medium_high_efficacy   Binary  -0.0020 Balanced, <0.05\nprerelapse_num                        Contin.  -0.0034 Balanced, <0.05\n                                     V.Ratio.Adj\nprop.score                                0.9926\nage                                       1.0102\nfemale                                         .\nprevDMTefficacy_None                           .\nprevDMTefficacy_Low_efficacy                   .\nprevDMTefficacy_Medium_high_efficacy           .\nprerelapse_num                            1.0941\n\nBalance tally for mean differences\n                    count\nBalanced, <0.05         7\nNot Balanced, >0.05     0\n\nVariable with the greatest mean difference\n Variable Diff.Adj     M.Threshold\n      age   0.0054 Balanced, <0.05\n\nEffective sample sizes\n              DMT0 DMT1\nUnadjusted 2300.   7700\nAdjusted   1043.16 7700\n\n\n\n\n3.6.3 Estimate the ATT\nOne way to estimate the ATT is to use the survey package. The function svyglm() generates model-robust (Horvitz-Thompson-type) standard errors by default, and thus does not require additional adjustments.\n\nlibrary(survey)\n\nweighted.data <- svydesign(ids = ~1, data = dat, weights = ~w.out$weights)\n\nweighted.fit <- svyglm(y ~ treatment + offset(log(years)),\n                       family = poisson(link = \"log\"),\n                       design = weighted.data)\n\nexp(coef(weighted.fit)[\"treatmentDMT1\"])\n\ntreatmentDMT1 \n    0.7083381 \n\nexp(confint(weighted.fit))[\"treatmentDMT1\",] \n\n    2.5 %    97.5 % \n0.6245507 0.8033662 \n\n\n\n\n\nAs indicated above, propensity score weighting yielded an ATT estimate of 0.71 (95% CI: 0.66; 0.76).\nAn alternative approach is to use glm() to estimate the treatment effect and calculate robust standard errors.\n\n# Alternative way to estimate treatment effect\nweighted.fit2 <- glm(y ~ treatment + offset(log(years)),\n              family = poisson(link = \"log\"),\n              data = dat,\n              weights = w.out$weights)\n\n# Extract the estimated ARR\nexp(coef(weighted.fit2))[\"treatmentDMT1\"]\n\ntreatmentDMT1 \n    0.7083381 \n\n# Calculate robust standard error and p-value of the log ARR\ncoeftest(weighted.fit2, vcov. = vcovHC)[\"treatmentDMT1\",]\n\n     Estimate    Std. Error       z value      Pr(>|z|) \n-3.448337e-01  6.442745e-02 -5.352280e+00  8.685284e-08 \n\n# Derive 95% confidence interval of the ARR\nexp(lmtest::coefci(weighted.fit2, \n       level = 0.95, # 95% confidence interval\n       vcov. = vcovHC)[\"treatmentDMT1\",])\n\n    2.5 %    97.5 % \n0.6243094 0.8036767 \n\n\n\n\n\nUsing this approach, the ATT estimate was 0.71 (95% CI: 0.62; 0.8)."
  },
  {
    "objectID": "chapter_06.html#regression-adjustment-for-the-propensity-score-for-the-ate",
    "href": "chapter_06.html#regression-adjustment-for-the-propensity-score-for-the-ate",
    "title": "3  Confounding adjustment using propensity score methods",
    "section": "3.7 Regression adjustment for the propensity score for the ATE",
    "text": "3.7 Regression adjustment for the propensity score for the ATE\nIn this approach, a regression model is fitted to describe the observed outcome as a function of the received treatment and the estimated propensity score:\n\nps.reg.fit <- glm(y ~ treatment + ps + offset(log(years)),\n                  family = poisson(link = \"log\"),\n                  data = dat)\n\nsummary(ps.reg.fit)\n\n\nCall:\nglm(formula = y ~ treatment + ps + offset(log(years)), family = poisson(link = \"log\"), \n    data = dat)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.0160  -0.7336  -0.4441  -0.1352   4.2634  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   -1.99585    0.10359 -19.266  < 2e-16 ***\ntreatmentDMT1 -0.25598    0.04431  -5.777 7.60e-09 ***\nps             1.07521    0.13878   7.748 9.36e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 7514.7  on 9999  degrees of freedom\nResidual deviance: 7443.0  on 9997  degrees of freedom\nAIC: 12378\n\nNumber of Fisher Scoring iterations: 6\n\n# ATE\nexp(coef(ps.reg.fit))[\"treatmentDMT1\"] \n\ntreatmentDMT1 \n    0.7741606 \n\n\n\n\nWaiting for profiling to be done...\nWaiting for profiling to be done...\n\n\nBootstrapped confidence intervals can be obtained as follows:\n\n# Function to bootstrap for 95% CIs\nps.reg.bootstrap <- function(data, inds) {\n  d <- data[inds,]\n  \n  fit <- glm(y ~ treatment + ps + offset(log(years)),\n              family = poisson(link = \"log\"),\n              data = d)\n  \n  return(exp(coef(fit))[\"treatmentDMT1\"])\n}\n\nset.seed(1854)\n\n# Generate 1000 bootstrap replicates\narr.boot <- boot(dat, statistic = ps.reg.bootstrap, R = 1000) \n\n# Extract the median annualized relapse rate across 1000 bootstrap replicates\nmedian(arr.boot$t) \n\n[1] 0.7750426\n\n# Take 2.5th and 97.5th percentiles to be 95% CI\nquantile(arr.boot$t[,1], c(0.025, 0.975)) \n\n     2.5%     97.5% \n0.7010540 0.8545169"
  },
  {
    "objectID": "chapter_06.html#overview",
    "href": "chapter_06.html#overview",
    "title": "3  Confounding adjustment using propensity score methods",
    "section": "3.8 Overview",
    "text": "3.8 Overview\n\n\n\n\n\n\n\n\n\n\n\n\nMethod\nEstimand\nEstimate\n95% CI (lower)\n95% CI (upper)\n\n\n\n\nOptimal full matching\nATT\n0.8039901\n0.6040414\n1.0039388\n\n\nPropensity score stratification\nATT\n0.7482462\nNA\nNA\n\n\nPropensity score stratification (with bootstrapping)\nATT\n0.7558609\n0.6835885\n0.8362947\n\n\nPropensity score weighting\nATT\n0.7083381\n0.6245507\n0.8033662\n\n\nPropensity score weighting (robust SE)\nATT\n0.7083381\n0.6243094\n0.8036767\n\n\nPS regression adjustment\nATE\n0.7741606\n0.7101080\n0.8448218\n\n\nPS regression adjustment (bootstrapping)\nATE\n0.7750426\n0.7010540\n0.8545169"
  },
  {
    "objectID": "chapter_06.html#version-info",
    "href": "chapter_06.html#version-info",
    "title": "3  Confounding adjustment using propensity score methods",
    "section": "Version info",
    "text": "Version info\nThis chapter was rendered using the following version of R and its packages:\n\n\nR version 4.2.3 (2023-03-15 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19044)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=Dutch_Netherlands.utf8  LC_CTYPE=Dutch_Netherlands.utf8   \n[3] LC_MONETARY=Dutch_Netherlands.utf8 LC_NUMERIC=C                      \n[5] LC_TIME=Dutch_Netherlands.utf8    \n\nattached base packages:\n[1] grid      stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] MatchIt_4.5.3          sandwich_3.0-2         WeightIt_0.14.1       \n [4] truncnorm_1.0-9        tableone_0.13.2        survey_4.2-1          \n [7] survival_3.5-5         Matrix_1.5-4           MASS_7.3-58.3         \n[10] marginaleffects_0.12.0 lmtest_0.9-40          zoo_1.8-12            \n[13] knitr_1.42             ggplot2_3.4.2          data.table_1.14.8     \n[16] cobalt_4.5.1           boot_1.3-28.1          dplyr_1.1.1           \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.10       pillar_1.9.0      compiler_4.2.3    tools_4.2.3      \n [5] digest_0.6.31     lattice_0.21-8    jsonlite_1.8.4    evaluate_0.21    \n [9] lifecycle_1.0.3   tibble_3.2.1      gtable_0.3.3      pkgconfig_2.0.3  \n[13] rlang_1.1.0       DBI_1.1.3         cli_3.6.1         rstudioapi_0.14  \n[17] yaml_2.3.7        xfun_0.39         fastmap_1.1.1     withr_2.5.0      \n[21] mitools_2.4       generics_0.1.3    vctrs_0.6.1       htmlwidgets_1.6.2\n[25] tidyselect_1.2.0  glue_1.6.2        R6_2.5.1          fansi_1.0.4      \n[29] rmarkdown_2.21    magrittr_2.0.3    splines_4.2.3     scales_1.2.1     \n[33] backports_1.4.1   htmltools_0.5.5   colorspace_2.1-0  utf8_1.2.3       \n[37] munsell_0.5.0     crayon_1.5.2"
  },
  {
    "objectID": "chapter_06.html#references",
    "href": "chapter_06.html#references",
    "title": "3  Confounding adjustment using propensity score methods",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "chapter_07.html#simulation",
    "href": "chapter_07.html#simulation",
    "title": "4  Effect Modification Analysis within the Propensity score Framework",
    "section": "4.1 Simulation",
    "text": "4.1 Simulation\nFirst, we need to install the R package simcausal, which can be obtained from GitHub:\n\ndevtools::install_github('osofr/simcausal', build_vignettes = FALSE)\n\nWe will use the following data-generation model:\n\nrequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"age\", distr = \"rnorm\", \n       mean = 2, sd = 4) + \n  node(\"gender\", distr = \"rbern\", \n       prob = plogis(4)) +\n  node(\"education\", distr = \"rbern\", \n       prob = plogis(3 + 5 * age)) +\n  node(\"diet\", distr = \"rbern\", \n       prob = plogis(1 - 3 * education)) +\n  node(\"income\", distr = \"rbern\", \n       prob = plogis(2 - 5 * education - 4 * age)) +\n  node(\"smoking\", distr = \"rbern\", \n       prob = plogis(1 + 1.2 * gender + 2 * age)) +\n  node(\"hypertension\", distr = \"rbern\", \n       prob = plogis(1 + log(3) * diet + \n                       log(1.3) * age + \n                       log(3.5) * smoking + \n                       log(0.5) * gender))\nDset <- set.DAG(D)\n\nBelow is the diagram, with pink lines representing open backdoor path.\n\n\nusing the following vertex attributes: \n\n\nNAdarkbluenone100.50\n\n\nusing the following edge attributes: \n\n\nblack0.210.60.5\n\n\n\n\n\nWe can now generate an example dataset:\n\nObs.Data <- sim(DAG = Dset, n = 50000, rndseed = 123)\nObs.Data$smoking <- as.character(Obs.Data$smoking)\nObs.Data$income <- as.factor(Obs.Data$income)\nObs.Data$income <- relevel(Obs.Data$income, ref = \"1\")\n\nSample data from the hypothetical example of association between hypertension and smoking, where other variables such as income, age [centered], gender, education and diet also plays a role in the data generation process.\n\n\n\n\n \n  \n      \n    age \n    gender \n    education \n    diet \n    income \n    smoking \n    hypertension \n  \n \n\n  \n    34901 \n    12.29 \n    1 \n    1 \n    1 \n    0 \n    1 \n    1 \n  \n  \n    149 \n    10.40 \n    1 \n    1 \n    0 \n    0 \n    1 \n    1 \n  \n  \n    10060 \n    2.99 \n    1 \n    1 \n    0 \n    0 \n    1 \n    0 \n  \n  \n    22220 \n    -4.31 \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n  \n  \n    9979 \n    -6.44 \n    0 \n    0 \n    0 \n    1 \n    0 \n    1"
  },
  {
    "objectID": "chapter_07.html#covariate-adjustment",
    "href": "chapter_07.html#covariate-adjustment",
    "title": "4  Effect Modification Analysis within the Propensity score Framework",
    "section": "4.2 Covariate adjustment",
    "text": "4.2 Covariate adjustment\n\n4.2.1 Interaction approach\nBelow, we estimate a logistic regression model to assess whether the effect of smoking (the exposure) on hypertension is modified by income levels. This model considers the following variables:\n\nOutcome: hypertension\nExposure variables: smoking and income\nConfounders: age and gender\n\n\nrequire(jtools)\n\nfit.w.em <- glm(hypertension ~ smoking * income + age + gender, \n            family = binomial(link = \"logit\"), data = Obs.Data)\n\nresults.model <- summ(fit.w.em, exp = TRUE)\n\n\n\n\n\n \n  \n      \n    exp(Est.) \n    2.5% \n    97.5% \n    z val. \n    p \n  \n \n\n  \n    (Intercept) \n    5.46 \n    4.37 \n    6.82 \n    14.97 \n    0.00 \n  \n  \n    smoking1 \n    2.93 \n    2.60 \n    3.30 \n    17.69 \n    0.00 \n  \n  \n    income0 \n    0.48 \n    0.41 \n    0.57 \n    -8.28 \n    0.00 \n  \n  \n    age \n    1.29 \n    1.27 \n    1.31 \n    36.77 \n    0.00 \n  \n  \n    gender \n    0.54 \n    0.43 \n    0.67 \n    -5.55 \n    0.00 \n  \n  \n    smoking1:income0 \n    1.27 \n    1.04 \n    1.56 \n    2.33 \n    0.02 \n  \n\n\n\n\n\nResults indicate that the interaction between smoking status and income level is statistically significant (p = 0.02).\nIf we expand previous model to adjust for an additional confounder education, we have:\n\nfit.w.int <- glm(hypertension ~ smoking * income + age + gender + education, \n                 family = binomial(link = \"logit\"), \n                 data = Obs.Data)\n\nresults.int.model <- summ(fit.w.int, exp = TRUE)\n\n\n\n\n\n \n  \n      \n    exp(Est.) \n    2.5% \n    97.5% \n    z val. \n    p \n  \n \n\n  \n    (Intercept) \n    5.69 \n    4.56 \n    7.11 \n    15.31 \n    0.00 \n  \n  \n    smoking1 \n    3.35 \n    2.95 \n    3.79 \n    18.85 \n    0.00 \n  \n  \n    income0 \n    1.09 \n    0.85 \n    1.40 \n    0.68 \n    0.49 \n  \n  \n    age \n    1.30 \n    1.28 \n    1.32 \n    37.32 \n    0.00 \n  \n  \n    gender \n    0.54 \n    0.43 \n    0.67 \n    -5.58 \n    0.00 \n  \n  \n    education \n    0.42 \n    0.35 \n    0.51 \n    -8.87 \n    0.00 \n  \n  \n    smoking1:income0 \n    1.10 \n    0.90 \n    1.35 \n    0.93 \n    0.35 \n  \n\n\n\n\n\nThe interaction term between income and smoking is no longer statistically significant (p = 0.35).\nWe can generate a summary report from aforementioned effect modification analysis.\n\nrequire(interactionR)\n\nem.object <- interactionR(fit.w.em, \n                          exposure_names = c(\"income0\", \"smoking1\"), \n                          ci.type = \"mover\", ci.level = 0.95, \n                          em = TRUE, recode = FALSE)\n\nThe table below depicts the adjusted odds ratios for income levels (high = 0, and low = 1). The variables CI.ll and CI.ul depict the lower and upper limits of the 95 percent confidence intervals, OR11 = \\(OR_{A = 1, M = 1}\\) , OR10 = \\(OR_{A = 1}\\), OR01 = \\(OR_{M = 1}\\) and OR00 captures the reference.\n\n\n\n\nTable 4.1:  Summary report from an interaction analysis when investigating association between two exposure variables (smoking and income) and hypertension. \n \n  \n    Measures \n    Estimates \n    CI.ll \n    CI.ul \n  \n \n\n  \n    OR00 \n    1.00 \n    NA \n    NA \n  \n  \n    OR01 \n    2.93 \n    2.60 \n    3.30 \n  \n  \n    OR10 \n    0.48 \n    0.41 \n    0.57 \n  \n  \n    OR11 \n    1.80 \n    1.63 \n    1.98 \n  \n  \n    OR(smoking1 on outcome [income0==0] \n    2.93 \n    2.60 \n    3.30 \n  \n  \n    OR(smoking1 on outcome [income0==1] \n    3.72 \n    3.14 \n    4.41 \n  \n  \n    Multiplicative scale \n    1.27 \n    1.04 \n    1.56 \n  \n  \n    RERI \n    -0.61 \n    -0.98 \n    -0.29 \n  \n\n\n\n\n\n\nSimilarly, for the analysis adjusting for an additional confounder education, we have:\n\n\n\n\nTable 4.2:  Summary report from an interaction analysis when investigating association between two exposure variables (smoking and income) and hypertension. \n \n  \n    Measures \n    Estimates \n    CI.ll \n    CI.ul \n  \n \n\n  \n    OR00 \n    1.00 \n    NA \n    NA \n  \n  \n    OR01 \n    1.09 \n    0.85 \n    1.40 \n  \n  \n    OR10 \n    3.35 \n    2.95 \n    3.79 \n  \n  \n    OR11 \n    4.02 \n    3.29 \n    4.92 \n  \n  \n    OR(income0 on outcome [smoking1==0] \n    1.09 \n    0.85 \n    1.40 \n  \n  \n    OR(income0 on outcome [smoking1==1] \n    1.20 \n    1.00 \n    1.45 \n  \n  \n    OR(smoking1 on outcome [income0==0] \n    3.35 \n    2.95 \n    3.79 \n  \n  \n    OR(smoking1 on outcome [income0==1] \n    3.69 \n    3.11 \n    4.37 \n  \n  \n    Multiplicative scale \n    1.10 \n    0.90 \n    1.35 \n  \n  \n    RERI \n    0.59 \n    0.03 \n    1.27 \n  \n  \n    AP \n    0.15 \n    0.00 \n    0.26 \n  \n  \n    SI \n    1.24 \n    1.01 \n    1.53 \n  \n\n\n\n\n\n\n\n# test run with additive model\nObs.Data$smoking <- as.numeric(as.character(Obs.Data$smoking))\nObs.Data$income <- as.numeric(as.character(Obs.Data$income))\nfit.w.int.add <- glm(hypertension ~ smoking * income + age + gender + education, \n                     family = gaussian(link = \"identity\"), data = Obs.Data)\nsim_slopes(fit.w.int.add, pred = smoking, modx = income,\n           exp = TRUE, robust = TRUE,\n           confint = TRUE, data = Obs.Dat)\n\nJOHNSON-NEYMAN INTERVAL \n\nWhen income is INSIDE the interval [-3.27, 16.87], the slope of smoking is\np < .05.\n\nNote: The range of observed values of income is [0.00, 1.00]\n\nSIMPLE SLOPES ANALYSIS \n\nSlope of smoking when income = 0.00 (0): \n\n  Est.   S.E.   2.5%   97.5%   t val.      p\n------ ------ ------ ------- -------- ------\n  0.25   0.02   1.24    1.34    12.76   0.00\n\nSlope of smoking when income = 1.00 (1): \n\n  Est.   S.E.   2.5%   97.5%   t val.      p\n------ ------ ------ ------- -------- ------\n  0.28   0.01   1.30    1.34    34.53   0.00\n\n\n\n\n4.2.2 Stratification\nThis approach involves estimating a regression model in different strata of the discrete effect modifier income:\n\n# Estimate the prognostic effect of smoking in low income individuals\nfit.income1 <- glm(hypertension ~ smoking + age + gender, \n            family = binomial(link = \"logit\"), \n            data = subset(Obs.Data, income == 1))\n\n# Estimate the prognostic effect of smoking in high income individuals\nfit.income0 <- glm(hypertension ~ smoking + age + gender, \n            family = binomial(link = \"logit\"), \n            data = subset(Obs.Data, income == 0))\n\nThe table below summarizes the adjusted odds ratios for smoking across the different income levels (low = 1, and high = 0) as obtained using the stratified approach.\n\n\n\n\n\n\n\n \n  \n    Value of income \n    Estimate \n    2.5 % \n    97.5 % \n    z value \n    p value \n  \n \n\n  \n    1 \n    3.07 \n    2.71 \n    3.47 \n    17.65 \n    0 \n  \n  \n    0 \n    3.59 \n    3.02 \n    4.26 \n    14.57 \n    0 \n  \n\n\n\n\n\nNote that we can obtain the same results by estimating a regression model with an interaction term between the modifier and all covariates:\n\nfit.all.int <- glm(hypertension ~ income * (smoking + age + gender), \n                   family = binomial(link = \"logit\"), data = Obs.Data)\n\n# Odds ratio for smoking in individuals with low income \nexp(coef(fit.all.int)[\"smoking\"])\n\nsmoking \n3.59026 \n\n# Odds ratio for smoking in individuals with high income\nexp(coef(fit.all.int)[\"smoking\"] + coef(fit.all.int)[\"income:smoking\"])\n\n smoking \n3.066878"
  },
  {
    "objectID": "chapter_07.html#propensity-score-matching",
    "href": "chapter_07.html#propensity-score-matching",
    "title": "4  Effect Modification Analysis within the Propensity score Framework",
    "section": "4.3 Propensity score matching",
    "text": "4.3 Propensity score matching\n\n4.3.1 Stratification with exact matching within subgroups\nWe simulate another example dataset using aforementioned DAG, but restrict the sample size to 5000 individuals to reduce computational burden.\n\nset.seed(123)\nObs.Data <- sim(DAG = Dset, n = 5000, rndseed = 123)\n\nWe first estimate the propensity of smoking in the high-income group (income == 0):\n\nrequire(MatchIt)\n\nmatch.income.0 <- matchit(smoking ~ age + gender, \n                          data = subset(Obs.Data, income == 0),\n                          method = \"full\", distance = \"glm\", link = \"logit\")\ndata.income.0 <- match.data(match.income.0)\n\nBelow, we draw a sample from the high-income group based on the hypothetical example of an association between hypertension and smoking. Here age [centered], gender, education, and diet are covariates.\n\n\n            age gender education diet income smoking hypertension  distance\n657   6.0810120      0         1    1      0       1            1 0.9999874\n4932  1.6109860      1         1    0      0       1            0 0.9943155\n252  -0.2475055      1         1    1      0       0            1 0.8525107\n2693 -0.2511048      1         1    0      0       1            1 0.8516785\n1646 -0.2836155      1         0    1      0       1            1 0.8439843\n        weights subclass\n657  1.00000000       36\n4932 1.00000000       50\n252  0.03296089       25\n2693 1.00000000       25\n1646 1.00000000        4\n\n\nNow, we do the same for the low-income group (income == 1):\n\nmatch.income.1 <- matchit(smoking ~ age + gender, \n                          data = subset(Obs.Data, income == 1),\n                          method = \"full\", distance = \"glm\", link = \"logit\")\ndata.income.1 <- match.data(match.income.1)\n\nWe estimated the exposure effect from a weighted outcome model for the matched data. While the weights are essential for estimating the point estimate from the outcome model, the subclass variable assists in calculating the robust variance of the exposure effect estimate.\n\n# Treatment effect estimation\nfit.income.0 <- glm(hypertension ~ smoking + age + gender, \n                   data = data.income.0, weights = weights,\n                   family = binomial(\"logit\"))\nfit.income.1 <- glm(hypertension ~ smoking + age + gender, \n                   data = data.income.1, weights = weights,\n                   family = binomial(\"logit\"))\n# Robust variance calculation\nfit.nexp.adj.res1 <- summ(fit.income.1,  \n                          robust = TRUE,\n                          cluster = \"subclass\",\n                          confint = TRUE)\nfit.nexp.adj.res0 <- summ(fit.income.0, \n                          robust = TRUE,\n                          cluster = \"subclass\",\n                          confint = TRUE)\n\n\n\n\n\nTable 4.3:  Subgroup-specific treatment effect estimates (expressed in log-OR) from the hypothetical example using the stratified approach. \n \n  \n    Value of income \n    Est. \n    2.5% \n    97.5% \n    z val. \n    p \n  \n \n\n  \n    0 \n    3.74 \n    -37.58 \n    45.06 \n    0.18 \n    0.86 \n  \n  \n    1 \n    1.39 \n    0.94 \n    1.85 \n    6.04 \n    0.00 \n  \n\n\n\n\n\n\n\n\n4.3.2 Joint approach without exact matching within subgroups\nHere, entire cohort data is used to estimate the propensity scores, and the effect modifier income is considered as a covariate in the propensity score model:\n\nps.formula <- as.formula(\"smoking ~ age + gender + income\")\nmatch.obj.j <- matchit(ps.formula, data = Obs.Data,\n                      method = \"full\", \n                      distance = \"glm\",\n                      link = \"logit\")\nmatch.data.j <- match.data(match.obj.j)\n\n\nfit.joint.no.exact <- glm(hypertension ~ smoking*income + age + gender, \n                          data = match.data.j, \n                          weights = weights,\n                          family = binomial(\"logit\"))\nrequire(interactions)\nnem.nexp.adj.res <- sim_slopes(fit.joint.no.exact, \n                               pred = smoking, \n                               modx = income,\n                               robust = \"HC1\", \n                               cluster = \"subclass\",\n                               johnson_neyman = TRUE, \n                               confint = TRUE,\n                               data = match.data.j)\n\n\n\n\n\nTable 4.4:  Subgroup-specific treatment effect estimates (expressed in log-OR) from the hypothetical example using the joint approach. \n \n  \n    Value of income \n    Est. \n    S.E. \n    2.5% \n    97.5% \n    z val. \n    p \n  \n \n\n  \n    0 \n    3.85 \n    1.00 \n    1.89 \n    5.82 \n    3.84 \n    0 \n  \n  \n    1 \n    1.40 \n    0.28 \n    0.85 \n    1.95 \n    4.99 \n    0 \n  \n\n\n\n\n\n\n\n\n4.3.3 Joint approach with exact matching within subgroups\nWe specify the moderator variable’s name in the exact argument of the matchit function.\n\nps.formula.no.mod <- as.formula(\"smoking ~ age + gender\")\nmatch.obj.js <- matchit(ps.formula.no.mod, data = Obs.Data,\n                        method = \"full\", distance = \"glm\",link = \"logit\",\n                        exact = \"income\")\nmatch.data.js <- match.data(match.obj.js)\nfit.joint.exact <- glm(hypertension ~ smoking*income + age + gender, \n                       data = match.data.js, weights = weights,\n                       family = binomial(\"logit\"))\njs.nexp.adj.res <- sim_slopes(fit.joint.exact, \n                              pred = smoking, modx = income,\n                              robust = \"HC1\", cluster = \"subclass\",\n                              johnson_neyman = FALSE, confint = TRUE,\n                              data = match.data.js)\n\n\n\n\n\nTable 4.5:  Subgroup-specific exposure effect estimates (expressed in log-OR) from the hypothetical example using the Joint model, separate matching approach. \n \n  \n    Value of income \n    Est. \n    S.E. \n    2.5% \n    97.5% \n    z val. \n    p \n  \n \n\n  \n    0 \n    3.89 \n    1.01 \n    1.92 \n    5.87 \n    3.87 \n    0 \n  \n  \n    1 \n    1.38 \n    0.28 \n    0.84 \n    1.93 \n    4.95 \n    0 \n  \n\n\n\n\n\n\n\n\n4.3.4 Interaction approach without exact matching within subgroups\nAnalysts incorporate relevant moderator-covariate interactions into the propensity score model that align with biological plausibility. For instance, in the case study we considered an interaction between age (a covariate) and income (a moderator), but did not include other interactions terms.\n\nps.formula.with.int <- formula(\"smoking ~ age*income + gender\")\nmatch.obj.i <- matchit(ps.formula.with.int, data = Obs.Data,\n                       method = \"full\", distance = \"glm\",link = \"logit\")\nmatch.data.i <- match.data(match.obj.i)\nfit.int.no.exact <- glm(hypertension ~ smoking*income + age + gender, \n                        data = match.data.i, weights = weights,\n                        family = binomial(\"logit\"))\ni.nexp.adj.res <- sim_slopes(fit.int.no.exact, \n                             pred = smoking, modx = income,\n                             robust = \"HC1\", cluster = \"subclass\",\n                             johnson_neyman = FALSE, confint = TRUE,\n                             data = match.data.i)\n\n\n\n\n\nTable 4.6:  Subgroup-specific exposure effect estimates (expressed in log-OR) from the hypothetical example using the interaction approach. \n \n  \n    Value of income \n    Est. \n    S.E. \n    2.5% \n    97.5% \n    z val. \n    p \n  \n \n\n  \n    0 \n    3.87 \n    1.00 \n    1.90 \n    5.83 \n    3.86 \n    0 \n  \n  \n    1 \n    1.39 \n    0.28 \n    0.84 \n    1.94 \n    4.95 \n    0 \n  \n\n\n\n\n\n\n\n\n4.3.5 Interaction approach with exact matching within subgroups\nThis method bears resemblance to the interaction approach for propensity score estimation. However, when it comes to matching, researchers match within each moderator subgroup.\n\nmatch.obj.is <- matchit(ps.formula.with.int, data = Obs.Data,\n                      method = \"full\", distance = \"glm\",link = \"logit\",\n                      exact = \"income\")\nmatch.data.is <- match.data(match.obj.is)\nfit.int.exact <- glm(hypertension ~ smoking*income + age + gender, \n                     data = match.data.is, weights = weights,\n                     family = binomial(\"logit\"))\nis.nexp.adj.res <- sim_slopes(fit.int.exact, \n                              pred = smoking, modx = income,\n                              robust = \"HC1\", cluster = \"subclass\",\n                              johnson_neyman = FALSE, confint = TRUE,\n                              data = match.data.is)\n\n\n\n\n\nTable 4.7:  Subgroup-specific exposure effect estimates (expressed in log-OR) from the hypothetical example using the interaction model, separate matching approach. \n \n  \n    Value of income \n    Est. \n    S.E. \n    2.5% \n    97.5% \n    z val. \n    p \n  \n \n\n  \n    0 \n    3.86 \n    1.00 \n    1.90 \n    5.83 \n    3.85 \n    0 \n  \n  \n    1 \n    1.40 \n    0.28 \n    0.85 \n    1.95 \n    4.99 \n    0"
  },
  {
    "objectID": "chapter_07.html#propensity-score-weighting",
    "href": "chapter_07.html#propensity-score-weighting",
    "title": "4  Effect Modification Analysis within the Propensity score Framework",
    "section": "4.4 Propensity Score Weighting",
    "text": "4.4 Propensity Score Weighting\n\n4.4.1 Common model\nThis approach adds confounder-moderator interactions in the common weight model.\n\nrequire(WeightIt)\nW.out <- weightit(ps.formula.with.int, \n                  data = Obs.Data,\n                  method = \"ps\", \n                  estimand = \"ATT\")\nrequire(survey)\nd.w <- svydesign(~1, weights = W.out$weights, data = Obs.Data)\nfit2w <- svyglm(hypertension ~ smoking*income, design = d.w,\n                family = binomial(\"logit\"))\nw.nexp.adj.res <- sim_slopes(fit2w, pred = smoking, modx = income, \n                             confint = TRUE)\n\n\n\n\n\nTable 4.8:  Subgroup-specific exposure effect estimates (expressed in log-OR) from the hypothetical example using the weighting approach. \n \n  \n    Value of income \n    Est. \n    S.E. \n    2.5% \n    97.5% \n    t val. \n    p \n  \n \n\n  \n    0 \n    2.66 \n    0.63 \n    1.42 \n    3.89 \n    4.23 \n    0 \n  \n  \n    1 \n    1.32 \n    0.25 \n    0.83 \n    1.82 \n    5.24 \n    0 \n  \n\n\n\n\n\n\nWe can adjust previous analysis model to adopt stabilized weights for the propensity score (stabilize = TRUE):\n\nW.out.st <- weightit(ps.formula.with.int, data = Obs.Data,\n                     method = \"ps\", \n                     estimand = \"ATT\", \n                     stabilize = TRUE)\nd.sw <- svydesign(~1, weights = W.out.st$weights, data = Obs.Data)\nfit2sw <- svyglm(hypertension ~ smoking*income + age + gender, \n                  design = d.sw,\n                  family = binomial(\"logit\"))\nws.nexp.adj.res <- sim_slopes(fit2sw, \n                              pred = smoking, modx = income, \n                              confint = TRUE)\n\n\n\n\n\n\n\n\nTable 4.9:  Subgroup-specific exposure effect estimates (expressed in log-OR) from the hypothetical example using stabilized propensity score weights. \n \n  \n    Value of income \n    Est. \n    S.E. \n    2.5% \n    97.5% \n    t val. \n    p \n  \n \n\n  \n    0 \n    2.27 \n    0.73 \n    0.84 \n    3.69 \n    3.12 \n    0 \n  \n  \n    1 \n    1.32 \n    0.25 \n    0.83 \n    1.82 \n    5.23 \n    0 \n  \n\n\n\n\n\n\n\n\n4.4.2 Separate models\nPropensity score weighting approach with weights estimated separately from each subgroup:\n\nps.formula.with.no.int <- formula(\"smoking ~ age + gender\")\nW.out1 <- weightit(ps.formula.with.no.int, \n                   data = subset(Obs.Data, income == 1),\n                   method = \"ps\", \n                   estimand = \"ATT\")\ntrimmed.weight.1.percent1 <- trim(W.out1$weights, \n                                  at = 1, lower = TRUE)\n\n\n\n\n\nTable 4.10:  Weight summaries before and after truncation. \n \n  \n    Weight \n    Min. \n    1st Qu. \n    Median \n    Mean \n    3rd Qu. \n    Max. \n  \n \n\n  \n    Raw weights \n    0 \n    0.01 \n    0.11 \n    0.45 \n    1 \n    11.69 \n  \n  \n    1% truncated weights \n    0 \n    0.01 \n    0.11 \n    0.44 \n    1 \n    7.61 \n  \n\n\n\n\n\n\n\n# Outcome model for income = 1\nd.w1 <- svydesign(~1, weights = trimmed.weight.1.percent1, \n                  data = subset(Obs.Data, income == 1))\nfit2unadj1 <- svyglm(hypertension ~ smoking, design = d.w1,\n                     family = binomial(\"logit\"))\n\n# weight model for income = 0\nW.out0 <- weightit(ps.formula, data = subset(Obs.Data, income == 0),\n                  method = \"ps\", estimand = \"ATT\")\ntrimmed.weight.1.percent0 <- trim(W.out0$weights, at = 1, lower = TRUE)\n\n# Outcome model for income = 0\nd.w0 <- svydesign(~1, weights = trimmed.weight.1.percent0, \n                  data = subset(Obs.Data, income == 0))\nfit2unadj0 <- svyglm(hypertension ~ smoking, design = d.w0,\n                     family = binomial(\"logit\"))\n\nfit.exp.adj.res1 <- summ(fit2unadj1, confint = TRUE)\nfit.exp.adj.res0 <- summ(fit2unadj0, confint = TRUE)\n\n\n\n\n\nTable 4.11:  Subgroup-specific exposure effect estimates (expressed in log-OR) from the hypothetical example using the propensity score weighting approach (Separate weight models). \n \n  \n    Value of income \n    Est. \n    2.5% \n    97.5% \n    t val. \n    p \n  \n \n\n  \n    0 \n    2.21 \n    1.27 \n    3.15 \n    4.60 \n    0 \n  \n  \n    1 \n    1.34 \n    0.85 \n    1.83 \n    5.36 \n    0 \n  \n\n\n\n\n\n\n\n\n4.4.3 Weights from the subgroup balancing propensity scores\nSubgroup balancing propensity scores for propensity score weighting:\n\nw.out <- weightit(smoking ~ age + gender + income, \n                data = Obs.Data,\n                method = \"ps\", estimand = \"ATT\")\nw.out.sb <- sbps(w.out, moderator = \"income\")\nd.w.sb <- svydesign(~1, weights = w.out.sb$weights, data = Obs.Data)\nfit2unadj.sb <- svyglm(hypertension ~ smoking*income, design = d.w.sb,\n                       family = binomial(\"logit\"))\nsb.w.nexp.adj.res <- sim_slopes(fit2unadj.sb, \n                              pred = smoking, \n                              modx = income, \n                              confint = TRUE,\n                              johnson_neyman = FALSE,)\n\n\n\n\n\nTable 4.12:  Subgroup-specific exposure effect estimates (expressed in log-OR) from the hypothetical example using the subgroup balancing weighting approach. \n \n  \n    Value of income \n    Est. \n    S.E. \n    2.5% \n    97.5% \n    t val. \n    p \n  \n \n\n  \n    0 \n    2.68 \n    0.64 \n    1.44 \n    3.92 \n    4.22 \n    0 \n  \n  \n    1 \n    1.32 \n    0.25 \n    0.82 \n    1.82 \n    5.22 \n    0"
  },
  {
    "objectID": "chapter_07.html#covariate-adjustment-for-the-propensity-score",
    "href": "chapter_07.html#covariate-adjustment-for-the-propensity-score",
    "title": "4  Effect Modification Analysis within the Propensity score Framework",
    "section": "4.5 Covariate adjustment for the propensity score",
    "text": "4.5 Covariate adjustment for the propensity score\n\n4.5.1 As continuous covariate\nAn implementation of propensity scores as a continuous covariate in the outcome model:\n\n# Separate models for each subgroup\n\n# For subgroup income = 1 \nObs.Data$ps[Obs.Data$income == 1] <- glm(ps.formula, \n                                         data = subset(Obs.Data, income == 1), \n                                         family = \"binomial\")$fitted.values\nfit2adj1 <- glm(hypertension ~ smoking + age + gender, \n                family = binomial(\"logit\"), \n                data = subset(Obs.Data, income == 1))\n\n# For subgroup income = 0\nObs.Data$ps[Obs.Data$income == 0] <- glm(ps.formula, \n                                         data = subset(Obs.Data, income == 0), \n                                         family = \"binomial\")$fitted.values\nfit2adj0 <- glm(hypertension ~ smoking + age + gender, \n                family = binomial(\"logit\"), \n                data = subset(Obs.Data, income == 0))\n\nfit.nexp.adj.res1 <- summ(fit2adj1, robust = TRUE, confint = TRUE)\nfit.nexp.adj.res0 <- summ(fit2adj0, robust = TRUE, confint = TRUE)\n\n\n\n\n\nTable 4.13:  Subgroup-specific exposure effect estimates (expressed in log-OR) from the hypothetical example using Propensity Score as a covariate adjustment approach (considering separate models for each subgroup). \n \n  \n    Value of income \n    Est. \n    2.5% \n    97.5% \n    z val. \n    p \n  \n \n\n  \n    0 \n    1.16 \n    0.56 \n    1.75 \n    3.83 \n    0 \n  \n  \n    1 \n    1.37 \n    0.96 \n    1.77 \n    6.61 \n    0 \n  \n\n\n\n\n\n\n\n# Common model\nObs.Data$ps <- glm(ps.formula.with.int, data = Obs.Data,\n                       family = \"binomial\")$fitted.values\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nfit2adjc <- glm(hypertension ~ smoking*income + age + gender + ps, \n                family = binomial(\"logit\"), \n                data = Obs.Data)\nc.nexp.adj.res <- sim_slopes(fit2adjc,\n                             pred = smoking, modx = income,\n                             confint = TRUE,\n                             data = Obs.Data)\n\n\n\n\n\nTable 4.14:  Subgroup-specific exposure effect estimates (expressed in log-OR) from the hypothetical example using Propensity Score as a covariate adjustment approach (considering a common model). \n \n  \n    Value of income \n    Est. \n    S.E. \n    2.5% \n    97.5% \n    z val. \n    p \n  \n \n\n  \n    0 \n    1.17 \n    0.29 \n    0.61 \n    1.74 \n    4.07 \n    0 \n  \n  \n    1 \n    1.43 \n    0.23 \n    0.98 \n    1.87 \n    6.30 \n    0 \n  \n\n\n\n\n\n\n\n\n4.5.2 As quantiles\nThe propensity scores as a categorical covariate, broken by quintiles, in the outcome model.\n\nObs.Data$ps <- glm(ps.formula.with.int, \n                   data = Obs.Data, \n                   family = \"binomial\")$fitted.values\nquintiles <- quantile(Obs.Data$ps, \n                      prob = seq(from = 0, to = 1, by = 0.2), \n                      na.rm = T)\nObs.Data$psq <- cut(Obs.Data$ps, breaks = quintiles, \n                   labels = seq(1,5), include.lowest = T)\nObs.Data$psq <- as.factor(Obs.Data$psq)\n\nfit2adjq <- glm(hypertension ~ (smoking*psq)*income, \n                family = binomial(\"logit\"),\n                data = Obs.Data)\ncq.nexp.adj.res <- sim_slopes(fit2adjq, \n                              pred = smoking, \n                              modx = income, \n                              confint = TRUE,\n                              data = Obs.Data)\n\n\n\n\n\nTable 4.15:  Subgroup-specific exposure effect estimates (expressed in log-OR) from the hypothetical example using Propensity Score as a covariate adjustment approach (as quintiles). \n \n  \n    Value of income \n    Est. \n    S.E. \n    2.5% \n    97.5% \n    z val. \n    p \n  \n \n\n  \n    0 \n    3.08 \n    0.63 \n    1.85 \n    4.32 \n    4.91 \n    0 \n  \n  \n    1 \n    2.60 \n    0.47 \n    1.68 \n    3.51 \n    5.56 \n    0"
  },
  {
    "objectID": "chapter_07.html#propensity-score-stratification",
    "href": "chapter_07.html#propensity-score-stratification",
    "title": "4  Effect Modification Analysis within the Propensity score Framework",
    "section": "4.6 Propensity Score Stratification",
    "text": "4.6 Propensity Score Stratification\nHere is an implementation of propensity score stratification approach by using the marginal mean weighting through stratification (MMWS):\n\nmatch.obj <- matchit(ps.formula, data = Obs.Data,\n                      method = \"subclass\", subclass = 3, \n                      estimand = \"ATT\", min.n = 10)\ndata.subclass <- match.data(match.obj)\nsubclass.fit <- glm(hypertension ~ smoking*income, family = binomial(\"logit\"),\n              data = data.subclass,\n              weights = weights)\nsubclass.nexp.adj.res <- sim_slopes(subclass.fit, \n                                    pred = smoking, \n                                    modx = income, \n                                    confint = TRUE,\n                                    robust = \"HC3\",\n                                    johnson_neyman = FALSE,\n                                    data = data.subclass)\n\n\n\n\n\nTable 4.16:  Subgroup-specific exposure effect estimates (expressed in log-OR) from the hypothetical example using propensity score stratification approach. \n \n  \n    Value of income \n    Est. \n    S.E. \n    2.5% \n    97.5% \n    z val. \n    p \n  \n \n\n  \n    0 \n    2.21 \n    0.47 \n    1.29 \n    3.13 \n    4.71 \n    0 \n  \n  \n    1 \n    1.89 \n    0.19 \n    1.51 \n    2.26 \n    9.78 \n    0"
  },
  {
    "objectID": "chapter_07.html#summary",
    "href": "chapter_07.html#summary",
    "title": "4  Effect Modification Analysis within the Propensity score Framework",
    "section": "4.7 Summary",
    "text": "4.7 Summary\nThe marginal odds ratios for smoking are summarized below"
  },
  {
    "objectID": "chapter_07.html#version-info",
    "href": "chapter_07.html#version-info",
    "title": "4  Effect Modification Analysis within the Propensity score Framework",
    "section": "Version info",
    "text": "Version info\nThis chapter was rendered using the following version of R and its packages:\n\n\nR version 4.2.3 (2023-03-15 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=Dutch_Netherlands.utf8  LC_CTYPE=Dutch_Netherlands.utf8   \n[3] LC_MONETARY=Dutch_Netherlands.utf8 LC_NUMERIC=C                      \n[5] LC_TIME=Dutch_Netherlands.utf8    \n\nattached base packages:\n[1] grid      stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] DiagrammeR_1.0.10  scales_1.2.1       ggplot2_3.4.2      interactionR_0.1.6\n [5] simcausal_0.5.6    xtable_1.8-4       dplyr_1.1.1        kableExtra_1.3.4  \n [9] knitr_1.42         cowplot_1.1.1      readstata13_0.10.1 Publish_2023.01.17\n[13] prodlim_2023.03.31 survey_4.2-1       survival_3.5-5     Matrix_1.5-4      \n[17] broom_1.0.4        MatchIt_4.5.3      interactions_1.1.5 jtools_2.2.1      \n[21] sandwich_3.0-2     lmtest_0.9-40      zoo_1.8-12         optmatch_0.10.6   \n[25] WeightIt_0.14.1    cobalt_4.5.1       table1_1.4.3      \n\nloaded via a namespace (and not attached):\n [1] colorspace_2.1-0        ellipsis_0.3.2          flextable_0.9.1        \n [4] rstudioapi_0.14         httpcode_0.3.0          listenv_0.9.0          \n [7] fansi_1.0.4             mvtnorm_1.1-3           xml2_1.3.3             \n[10] codetools_0.2-19        splines_4.2.3           Formula_1.2-5          \n[13] jsonlite_1.8.4          shiny_1.7.4             compiler_4.2.3         \n[16] httr_1.4.6              backports_1.4.1         assertthat_0.2.1       \n[19] fastmap_1.1.1           cli_3.6.1               later_1.3.0            \n[22] visNetwork_2.1.2        htmltools_0.5.5         tools_4.2.3            \n[25] igraph_1.4.2            gtable_0.3.3            glue_1.6.2             \n[28] Rcpp_1.0.10             msm_1.7                 carData_3.0-5          \n[31] fontquiver_0.2.1        vctrs_0.6.1             crul_1.4.0             \n[34] svglite_2.1.1           xfun_0.39               stringr_1.5.0          \n[37] globals_0.16.2          rvest_1.0.3             mime_0.12              \n[40] lifecycle_1.0.3         future_1.32.0           ragg_1.2.5             \n[43] promises_1.2.0.1        parallel_4.2.3          expm_0.999-7           \n[46] RColorBrewer_1.1-3      fontLiberation_0.1.0    yaml_2.3.7             \n[49] curl_5.0.0              pander_0.6.5            gdtools_0.3.3          \n[52] stringi_1.7.12          fontBitstreamVera_0.1.1 zip_2.3.0              \n[55] lava_1.7.2.1            rlang_1.1.0             pkgconfig_2.0.3        \n[58] systemfonts_1.0.4       evaluate_0.21           lattice_0.21-8         \n[61] purrr_1.0.1             htmlwidgets_1.6.2       tidyselect_1.2.0       \n[64] parallelly_1.35.0       magrittr_2.0.3          R6_2.5.1               \n[67] generics_0.1.3          DBI_1.1.3               withr_2.5.0            \n[70] pillar_1.9.0            abind_1.4-5             tibble_3.2.1           \n[73] future.apply_1.10.0     crayon_1.5.2            car_3.1-2              \n[76] gfonts_0.2.0            uuid_1.1-0              utf8_1.2.3             \n[79] rmarkdown_2.21          officer_0.6.2           data.table_1.14.8      \n[82] digest_0.6.31           webshot_0.5.4           tidyr_1.3.0            \n[85] httpuv_1.6.9            textshaping_0.3.6       openssl_2.0.6          \n[88] munsell_0.5.0           viridisLite_0.4.2       mitools_2.4            \n[91] askpass_1.1"
  },
  {
    "objectID": "chapter_09.html#setup",
    "href": "chapter_09.html#setup",
    "title": "5  Dealing with missing data",
    "section": "5.1 Setup",
    "text": "5.1 Setup\n\n5.1.1 Prepare R environment\n\nlibrary(mice)\nlibrary(dplyr)\nlibrary(ggmice)\nlibrary(MatchThem)\n\n\n\n5.1.2 Generating an observational dataset\nWe can simulate an observational dataset of \\(N = 3000\\) patients as follows:\n\ndata_noHTE <- generate_data(n = 3000, seed = 1234) \n\nThis dataset does not (yet) contain any missing values;\nThe simulated dataset contains two treatment groups with differences in baseline characteristics. For example, the figure below shows that we have baseline imbalance in age.\n\n\n\n\n\nFigure 5.1: Distribution of the EDSS score at each time point\n\n\n\n\n\n\n\n\n\n5.1.3 Generating missing values\nMissing values can be generated using the function getmissdata(), which considers the following patterns of missingness for the previous number of relapses (prerelapse_num):\n\nMAR: missingness depends on age and sex\nMART: missingness depends on age, sex and the treatment variable treatment\nMARTY: missingness depends on age, sex, treatment and the outcome variable y\nMNAR: missingness depends on age, sex and prerelapse_num\n\n\nmdata_noHTE <- getmissdata(data_noHTE, \"MART\")\n\nAfter introducing missing values, we only have complete data for \\(N=\\) 946 patients.\n\n\n\nBaseline characteristics of the incomplete dataset.\n\n\n\n\nDMF(N=2265)\nTERI(N=735)\nOverall(N=3000)\n\n\n\n\nAge (years)\n\n\n\n\n\nMean (SD)\n44.4 (10.0)\n51.3 (8.72)\n46.2 (10.1)\n\n\nMedian [Min, Max]\n45.0 [18.0, 64.0]\n53.0 [23.0, 64.0]\n47.0 [18.0, 64.0]\n\n\nMissing\n248 (10.9%)\n57 (7.8%)\n305 (10.2%)\n\n\nFemale Sex\n\n\n\n\n\nYes\n1740 (76.8%)\n526 (71.6%)\n2266 (75.5%)\n\n\nNo\n525 (23.2%)\n209 (28.4%)\n734 (24.5%)\n\n\nEfficacy of previous DMT\n\n\n\n\n\nNone\n740 (32.7%)\n325 (44.2%)\n1065 (35.5%)\n\n\nLow\n190 (8.4%)\n59 (8.0%)\n249 (8.3%)\n\n\nMedium or High\n830 (36.6%)\n246 (33.5%)\n1076 (35.9%)\n\n\nMissing\n505 (22.3%)\n105 (14.3%)\n610 (20.3%)\n\n\nPrior medical costs\n\n\n\n\n\nMean (SD)\n9970 (10700)\n25500 (35400)\n13900 (21200)\n\n\nMedian [Min, Max]\n6530 [164, 102000]\n12500 [259, 337000]\n7450 [164, 337000]\n\n\nMissing\n257 (11.3%)\n52 (7.1%)\n309 (10.3%)\n\n\nNumber of prior symptoms\n\n\n\n\n\n0\n157 (6.9%)\n58 (7.9%)\n215 (7.2%)\n\n\n1\n1169 (51.6%)\n411 (55.9%)\n1580 (52.7%)\n\n\n>=2\n435 (19.2%)\n159 (21.6%)\n594 (19.8%)\n\n\nMissing\n504 (22.3%)\n107 (14.6%)\n611 (20.4%)\n\n\nNumber of prior relapses\n\n\n\n\n\nMean (SD)\n0.453 (0.671)\n0.408 (0.646)\n0.436 (0.662)\n\n\nMedian [Min, Max]\n0 [0, 4.00]\n0 [0, 3.00]\n0 [0, 4.00]\n\n\nMissing\n1365 (60.3%)\n152 (20.7%)\n1517 (50.6%)"
  },
  {
    "objectID": "chapter_09.html#analysis-of-incomplete-data",
    "href": "chapter_09.html#analysis-of-incomplete-data",
    "title": "5  Dealing with missing data",
    "section": "5.2 Analysis of incomplete data",
    "text": "5.2 Analysis of incomplete data\n\n5.2.1 Complete Case Analysis\nBelow, we describe how to estimate the ATE using propensity score matching.\n\nimpdata <- mdata_noHTE[complete.cases(mdata_noHTE), ]\n\n# Apply Matching\nmout <- matchit(DMF ~ age + female + prevDMTefficacy + premedicalcost + prerelapse_num, \n                data = impdata,\n                family = binomial,\n                method = \"full\",\n                caliper = 0.2,\n                estimand = \"ATE\",\n                replace = FALSE) \n\nmdata <- as.data.table(match.data(mout))\nmatch_mod <- glm(\"y ~ DMF + offset(log(years))\",\n                 family = poisson(link = \"log\"),\n                 data = mdata,\n                 weights = weights)\n\n# Estimate robust variance-covariance matrix\ntx_var <- vcovCL(match_mod, cluster = ~ subclass, sandwich = TRUE) \n\nWe can extract the treatment effect estimate as follows:\n\n# Treatment effect estimate (log rate ratio)\ncoef(match_mod)[\"DMF\"]\n\n       DMF \n-0.3685717 \n\n# Standard error\nsqrt(tx_var[\"DMF\", \"DMF\"])\n\n[1] 0.1521243\n\n\n\n\n\n\n\n5.2.2 Multiple Imputation (within method)\nIn this approach, we will generate \\(m=5\\) imputed datasets and perform matching within each imputed dataset. We first need to specify how the variables prevDMTefficacy, premedicalcost, numSymptoms, prerelapse_num and age will be imputed:\n\n# We add a covariate for log(years)\nimpdata <-  mdata_noHTE %>% mutate(logyears = log(years))\n\n# Specify the conditional imputation models\nform_y <- list(prevDMTefficacy ~ age + female + logyears + premedicalcost + numSymptoms + \n                 treatment + prerelapse_num + y,\n               premedicalcost ~ age + female + logyears + prevDMTefficacy + numSymptoms + \n                 treatment + prerelapse_num + y,\n               numSymptoms ~ age + female + premedicalcost + logyears + prevDMTefficacy + \n                 prerelapse_num + treatment + y,\n               prerelapse_num ~ age + female + premedicalcost + logyears + prevDMTefficacy + \n                 numSymptoms + treatment + y,\n               age ~ prerelapse_num + female + premedicalcost + logyears + prevDMTefficacy + \n                 numSymptoms + treatment + y)\nform_y <- name.formulas(form_y)\n\n# Adopt predictive mean matching for imputing the incomplete variables\nimp0 <- mice(impdata, form = form_y, maxit = 0)\nmethod <- imp0$method\nmethod[\"numSymptoms\"] <- \"pmm\"\nmethod[\"prevDMTefficacy\"] <- \"pmm\"\n\n# Generate 5 imputed datasets\nimp <- mice(impdata, form = form_y, method = method, m = 5, maxit = 100)\n\n\n\n\nWe can now estimate the ATE using propensity score analysis in each of the imputed datasets. We here adopt full matching without replacement.\n\n# Matching based on PS model\nmout <- matchthem(DMF ~ age + female + prevDMTefficacy + premedicalcost + prerelapse_num,\n                  datasets = imp,\n                  approach = \"within\",\n                  method = \"full\",\n                  caliper = 0.2,\n                  family = binomial,\n                  estimand = \"ATE\",\n                  distance = \"glm\",\n                  link = \"logit\",\n                  replace = FALSE) \n\nThe results are then combined using Rubin’s rules. We adopt robust standard errors to account for clustering of matched individuals.\n\nmatch_mod <- summary(pool(with(mout, svyglm(y ~ DMF + offset(log(years)), \n                                            family = poisson(link = \"log\")),\n                               cluster = TRUE)), conf.int = TRUE)\n\nWe can extract the treatment effect estimate and corresponding standard error as follows:\n\n# Treatment effect estimate (log rate ratio)\n(match_mod %>% filter(term == \"DMF\"))$estimate\n\n[1] -0.1554094\n\n# Standard error\n(match_mod %>% filter(term == \"DMF\"))$std.error\n\n[1] 0.2202132\n\n\n\n\n\n\n\n5.2.3 Multiple Imputation (across method)\n\n# Matching based on PS model\nmout <- matchthem(DMF ~ age + female + prevDMTefficacy + premedicalcost + prerelapse_num,\n                  datasets = imp,\n                  approach = \"across\",\n                  method = \"full\",\n                  caliper = 0.2,\n                  family = binomial,\n                  estimand = \"ATE\",\n                  distance = \"glm\",\n                  link = \"logit\",\n                  replace = FALSE) \n\nThe results are then combined using Rubin’s rules. We adopt robust standard errors to account for clustering of matched individuals.\n\nmatch_mod <- summary(pool(with(mout, svyglm(y ~ DMF + offset(log(years)), \n                                            family = poisson(link = \"log\")),\n                               cluster = TRUE)), conf.int = TRUE)\n\nWe can extract the treatment effect estimate and corresponding standard error as follows:\n\n# Treatment effect estimate (log rate ratio)\n(match_mod %>% filter(term == \"DMF\"))$estimate\n\n[1] -0.3461563\n\n# Standard error\n(match_mod %>% filter(term == \"DMF\"))$std.error\n\n[1] 0.1351187"
  },
  {
    "objectID": "chapter_09.html#convergence-checking",
    "href": "chapter_09.html#convergence-checking",
    "title": "5  Dealing with missing data",
    "section": "5.3 Convergence checking",
    "text": "5.3 Convergence checking\nWe can inspect convergence for the imputed variable prerelapse_num using a trace plot:\n\nplot_trace(imp, vrb = \"prerelapse_num\")"
  },
  {
    "objectID": "chapter_09.html#results",
    "href": "chapter_09.html#results",
    "title": "5  Dealing with missing data",
    "section": "5.4 Results",
    "text": "5.4 Results\nAnalysis methods:\n\nFull Data: The treatment effect is estimated in the original data of \\(N=3000\\) patients where no missing values are present. This estimate can be used as a benchmark to compare the missing data methods.\nComplete Case Analysis: The treatment effect is estimated using all data from \\(N=\\) 946 patients that do not have any missing values.\nMissing Indicator: The treatment effect is estimated in the incomplete dataset of \\(N=3000\\) patients. The propensity score model includes a missing indicator variable for each incomplete covariate.\nMICE (within method): A treatment effect is estimated within each imputed dataset using propensity score analysis. Using Rubin’s rule, the five treatment effects are combined into a single treatment effect.\nMICE (ITE method): The missing covariates and potential outcomes are imputed simultaneously. Treatment effect estimates are derived by taking the average of the individualized treatment effect estimates Y|DMF - Y|TERI."
  },
  {
    "objectID": "chapter_09.html#version-info",
    "href": "chapter_09.html#version-info",
    "title": "5  Dealing with missing data",
    "section": "Version info",
    "text": "Version info\nThis chapter was rendered using the following version of R and its packages:\n\n\nR version 4.2.3 (2023-03-15 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19044)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=Dutch_Netherlands.utf8  LC_CTYPE=Dutch_Netherlands.utf8   \n[3] LC_MONETARY=Dutch_Netherlands.utf8 LC_NUMERIC=C                      \n[5] LC_TIME=Dutch_Netherlands.utf8    \n\nattached base packages:\n[1] grid      stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] ggmice_0.0.1      table1_1.4.3      kableExtra_1.3.4  ggplot2_3.4.2    \n [5] missForest_1.5    sandwich_3.0-2    PSweight_1.1.8    MatchThem_1.0.1  \n [9] mice_3.15.0       cobalt_4.5.1      WeightIt_0.14.1   MatchIt_4.5.3    \n[13] optmatch_0.10.6   truncnorm_1.0-9   MASS_7.3-58.3     survey_4.2-1     \n[17] survival_3.5-5    Matrix_1.5-4      data.table_1.14.8 tidyr_1.3.0      \n[21] dplyr_1.1.1      \n\nloaded via a namespace (and not attached):\n [1] nlme_3.1-162         webshot_0.5.4        RColorBrewer_1.1-3  \n [4] httr_1.4.6           numDeriv_2016.8-1.1  tools_4.2.3         \n [7] backports_1.4.1      doRNG_1.8.6          utf8_1.2.3          \n[10] R6_2.5.1             DBI_1.1.3            colorspace_2.1-0    \n[13] nnet_7.3-19          withr_2.5.0          gbm_2.1.8.1         \n[16] tidyselect_1.2.0     compiler_4.2.3       cli_3.6.1           \n[19] rvest_1.0.3          see_0.7.5            xml2_1.3.3          \n[22] labeling_0.4.2       scales_1.2.1         nnls_1.4            \n[25] randomForest_4.7-1.1 systemfonts_1.0.4    stringr_1.5.0       \n[28] digest_0.6.31        minqa_1.2.5          rmarkdown_2.21      \n[31] svglite_2.1.1        pkgconfig_2.0.3      htmltools_0.5.5     \n[34] lme4_1.1-32          fastmap_1.1.1        itertools_0.1-3     \n[37] htmlwidgets_1.6.2    rlang_1.1.0          rstudioapi_0.14     \n[40] generics_0.1.3       farver_2.1.1         zoo_1.8-12          \n[43] jsonlite_1.8.4       magrittr_2.0.3       Formula_1.2-5       \n[46] Rcpp_1.0.10          munsell_0.5.0        fansi_1.0.4         \n[49] lifecycle_1.0.3      chk_0.8.1            stringi_1.7.12      \n[52] yaml_2.3.7           parallel_4.2.3       crayon_1.5.2        \n[55] lattice_0.21-8       splines_4.2.3        knitr_1.42          \n[58] pillar_1.9.0         boot_1.3-28.1        rngtools_1.5.2      \n[61] codetools_0.2-19     glue_1.6.2           evaluate_0.21       \n[64] mitools_2.4          vctrs_0.6.1          nloptr_2.0.3        \n[67] foreach_1.5.2        gtable_0.3.3         purrr_1.0.1         \n[70] xfun_0.39            SuperLearner_2.0-28  broom_1.0.4         \n[73] viridisLite_0.4.2    tibble_3.2.1         iterators_1.0.14    \n[76] gam_1.22-2           rlemon_0.2.1"
  },
  {
    "objectID": "chapter_10.html#introduction",
    "href": "chapter_10.html#introduction",
    "title": "6  Systematic review and meta-analysis of Real-World Evidence",
    "section": "6.1 Introduction",
    "text": "6.1 Introduction\nWe first load the required packages\n\nlibrary(dplyr)\nlibrary(gemtc)\nlibrary(netmeta)"
  },
  {
    "objectID": "chapter_10.html#pairwise-meta-analysis-of-clinical-trials",
    "href": "chapter_10.html#pairwise-meta-analysis-of-clinical-trials",
    "title": "6  Systematic review and meta-analysis of Real-World Evidence",
    "section": "6.2 Pairwise meta-analysis of clinical trials",
    "text": "6.2 Pairwise meta-analysis of clinical trials\n\n6.2.1 Toculizumab for coronavirus disease 2019\nIn this example, we consider the results from a systematic literature review of clinical trials investigating any pharmacological in hosptialized patients with coronavirus disease 2019 (Selvarajan et al. 2022). A total of 23 randomized controlled trials were included and studied seven different interventions: dexamethasone, remdesivir, tocilizumab, hydroxychloroquine, combination of lopinavir/ritonavir, favipiravir and interferon-β. We here focus on the synthesis of 7 trials that comparted toculizumab (TOCI) to standard care (STD) and collected mortality data.\n\n\n\n\n\n\n\n \n  \n    studlab \n    treat1 \n    treat2 \n    event1 \n    n1 \n    event2 \n    n2 \n  \n \n\n  \n    Hermine et al \n    TOCI \n    STD \n    7 \n    63 \n    8 \n    67 \n  \n  \n    Rosas et al \n    TOCI \n    STD \n    58 \n    294 \n    28 \n    144 \n  \n  \n    Salama et al \n    TOCI \n    STD \n    26 \n    249 \n    11 \n    128 \n  \n  \n    Salvarini et al \n    TOCI \n    STD \n    2 \n    60 \n    1 \n    66 \n  \n  \n    Stone et al \n    TOCI \n    STD \n    9 \n    161 \n    3 \n    82 \n  \n  \n    Veiga et al \n    TOCI \n    STD \n    14 \n    65 \n    6 \n    64 \n  \n\n\n\n\n\nWe now conduct a pairwise meta-analysis to assess the pooled effect of tocilizumab versus standard care. For each study, the log odds ratio and corresponding standard error is derived after which the corresponding estimates are pooled using the Mantel-Haenszel method.\n\nresults.TOCI <- metabin(event1,n1,event2,n2,studlab,data=tocilizumab,\n                        sm=\"OR\",main=\"tocilizumab vs standard care\", \n                        prediction=TRUE)\nforest(results.TOCI, leftcols = \"studlab\", rightcols = \"effect.ci\")\n\n\n\n\nAltough a random effects meta-analysis was conducted, no heterogeneity was found (\\(\\tau\\)=0, with a 95% confidence interval ranging from 0 to 0.85).\n\n\n6.2.2 Remdesivir for coronavirus disease 2019\nIn aforementioned example, a total of 4 trials compared remdesivir to standard care:"
  },
  {
    "objectID": "chapter_10.html#network-meta-analysis-of-clinical-trials",
    "href": "chapter_10.html#network-meta-analysis-of-clinical-trials",
    "title": "6  Systematic review and meta-analysis of Real-World Evidence",
    "section": "6.3 Network meta-analysis of clinical trials",
    "text": "6.3 Network meta-analysis of clinical trials\nWe here use the R packages netmeta for conducting a frequentist network meta-analysis. A detailed tutorial on the use of netmeta is available from the book Doing Meta-Analysis with R: A Hands-On Guide.\n\n6.3.1 Interventions for coronavirus disease 2019\nWe here consider data from a study which aimed to assess the comparative effectiveness of remdesivir and tocilizumab for reducing mortality in hospitalised COVID-19 patients. 80 trials were identified from two published network meta-analyses (Selvarajan et al. 2022), (Siemieniuk et al. 2020), a living COVID-19 trial database (COVID-NMA Initiative) [Covid-NMA.com], and a clinical trial database [clinicaltrials.gov]. Trials were included in this study if the patient population included hospitalized COVID-19 patients, active treatment was remdesivir or tocilizumab, comparator treatment was placebo or standard care, short-term mortality data was available, and the trial was published. 21 trials were included. For included trials, a risk of bias score was extracted from the COVID-NMA Initiative.\n\n\n\n\n \n  \n    studlab \n    treat1 \n    treat2 \n    event1 \n    n1 \n    event2 \n    n2 \n  \n \n\n  \n    Ader \n    REM \n    STD \n    34 \n    414 \n    37 \n    418 \n  \n  \n    Beigel (ACTT-1) \n    REM \n    STD \n    59 \n    541 \n    77 \n    521 \n  \n  \n    Broman \n    TOCI \n    STD \n    1 \n    57 \n    0 \n    29 \n  \n  \n    Criner \n    REM \n    STD \n    4 \n    384 \n    4 \n    200 \n  \n  \n    Declerq (COV-AID) \n    TOCI \n    STD \n    10 \n    81 \n    9 \n    74 \n  \n  \n    Gordon (REMAP-CAP) \n    TOCI \n    STD \n    83 \n    353 \n    116 \n    358 \n  \n  \n    Hermine (CORIMUNO) \n    TOCI \n    STD \n    7 \n    63 \n    8 \n    67 \n  \n  \n    Horby (RECOVERY) \n    TOCI \n    STD \n    621 \n    2022 \n    729 \n    2094 \n  \n  \n    Islam \n    REM \n    STD \n    0 \n    30 \n    0 \n    30 \n  \n  \n    Mahajan \n    REM \n    STD \n    5 \n    34 \n    3 \n    36 \n  \n  \n    Pan (WHO Solidarity) \n    REM \n    STD \n    602 \n    4146 \n    643 \n    4129 \n  \n  \n    Rosas (COVACTA) \n    TOCI \n    STD \n    58 \n    294 \n    28 \n    144 \n  \n  \n    Rutgers \n    TOCI \n    STD \n    21 \n    174 \n    34 \n    180 \n  \n  \n    Salama (EMPACTA) \n    TOCI \n    STD \n    26 \n    249 \n    11 \n    128 \n  \n  \n    Salvarani \n    TOCI \n    STD \n    2 \n    60 \n    1 \n    63 \n  \n  \n    Soin (COVINTOC) \n    TOCI \n    STD \n    11 \n    92 \n    15 \n    88 \n  \n  \n    Spinner \n    REM \n    STD \n    5 \n    384 \n    4 \n    200 \n  \n  \n    Stone (BACC-BAY) \n    TOCI \n    STD \n    9 \n    161 \n    4 \n    82 \n  \n  \n    Talaschian \n    TOCI \n    STD \n    5 \n    17 \n    4 \n    19 \n  \n  \n    Veiga (TOCIBRAS) \n    TOCI \n    STD \n    14 \n    65 \n    6 \n    64 \n  \n  \n    Wang \n    REM \n    STD \n    22 \n    158 \n    10 \n    78 \n  \n\n\n\n\n\nThe corresponding network is displayed below:\n\n\n\n\n\nEvidence network of the 21 coronavirus-19 trials\n\n\n\n\nWe use the following command to calculate the log odds ratios and corresponding standard errors for each study:\n\ncovid <- pairwise(treat = treat, event = event, n = n, studlab = studlab, sm = \"OR\")\nhead(covid)\n\n\n\n\n\n \n  \n    TE \n    seTE \n    studlab \n    treat1 \n    treat2 \n    event1 \n    n1 \n    event2 \n    n2 \n    incr \n    allstudies \n  \n \n\n  \n    -0.0819293 \n    0.2483849 \n    Ader \n    REM \n    STD \n    34 \n    414 \n    37 \n    418 \n    0.0 \n    FALSE \n  \n  \n    -0.3483875 \n    0.1851030 \n    Beigel (ACTT-1) \n    REM \n    STD \n    59 \n    541 \n    77 \n    521 \n    0.0 \n    FALSE \n  \n  \n    0.4487619 \n    1.6487159 \n    Broman \n    TOCI \n    STD \n    1 \n    57 \n    0 \n    29 \n    0.5 \n    FALSE \n  \n  \n    -0.6620566 \n    0.7125543 \n    Criner \n    REM \n    STD \n    4 \n    384 \n    4 \n    200 \n    0.0 \n    FALSE \n  \n  \n    0.0170679 \n    0.4904898 \n    Declerq (COV-AID) \n    TOCI \n    STD \n    10 \n    81 \n    9 \n    74 \n    0.0 \n    FALSE \n  \n  \n    -0.4442338 \n    0.1688337 \n    Gordon (REMAP-CAP) \n    TOCI \n    STD \n    83 \n    353 \n    116 \n    358 \n    0.0 \n    FALSE \n  \n\n\n\n\n\nBelow, we conduct a random effects network meta-analysis where we consider standard care (STD) as the control treatment. Note that we have one study where zero cell counts occur, this study will not contribute to the NMA as the log odds ratio and its standard error cannot be determined.\n\nNMA.covid <- netmeta(TE = TE, seTE = seTE, treat1 = treat1, treat2 = treat2,\n                     studlab = studlab, data = covid, sm = \"OR\", ref = \"STD\",\n                     comb.random = TRUE, common = FALSE, warn = FALSE)\nNMA.covid \n\nNumber of studies: k = 20\nNumber of pairwise comparisons: m = 20\nNumber of treatments: n = 3\nNumber of designs: d = 2\n\nRandom effects model\n\nTreatment estimate (sm = 'OR', comparison: other treatments vs 'STD'):\n         OR           95%-CI     z p-value\nREM  0.9244 [0.8023; 1.0651] -1.09  0.2766\nSTD       .                .     .       .\nTOCI 0.8301 [0.7434; 0.9268] -3.31  0.0009\n\nQuantifying heterogeneity / inconsistency:\ntau^2 = 0; tau = 0; I^2 = 0% [0.0%; 48.9%]\n\nTests of heterogeneity (within designs) and inconsistency (between designs):\n                    Q d.f. p-value\nTotal           17.04   18  0.5206\nWithin designs  17.04   18  0.5206\nBetween designs  0.00    0      --\n\n\nA league table of the treatment effect estimates is given below:\n\nnetleague(NMA.covid)\n\nLeague table (random effects model):\n                                                                        \n                     REM 0.9244 [0.8023; 1.0651]                       .\n 0.9244 [0.8023; 1.0651]                     STD 1.2047 [1.0789; 1.3451]\n 1.1136 [0.9306; 1.3326] 1.2047 [1.0789; 1.3451]                    TOCI\n\n\nWe can also present the results in a forest plot:\n\n\n\n\n\nThe figure below shows the percentage of direct and indirect evidence used for each estimated comparison.\n\n\n\n\n\nWe now consider a Bayesian random effects network meta-analysis that analyzes the observed event counts using a binomial link function.\n\nbdata <- data.frame(study = studlab,\n                    treatment = treat,\n                    responders = event,\n                    sampleSize = n)\n\nnetwork <- mtc.network(data.ab  = bdata)\n\nmodel <- mtc.model(network,\n                   likelihood = \"binom\",\n                   link = \"log\",\n                   linearModel = \"random\",\n                   n.chain = 3)\n\n\n# Adaptation\nmcmc1 <- mtc.run(model, n.adapt = 1000, n.iter = 1000, thin = 10)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 42\n   Unobserved stochastic nodes: 45\n   Total graph size: 930\n\nInitializing model\n\n# Sampling\nmcmc2 <- mtc.run(model, n.adapt = 10000, n.iter = 100000, thin = 10)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 42\n   Unobserved stochastic nodes: 45\n   Total graph size: 930\n\nInitializing model\n\n\nWe can extract the pooled treatment effect estimates from the posterior distribution. When using STD as control group, we have:\n\nsummary(relative.effect(mcmc2, t1 = \"STD\"))\n\n\nResults on the Log Risk Ratio scale\n\nIterations = 10010:110000\nThinning interval = 10 \nNumber of chains = 3 \nSample size per chain = 10000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n              Mean      SD  Naive SE Time-series SE\nd.STD.REM  -0.1076 0.09769 0.0005640      0.0008707\nd.STD.TOCI -0.1124 0.08239 0.0004757      0.0008776\nsd.d        0.1115 0.08945 0.0005164      0.0019005\n\n2. Quantiles for each variable:\n\n                2.5%      25%      50%      75%   97.5%\nd.STD.REM  -0.317634 -0.15986 -0.10409 -0.05134 0.08544\nd.STD.TOCI -0.256631 -0.16360 -0.12047 -0.07114 0.07761\nsd.d        0.003194  0.04161  0.09253  0.15950 0.33168\n\n\nThe corresponding odds ratios are as follows:\n\n\n\n\n \n  \n    Comparison \n    95% CrI \n  \n \n\n  \n    REM vs. STD \n    0.9 (0.73; 1.09) \n  \n  \n    TOCI vs. STD \n    0.89 (0.77; 1.08) \n  \n  \n    REM vs. TOCI \n    1.02 (0.74; 1.27) \n  \n\n\n\n\n\nFinally, we expand the COVID-19 network with trials investigating the effectiveness of hydroxychloroquine (HCQ), lopinavir/ritonavir (LOPI), dexamethasone (DEXA) or interferon-\\(\\beta\\) (INTB) (Selvarajan et al. 2022). The corresponding network is displayed below:\n\n\n\n\n\nEvidence network of the 33 coronavirus-19 trials\n\n\n\n\nWe conducted a random effects network meta-analysis, results are depicted below:\n\n\nNumber of studies: k = 33\nNumber of pairwise comparisons: m = 33\nNumber of treatments: n = 7\nNumber of designs: d = 6\n\nRandom effects model\n\nTreatment estimate (sm = 'OR', comparison: other treatments vs 'STD'):\n         OR           95%-CI     z p-value            95%-PI\nDEXA 0.8557 [0.7558; 0.9688] -2.46  0.0139  [0.7463; 0.9812]\nHCQ  1.1809 [0.8934; 1.5610]  1.17  0.2428  [0.8786; 1.5872]\nINTB 1.1606 [0.9732; 1.3841]  1.66  0.0973  [0.9604; 1.4026]\nLOPI 1.0072 [0.8906; 1.1392]  0.11  0.9085  [0.8794; 1.1537]\nREM  0.8983 [0.8014; 1.0070] -1.84  0.0658  [0.7913; 1.0199]\nSTD       .                .     .       .                 .\nTOCI 0.8304 [0.7410; 0.9306] -3.20  0.0014  [0.7316; 0.9426]\n\nQuantifying heterogeneity / inconsistency:\ntau^2 = 0.0004; tau = 0.0205; I^2 = 0.6% [0.0%; 42.3%]\n\nTests of heterogeneity (within designs) and inconsistency (between designs):\n                    Q d.f. p-value\nTotal           27.18   27  0.4543\nWithin designs  27.18   27  0.4543\nBetween designs  0.00    0      --\n\n\nWe can calculate the P score for each treatment as follows:\n\nnetrank(NMA.covidf)\n\n     P-score\nTOCI  0.9070\nDEXA  0.8357\nREM   0.7143\nSTD   0.4027\nLOPI  0.3899\nHCQ   0.1336\nINTB  0.1166\n\n\n\n\n6.3.2 Pharmacologic treatments for chronic obstructive pulmonary disease\nIn this example, we consider the resuls from a systematic review of randomized controlled trials on pharmacologic treatments for chronic obstructive pulmonary disease (Baker, Baker, and Coleman 2009). The primary outcome, occurrence of one or more episodes of COPD exacerbation, is binary (yes / no). For this outcome, five drug treatments (fluticasone, budesonide, salmeterol, formoterol, tiotropium) and two combinations (fluticasone + salmeterol, budesonide + formoterol) were compared to placebo. The authors considered the two combinations as separate treatments instead of evaluating the individual components.\n\ndata(Baker2009)\n\n\n\n\n\n \n  \n    study \n    year \n    id \n    treatment \n    exac \n    total \n  \n \n\n  \n    Llewellyn-Jones 1996 \n    1996 \n    1 \n    Fluticasone \n    0 \n    8 \n  \n  \n    Llewellyn-Jones 1996 \n    1996 \n    1 \n    Placebo \n    3 \n    8 \n  \n  \n    Boyd 1997 \n    1997 \n    2 \n    Salmeterol \n    47 \n    229 \n  \n  \n    Boyd 1997 \n    1997 \n    2 \n    Placebo \n    59 \n    227 \n  \n  \n    Paggiaro 1998 \n    1998 \n    3 \n    Fluticasone \n    45 \n    142 \n  \n  \n    Paggiaro 1998 \n    1998 \n    3 \n    Placebo \n    51 \n    139 \n  \n\n\n\n\n\n\nBaker <- pairwise(treat = treatment,\n                  event = exac,\n                  n = total,\n                  studlab = id,\n                  sm = \"OR\",\n                  data = Baker2009)\n\nNMA.COPD <- netmeta(TE = TE, seTE = seTE, treat1 = treat1, treat2 = treat2,\n                    studlab = studlab, data = Baker, sm=\"OR\", ref = \"Placebo\",\n                    comb.random = TRUE)\n\nWarning: Comparisons with missing TE / seTE or zero seTE not considered in\nnetwork meta-analysis.\n\n\nComparisons not considered in network meta-analysis:\n studlab                 treat1     treat2 TE seTE\n      39 Fluticasone+Salmeterol    Placebo NA   NA\n      39 Fluticasone+Salmeterol Salmeterol NA   NA\n      39             Salmeterol    Placebo NA   NA\n\nnetgraph(NMA.COPD)\n\n\n\n\n\n\n6.3.3 Advanced Therapies for Ulcerative Colitis\nIn this example, we consider a systematic literature review of Phase 3 randomized controlled trials investigating the following advanced therapies: infliximab, adalimumab, vedolizumab, golimumab, tofacitinib, ustekinumab, filgotinib, ozanimod, and upadacitinib (Panaccione et al. 2023). This review included 48 RCTs, from which 23 were found eligible for inclusion in a network meta-analysis. The included RCT populations were largely comparable in their baseline characteristics, though some heterogeneity was noted in weight, disease duration, extent of disease, and concomitant medications. A risk of bias assessment showed a low risk of bias for all included RCTs, which were all industry sponsored.\nWe here focus on the synthesis of 18 trials that contributed efficacy data for induction in bio-naive populations. The following FDA- and/or EMA-approved biologic or SMD doses were investigated:\n\nAdalimumab subcutaneous 160 mg at week 0, 80 mg at week 2, and 40 mg at week 4 (ADA160/80)\nInfliximab intravenous 5 mg/kg (INF5) at weeks 0, 2, and 6 then every 8 weeks\nInfliximab intravenous 10 mg/kg (INF10) at weeks 0, 2, and 6 then every 8 weeks\nFilgotinib oral 100 mg once daily (FIL100)\nFilgotinib oral 200 mg once daily (FIL200)\nGolimumab subcutaneous 200 mg at week 0 and 100 mg at week 2 (GOL200/100)\nOzanimod oral 0.23 mg once daily for 4 days, 0.46 mg once daily for 3 days, then 0.92 mg once daily (OZA0.92)\nTofacitinib oral 10 mg twice daily for 8 weeks (TOF10)\nUpadacitinib oral 45 mg once daily for 8 weeks (UPA45)\nUstekinumab intravenous 6 mg/kg at week 0 (UST6)\nVedolizumab intravenous 300 mg at weeks 0, 2, and 6 (VED300)\n\nThe reference treatment is placebo (PBO).\n\n\n\nEfficacy outcomes (i.e., clinical remission) data of induction bio-naïve populations\n \n  \n    studlab \n    treat1 \n    treat2 \n    event1 \n    n1 \n    event2 \n    n2 \n  \n \n\n  \n    ACT-1 \n    INF10 \n    INF5 \n    39 \n    122 \n    47 \n    121 \n  \n  \n    ACT-1 \n    INF10 \n    PBO \n    39 \n    122 \n    18 \n    121 \n  \n  \n    ACT-1 \n    INF5 \n    PBO \n    47 \n    121 \n    18 \n    121 \n  \n  \n    ACT-2 \n    INF10 \n    INF5 \n    33 \n    120 \n    41 \n    121 \n  \n  \n    ACT-2 \n    INF10 \n    PBO \n    33 \n    120 \n    7 \n    123 \n  \n  \n    ACT-2 \n    INF5 \n    PBO \n    41 \n    121 \n    7 \n    123 \n  \n  \n    GEMINI 1 \n    VED300 \n    PBO \n    30 \n    130 \n    5 \n    76 \n  \n  \n    Japic CTI-060298 \n    INF5 \n    PBO \n    21 \n    104 \n    11 \n    104 \n  \n  \n    Jiang 2015 \n    INF5 \n    PBO \n    22 \n    41 \n    9 \n    41 \n  \n  \n    M10-447 \n    ADA160/80 \n    PBO \n    9 \n    90 \n    11 \n    96 \n  \n  \n    NCT01551290 \n    INF5 \n    PBO \n    11 \n    50 \n    5 \n    49 \n  \n  \n    NCT02039505 \n    VED300 \n    PBO \n    22 \n    79 \n    6 \n    41 \n  \n  \n    OCTAVE 1 \n    TOF10 \n    PBO \n    56 \n    222 \n    9 \n    57 \n  \n  \n    OCTAVE 2 \n    TOF10 \n    PBO \n    43 \n    195 \n    4 \n    47 \n  \n  \n    PURSUIT-SC \n    GOL200/100 \n    PBO \n    45 \n    253 \n    16 \n    251 \n  \n  \n    SELECTION \n    FIL100 \n    FIL200 \n    47 \n    277 \n    60 \n    245 \n  \n  \n    SELECTION \n    FIL100 \n    PBO \n    47 \n    277 \n    17 \n    137 \n  \n  \n    SELECTION \n    FIL200 \n    PBO \n    60 \n    245 \n    17 \n    137 \n  \n  \n    TRUE NORTH \n    OZA0.92 \n    PBO \n    66 \n    299 \n    10 \n    151 \n  \n  \n    U-ACCOMPLISH \n    UPA45 \n    PBO \n    54 \n    166 \n    3 \n    81 \n  \n  \n    U-ACHIEVE Study 2 \n    UPA45 \n    PBO \n    41 \n    145 \n    4 \n    72 \n  \n  \n    ULTRA-1 \n    ADA160/80 \n    PBO \n    24 \n    130 \n    12 \n    130 \n  \n  \n    ULTRA-2 \n    ADA160/80 \n    PBO \n    32 \n    150 \n    16 \n    145 \n  \n  \n    UNIFI \n    UST6 \n    PBO \n    27 \n    147 \n    15 \n    151 \n  \n\n\n\n\n\nThe corresponding network is displayed below:\n\n\n\n\n\nEvidence network of 18 trials that contributed efficacy data for induction in bio-naive populations\n\n\n\n\nBelow, we conduct a random effects network meta-analysis of the reported study effects (expressed as odds ratio) and consider placebo (treat = \"PBO\") as the control treatment.\n\nNMA.uc <- netmeta(TE = TE, seTE = seTE, treat1 = treat1, treat2 = treat2,\n                  studlab = studlab, data = UlcerativeColitis, sm = \"OR\", \n                  ref = \"PBO\", common = FALSE, comb.random = TRUE)\nNMA.uc\n\nAll treatments except FIL100 and UST6 are significantly more efficacious than PBO at inducing clinical remission. We can now estimate the probabilities of each treatment being at each possible rank and the SUCRAs (Surface Under the Cumulative RAnking curve):\n\nsucra.uc <- rankogram(NMA.uc, nsim = 100, random = TRUE, common = FALSE, \n                      small.values = \"undesirable\")\n\n# Exctract the SUCRA values\nsucra.uc$ranking.random\n\n ADA160/80     FIL100     FIL200 GOL200/100      INF10       INF5    OZA0.92 \n0.27818182 0.19454545 0.41272727 0.62454545 0.61545455 0.76000000 0.73909091 \n       PBO      TOF10      UPA45       UST6     VED300 \n0.01454545 0.39000000 0.98181818 0.36181818 0.62727273 \n\n\nThese results indicate that 98.2% of the evaluated treatments are worse than UPA45."
  },
  {
    "objectID": "chapter_10.html#version-info",
    "href": "chapter_10.html#version-info",
    "title": "6  Systematic review and meta-analysis of Real-World Evidence",
    "section": "Version info",
    "text": "Version info\nThis chapter was rendered using the following version of R and its packages:\n\n\nR version 4.2.3 (2023-03-15 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19044)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=Dutch_Netherlands.utf8  LC_CTYPE=Dutch_Netherlands.utf8   \n[3] LC_MONETARY=Dutch_Netherlands.utf8 LC_NUMERIC=C                      \n[5] LC_TIME=Dutch_Netherlands.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] gemtc_1.0-1      coda_0.19-4      dmetar_0.0.9000  kableExtra_1.3.4\n[5] netmeta_2.8-2    meta_6.2-1       dplyr_1.1.1     \n\nloaded via a namespace (and not attached):\n [1] httr_1.4.6          magic_1.6-1         jsonlite_1.8.4     \n [4] viridisLite_0.4.2   splines_4.2.3       highr_0.10         \n [7] stats4_4.2.3        metafor_4.0-0       slam_0.1-50        \n[10] yaml_2.3.7          robustbase_0.95-1   ggrepel_0.9.3      \n[13] numDeriv_2016.8-1.1 pillar_1.9.0        lattice_0.21-8     \n[16] glue_1.6.2          digest_0.6.31       rvest_1.0.3        \n[19] minqa_1.2.5         colorspace_2.1-0    MuMIn_1.47.5       \n[22] plyr_1.8.8          htmltools_0.5.5     Matrix_1.5-4       \n[25] pkgconfig_2.0.3     mvtnorm_1.1-3       Rglpk_0.6-5        \n[28] scales_1.2.1        webshot_0.5.4       svglite_2.1.1      \n[31] rjags_4-14          metadat_1.2-0       lme4_1.1-32        \n[34] tibble_3.2.1        generics_0.1.3      ggplot2_3.4.2      \n[37] withr_2.5.0         nnet_7.3-19         cli_3.6.1          \n[40] magrittr_2.0.3      mclust_6.0.0        evaluate_0.21      \n[43] fansi_1.0.4         nlme_3.1-162        MASS_7.3-58.3      \n[46] truncnorm_1.0-9     forcats_1.0.0       xml2_1.3.3         \n[49] class_7.3-22        tools_4.2.3         lifecycle_1.0.3    \n[52] stringr_1.5.0       kernlab_0.9-32      munsell_0.5.0      \n[55] cluster_2.1.4       fpc_2.2-10          compiler_4.2.3     \n[58] systemfonts_1.0.4   rlang_1.1.0         grid_4.2.3         \n[61] nloptr_2.0.3        rstudioapi_0.14     CompQuadForm_1.4.3 \n[64] htmlwidgets_1.6.2   igraph_1.4.2        rmarkdown_2.21     \n[67] boot_1.3-28.1       codetools_0.2-19    gtable_0.3.3       \n[70] abind_1.4-5         flexmix_2.3-19      R6_2.5.1           \n[73] gridExtra_2.3       knitr_1.42          prabclus_2.3-2     \n[76] fastmap_1.1.1       utf8_1.2.3          mathjaxr_1.6-0     \n[79] poibin_1.5          modeltools_0.2-23   stringi_1.7.12     \n[82] parallel_4.2.3      Rcpp_1.0.10         vctrs_0.6.1        \n[85] DEoptimR_1.0-13     tidyselect_1.2.0    xfun_0.39          \n[88] diptest_0.76-0"
  },
  {
    "objectID": "chapter_10.html#references",
    "href": "chapter_10.html#references",
    "title": "6  Systematic review and meta-analysis of Real-World Evidence",
    "section": "References",
    "text": "References\n\n\n\n\nBaker, William L, Erica L Baker, and Craig I Coleman. 2009. “Pharmacologic Treatments for Chronic Obstructive Pulmonary Disease: A Mixed-Treatment Comparison Meta-Analysis.” Pharmacotherapy 29 (8): 891–905. https://doi.org/10.1592/phco.29.8.891.\n\n\nPanaccione, Remo, Eric B Collins, Gil Y Melmed, Severine Vermeire, Silvio Danese, Peter D R Higgins, Christina S Kwon, et al. 2023. “Efficacy and Safety of Advanced Therapies for Moderately to Severely Active Ulcerative Colitis at Induction and Maintenance: An Indirect Treatment Comparison Using Bayesian Network Meta-Analysis.” Crohn’s & Colitis 360 5 (2). https://doi.org/10.1093/crocol/otad009.\n\n\nSelvarajan, Sandhiya, Annuja Anandaradje, Santhosh Shivabasappa, Deepthy Melepurakkal Sadanandan, N. Sreekumaran Nair, and Melvin George. 2022. “Efficacy of Pharmacological Interventions in COVID-19: A Network Meta-Analysis.” British Journal of Clinical Pharmacology 88 (9): 4080–91. https://doi.org/10.1111/bcp.15338.\n\n\nSiemieniuk, Reed AC, Jessica J Bartoszko, Dena Zeraatkar, Elena Kum, Anila Qasim, Juan Pablo Dı́az Martinez, Ariel Izcovich, et al. 2020. “Drug Treatments for Covid-19: Living Systematic Review and Network Meta-Analysis.” BMJ, July, m2980. https://doi.org/10.1136/bmj.m2980."
  },
  {
    "objectID": "chapter_12.html#introduction",
    "href": "chapter_12.html#introduction",
    "title": "7  Dealing with irregular and informative visits",
    "section": "7.1 Introduction",
    "text": "7.1 Introduction\nWe first load the required packages\n\nlibrary(dplyr)\nlibrary(broom)\nlibrary(ggplot2)\nlibrary(mice)\n\nSubsequently, we load the relevant R scripts:\n\nsource(\"resources/chapter12_sim.r\")\nsource(\"resources/chapter12_fig_functions.r\")\nsource(\"resources/chapter12_mlmi.r\")"
  },
  {
    "objectID": "chapter_12.html#example-dataset",
    "href": "chapter_12.html#example-dataset",
    "title": "7  Dealing with irregular and informative visits",
    "section": "7.2 Example dataset",
    "text": "7.2 Example dataset\nBelow, we generate an example dataset that contains information on the treatment allocation x and three baseline covariates age, sex and edss (EDSS at treatment start). The discrete outcome y represents the Expanded Disability Status Scale (EDSS) score after time months of treatment exposure. Briefly, the EDSS is a semi-continuous measure that varies from 0 (no disability) to 10 (death).\n\nset.seed(9843626)\n\ndataset  &lt;- sim_data_EDSS(npatients = 500,\n                          ncenters = 10,\n                          follow_up = 12*5, # Total follow-up (number of months)\n                          sd_a_t = 0.5,   # DGM - Within-visit variation in EDSS scores\n                          baseline_EDSS = 1.3295,    # DGM - Mean baseline EDDS score\n                          sd_alpha_ij = 1.46,    # DGM - Between-subject variation in baseline EDSS\n                          sd_beta1_j = 0.20,    # DGM - Between-site variation in baseline EDSS\n                          mean_age = 42.41,\n                          sd_age = 10.53,\n                          min_age = 18,\n                          beta_age = 0.05, # DGM - prognostic effect of age\n                          beta_t = 0.014,  # DGM - prognostic effect of time\n                          beta_t2 = 0,    # DGM - prognostic effect of time squared\n                          delta_xt = 0, # DGM - interaction treatment time\n                          delta_xt2 = 0, # 0.0005    # DGM - interaction treatment time2\n                          p_female = 0.75, \n                          beta_female = -0.2 ,  ## DGM - prognostic effect of male sex\n                          delta_xf = 0,      ## DGM - interaction sex treatment       \n                          rho = 0.8,             # DGM - autocorrelation of between alpha_tij\n                          corFUN = corAR1,       # DGM - correlation structure of the latent EDSS scores\n                          tx_alloc_FUN = treatment_alloc_confounding_v2 ) ## or treatment_alloc_randomized\n\n\n\n\n\n\nDistribution of the EDSS score at each time point\n\n\n\n\nWe remove the outcome y according to the informative visit process that depends on the received treatment, gender, and age.\n\ndataset_visit &lt;- censor_visits_a5(dataset, seed = 12345) %&gt;% \n  dplyr::select(-y) %&gt;%\n  mutate(time_x = time*x)\n\nIn the censored data, a total of 17 out of 5000 patients have a visit at time=60."
  },
  {
    "objectID": "chapter_12.html#estimation-of-treatment-effect",
    "href": "chapter_12.html#estimation-of-treatment-effect",
    "title": "7  Dealing with irregular and informative visits",
    "section": "7.3 Estimation of treatment effect",
    "text": "7.3 Estimation of treatment effect\nWe will estimate the marginal treatment effect at time time=60.\n\n7.3.1 Original data\n\norigdat60 &lt;- dataset %&gt;% filter(time == 60)\n\n# Predict probability of treatment allocation\nfitps &lt;- glm(x ~ age + sex + edss, family = 'binomial', \n             data = origdat60)\n\n# Derive the propensity score\norigdat60 &lt;- origdat60 %&gt;% mutate(ipt = ifelse(x == 1, 1/predict(fitps, type = 'response'),\n                                               1/(1-predict(fitps, type = 'response'))))\n\n# Estimate \nfit_ref_m &lt;- tidy(lm(y ~ x, weight = ipt, data = origdat60), conf.int = TRUE) \n\n\n\n7.3.2 Doubly-weighted marginal treatment effect\n\nobsdat60 &lt;- dataset_visit %&gt;% mutate(visit = ifelse(is.na(y_obs),0,1)) %&gt;% filter(time == 60)\n\ngamma &lt;- glm(visit ~ x + sex + age + edss, family = 'binomial', data = obsdat60)$coef   \n\nobsdat60 &lt;- obsdat60 %&gt;% mutate(rho_i = 1/exp(gamma[\"(Intercept)\"] +\n                                                          gamma[\"x\"]*x +\n                                                          gamma[\"sex\"]*sex +\n                                                          gamma[\"age\"]*age))\n\n# Predict probability of treatment allocation\nfitps &lt;- glm(x ~ age + sex + edss, family='binomial', data = obsdat60)\n\n# Derive the propensity score\nobsdat60 &lt;- obsdat60 %&gt;% mutate(ipt = ifelse(x==1, 1/predict(fitps, type='response'),\n                                            1/(1-predict(fitps, type='response'))))\n\n\nfit_w &lt;- tidy(lm(y_obs ~ x, weights = ipt*rho_i, data = obsdat60), conf.int = TRUE)\n\n\n\n7.3.3 Multilevel multiple imputation\nWe adopt the imputation approach proposed by Debray et al. (2023). Briefly, we impute the entire vector of y_obs for all 61 potential visits and generate 10 imputed datasets. Note: mlmi currently does not support imputation of treatment-covariate interaction terms.\n\nimp &lt;- impute_y_mice_3l(dataset_visit, seed = 12345)\n\nWe can now estimate the treatment effect in each imputed dataset\n\n# Predict probability of treatment allocation\nfitps &lt;- glm(x ~ age + sex + edss, family='binomial', data = dataset_visit)\n  \n# Derive the propensity score\ndataset_visit &lt;- dataset_visit %&gt;% mutate(ipt = ifelse(x==1, 1/predict(fitps, type='response'),\n                                                       1/(1-predict(fitps, type='response'))))\n  \nQ &lt;- U &lt;- rep(NA, 10) # Error variances\n\nfor (i in seq(10)) {\n  dati &lt;- cbind(dataset_visit[,c(\"x\",\"ipt\",\"time\")], y_imp = imp[,i]) %&gt;% filter(time == 60)\n  \n  # Estimate \n  fit &lt;- tidy(lm(y_imp ~ x, weight = ipt, data = dati), conf.int = TRUE) \n  \n  Q[i] &lt;- fit %&gt;% filter(term == \"x\") %&gt;% pull(estimate)\n  U[i] &lt;- (fit %&gt;% filter(term == \"x\") %&gt;% pull(std.error))**2\n}\n\nfit_mlmi &lt;- pool.scalar(Q = Q, U = U)"
  },
  {
    "objectID": "chapter_12.html#reproduce-the-results-using-all-data-to-compute-the-marginal-effect-with-iiv-weighted",
    "href": "chapter_12.html#reproduce-the-results-using-all-data-to-compute-the-marginal-effect-with-iiv-weighted",
    "title": "7  Dealing with irregular and informative visits",
    "section": "7.4 Reproduce the results using all data to compute the marginal effect with IIV-weighted",
    "text": "7.4 Reproduce the results using all data to compute the marginal effect with IIV-weighted\n\n7.4.1 Doubly -weighted marginal treatment effect total\n\nobsdatall &lt;- dataset_visit %&gt;% mutate(visit = ifelse(is.na(y_obs),0,1))  \ngamma &lt;- glm(visit ~ x + sex + age + edss, family = 'binomial', data = obsdatall)$coef   \nobsdatall &lt;- obsdatall %&gt;% mutate(rho_i = 1/exp(gamma[\"(Intercept)\"] +\n                                                gamma[\"x\"]*x +\n                                                gamma[\"sex\"]*sex +\n                                                gamma[\"age\"]*age))\n# Predict probability of treatment allocation\nfitps &lt;- glm(x ~ age + sex + edss, family='binomial', data = obsdatall)\n# Derive the propensity score\nobsdatall &lt;- obsdatall %&gt;% mutate(ipt = ifelse(x==1, 1/predict(fitps, type='response'),\n                                             1/(1-predict(fitps, type='response'))))\nfit_w &lt;- tidy(lm(y_obs ~ x, weights = ipt*rho_i, data = obsdatall), conf.int = TRUE)"
  },
  {
    "objectID": "chapter_12.html#results",
    "href": "chapter_12.html#results",
    "title": "7  Dealing with irregular and informative visits",
    "section": "7.5 Results",
    "text": "7.5 Results"
  },
  {
    "objectID": "chapter_12.html#version-info",
    "href": "chapter_12.html#version-info",
    "title": "7  Dealing with irregular and informative visits",
    "section": "Version info",
    "text": "Version info\nThis chapter was rendered using the following version of R and its packages:\n\n\nR version 4.2.3 (2023-03-15 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=Dutch_Netherlands.utf8  LC_CTYPE=Dutch_Netherlands.utf8   \n[3] LC_MONETARY=Dutch_Netherlands.utf8 LC_NUMERIC=C                      \n[5] LC_TIME=Dutch_Netherlands.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] broom_1.0.4     mice_3.15.0     ggplot2_3.4.2   dplyr_1.1.1    \n[5] truncnorm_1.0-9 MASS_7.3-58.3   nlme_3.1-162   \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.10        RColorBrewer_1.1-3 pillar_1.9.0       compiler_4.2.3    \n [5] tools_4.2.3        digest_0.6.31      jsonlite_1.8.4     evaluate_0.21     \n [9] lifecycle_1.0.3    tibble_3.2.1       gtable_0.3.3       lattice_0.21-8    \n[13] pkgconfig_2.0.3    rlang_1.1.0        cli_3.6.1          rstudioapi_0.14   \n[17] yaml_2.3.7         xfun_0.39          fastmap_1.1.1      withr_2.5.0       \n[21] knitr_1.42         generics_0.1.3     vctrs_0.6.1        htmlwidgets_1.6.2 \n[25] grid_4.2.3         tidyselect_1.2.0   glue_1.6.2         R6_2.5.1          \n[29] fansi_1.0.4        rmarkdown_2.21     farver_2.1.1       tidyr_1.3.0       \n[33] purrr_1.0.1        magrittr_2.0.3     ellipsis_0.3.2     codetools_0.2-19  \n[37] backports_1.4.1    scales_1.2.1       htmltools_0.5.5    colorspace_2.1-0  \n[41] labeling_0.4.2     utf8_1.2.3         munsell_0.5.0"
  },
  {
    "objectID": "chapter_16.html#estimating-heterogeneous-treatment-effects-in-pairwise-meta-analysis",
    "href": "chapter_16.html#estimating-heterogeneous-treatment-effects-in-pairwise-meta-analysis",
    "title": "8  Prediction of individual treatment effect using data from multiple studies",
    "section": "8.1 Estimating heterogeneous treatment effects in pairwise meta-analysis",
    "text": "8.1 Estimating heterogeneous treatment effects in pairwise meta-analysis\n\n\n\nWe hereby provide code for estimating patient-level treatment effects for the case when we have patient-level data from multiple randomized trials.\n\n8.1.1 Example of a continuous outcome\n\n8.1.1.1 Setup\nWe start by simulating an artificial dataset using the R package bipd:\n\nlibrary(bipd)\nds <- generate_ipdma_example(type = \"continuous\")\n\nLet us have a look at the dataset:\n\nhead(ds)\n\n  studyid treat         z1         z2  y\n1       1     0 -1.4080333  0.4976585 11\n2       1     1 -0.2808028  0.7054476 10\n3       1     1  0.1434411 -0.6242589  8\n4       1     0 -0.5011206  1.0614987 11\n5       1     1 -0.6102570  0.1756299  9\n6       1     0 -0.7205885  1.9777567 11\n\n\nThe simulated dataset contains information on the following variables:\n\nthe trial indicator studyid\nthe treatment indicator treat, which takes the values 0 for control and 1 for active treatment\ntwo prognostic variables z1 and z2\nthe continuous outcome y\n\n\n\n\n\nTable 8.1:  The simulated dataset with a continuous outcome \n\n\n\n0(N=300)\n1(N=300)\nOverall(N=600)\n\n\n\n\nz1\n\n\n\n\n\nMean (SD)\n-0.0375 (0.952)\n0.0952 (0.986)\n0.0288 (0.971)\n\n\nMedian [Min, Max]\n-0.0861 [-2.88, 2.88]\n0.0943 [-2.99, 3.25]\n0.00957 [-2.99, 3.25]\n\n\nz2\n\n\n\n\n\nMean (SD)\n-0.0839 (1.01)\n-0.122 (1.07)\n-0.103 (1.04)\n\n\nMedian [Min, Max]\n-0.0329 [-2.96, 2.81]\n-0.145 [-3.11, 2.66]\n-0.0843 [-3.11, 2.81]\n\n\nstudyid\n\n\n\n\n\n1\n44 (14.7%)\n56 (18.7%)\n100 (16.7%)\n\n\n2\n48 (16.0%)\n52 (17.3%)\n100 (16.7%)\n\n\n3\n47 (15.7%)\n53 (17.7%)\n100 (16.7%)\n\n\n4\n59 (19.7%)\n41 (13.7%)\n100 (16.7%)\n\n\n5\n55 (18.3%)\n45 (15.0%)\n100 (16.7%)\n\n\n6\n47 (15.7%)\n53 (17.7%)\n100 (16.7%)\n\n\n\n\n\n\n\n\n\n8.1.1.2 Model fitting\nWe synthesize the evidence using a Bayesian random effects meta-analysis model. The model is given in Equation 16.7 of the book. First we need set up the data and create the model:\n\nipd <- with(ds, ipdma.model.onestage(y = y, study = studyid, treat = treat,\n                                     X = cbind(z1, z2), \n                                     response = \"normal\", \n                                     shrinkage = \"none\"), \n                                     type=\"random\")\n\nThe JAGS model can be accessed as follows:\n\nipd$model.JAGS\n\nfunction () \n{\n    for (i in 1:Np) {\n        y[i] ~ dnorm(mu[i], sigma)\n        mu[i] <- alpha[studyid[i]] + inprod(beta[], X[i, ]) + \n            (1 - equals(treat[i], 1)) * inprod(gamma[], X[i, \n                ]) + d[studyid[i], treat[i]]\n    }\n    sigma ~ dgamma(0.001, 0.001)\n    for (j in 1:Nstudies) {\n        d[j, 1] <- 0\n        d[j, 2] ~ dnorm(delta[2], tau)\n    }\n    sd ~ dnorm(0, 1)\n    T(0, )\n    tau <- pow(sd, -2)\n    delta[1] <- 0\n    delta[2] ~ dnorm(0, 0.001)\n    for (j in 1:Nstudies) {\n        alpha[j] ~ dnorm(0, 0.001)\n    }\n    for (k in 1:Ncovariate) {\n        beta[k] ~ dnorm(0, 0.001)\n    }\n    for (k in 1:Ncovariate) {\n        gamma[k] ~ dnorm(0, 0.001)\n    }\n}\n<environment: 0x0000022fbe8d6760>\n\n\nWe can fit the treatment effect model as follows:\n\nsamples <- ipd.run(ipd, n.chains = 2, n.iter = 20,\n                   pars.save = c(\"alpha\", \"beta\", \"delta\", \"sd\", \"gamma\"))\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 600\n   Unobserved stochastic nodes: 19\n   Total graph size: 6034\n\nInitializing model\n\n\nHere are the estimated model parameters:\n\nsummary(samples)\n\n\nIterations = 2001:2020\nThinning interval = 1 \nNumber of chains = 2 \nSample size per chain = 20 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n            Mean      SD Naive SE Time-series SE\nalpha[1] 10.9224 0.04098 0.006479       0.009113\nalpha[2]  8.0255 0.04179 0.006607       0.006480\nalpha[3] 10.5387 0.05196 0.008216       0.011734\nalpha[4]  9.6712 0.03866 0.006113       0.011441\nalpha[5] 12.8963 0.04898 0.007744       0.009898\nalpha[6] 15.7597 0.06275 0.009921       0.015179\nbeta[1]   0.2009 0.01911 0.003022       0.004567\nbeta[2]   0.3261 0.02122 0.003356       0.005746\ndelta[1]  0.0000 0.00000 0.000000       0.000000\ndelta[2] -2.7797 0.88557 0.140020       0.117590\ngamma[1] -0.5375 0.02670 0.004221       0.009076\ngamma[2]  0.6140 0.02409 0.003809       0.004835\nsd        1.7917 0.38948 0.061582       0.081172\n\n2. Quantiles for each variable:\n\n            2.5%     25%     50%     75%   97.5%\nalpha[1] 10.8510 10.8987 10.9274 10.9525 10.9942\nalpha[2]  7.9603  7.9938  8.0195  8.0531  8.1029\nalpha[3] 10.4438 10.5105 10.5449 10.5766 10.6315\nalpha[4]  9.6069  9.6431  9.6675  9.6957  9.7395\nalpha[5] 12.8111 12.8646 12.8933 12.9260 13.0080\nalpha[6] 15.6232 15.7180 15.7610 15.8010 15.8648\nbeta[1]   0.1632  0.1856  0.2051  0.2136  0.2336\nbeta[2]   0.2900  0.3083  0.3284  0.3421  0.3572\ndelta[1]  0.0000  0.0000  0.0000  0.0000  0.0000\ndelta[2] -3.9673 -3.3691 -2.9707 -2.2861 -1.0929\ngamma[1] -0.5778 -0.5567 -0.5395 -0.5165 -0.4902\ngamma[2]  0.5652  0.5958  0.6204  0.6287  0.6471\nsd        1.1539  1.5357  1.7541  2.0721  2.6058\n\n\n\n\n8.1.1.3 Prection\nWe can now predict the individualized treatment effect for a new patient with covariate values z1=1 and z2=0.5.\n\nround(treatment.effect(ipd, samples, newpatient = c(z1 = 1, z2 = 0.5)), 2)\n\n0.025   0.5 0.975 \n-4.36 -2.88 -1.63 \n\n\nWe can also predict treatment benefit for all patients in the sample, and look at the distribution of predicted benefit.\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\nds <- ds %>% mutate(benefit = NA)\n\nfor (i in seq(nrow(ds))) {\n  newpat <- as.matrix(ds[i, c(\"z1\", \"z2\")])\n  ds$benefit[i] <- treatment.effect(ipd, samples, newpatient = newpat)[\"0.5\"]\n}\n\nggplot(ds, aes(x = benefit)) + geom_histogram() + facet_wrap(~studyid) + \n  xlab(\"Predicted treatment benefit\")\n\n\n\n\nFigure 8.1: Distribution of predicted treatment benefit in each trial\n\n\n\n\n\n\n8.1.1.4 Penalization\nLet us repeat the analysis, but this time while penalizing the treatment-covariate coefficients using a Bayesian LASSO prior.\n\nipd <- with(ds, ipdma.model.onestage(y = y, study = studyid, \n                                     treat = treat,\n                                     X = cbind(z1, z2), \n                                     response = \"normal\", \n                                     shrinkage = \"laplace\"), \n            type = \"random\")\n\nsamples <- ipd.run(ipd, n.chains = 2, n.iter = 20, \n                   pars.save = c(\"alpha\", \"beta\", \"delta\", \"sd\", \"gamma\"))\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 600\n   Unobserved stochastic nodes: 20\n   Total graph size: 6039\n\nInitializing model\n\nround(treatment.effect(ipd, samples, newpatient = c(1,0.5)), 2)\n\n0.025   0.5 0.975 \n-4.53 -2.92 -1.20 \n\n\n\n\n\n8.1.2 Example of a binary outcome\n\n8.1.2.1 Setup\nWe now present the case of a binary outcome. We first generate a dataset as before, using the bipd package.\n\nds2 <- generate_ipdma_example(type = \"binary\")\nhead(ds2)\n\n  studyid treat         w1         w2 y\n1       1     1  0.2020881 -0.1301420 0\n2       1     0  0.5818186  0.6049542 0\n3       1     1  0.4475379  0.3141771 0\n4       1     1  0.2214633 -0.4015542 0\n5       1     1 -0.6675085 -0.1631319 0\n6       1     1  0.1248460  0.7159271 1\n\n\nThe simulated dataset contains information on the following variables:\n\nthe trial indicator studyid\nthe treatment indicator treat, which takes the values 0 for control and 1 for active treatment\ntwo prognostic variables w1 and w2\nthe binary outcome y\n\n\n\n\n\nTable 8.2:  The simulated dataset with a binary outcome \n\n\n\n0(N=307)\n1(N=293)\nOverall(N=600)\n\n\n\n\nw1\n\n\n\n\n\nMean (SD)\n-0.0767 (0.957)\n-0.105 (1.01)\n-0.0904 (0.980)\n\n\nMedian [Min, Max]\n-0.0642 [-3.04, 2.98]\n-0.0543 [-2.87, 2.61]\n-0.0568 [-3.04, 2.98]\n\n\nw2\n\n\n\n\n\nMean (SD)\n-0.0628 (0.989)\n0.0866 (1.01)\n0.0101 (1.00)\n\n\nMedian [Min, Max]\n-0.0371 [-2.95, 2.37]\n0.0741 [-2.53, 3.20]\n0.0212 [-2.95, 3.20]\n\n\nstudyid\n\n\n\n\n\n1\n48 (15.6%)\n52 (17.7%)\n100 (16.7%)\n\n\n2\n48 (15.6%)\n52 (17.7%)\n100 (16.7%)\n\n\n3\n64 (20.8%)\n36 (12.3%)\n100 (16.7%)\n\n\n4\n50 (16.3%)\n50 (17.1%)\n100 (16.7%)\n\n\n5\n48 (15.6%)\n52 (17.7%)\n100 (16.7%)\n\n\n6\n49 (16.0%)\n51 (17.4%)\n100 (16.7%)\n\n\n\n\n\n\n\n\n\n8.1.2.2 Model fitting\nWe use a Bayesian random effects model with binomial likelihood. This is similar to the model 16.7 of the book, but with a Binomial likelihood, i.e. \n\\[\ny_{ij}\\sim Binomial(\\pi_{ij}) \\\\\n\\] \\[\nlogit(\\pi_{ij})==a_j+\\delta_j t_{ij}+ \\sum_{l=1}^{L}\\beta_l x_{ij}+ \\sum_{l=1}^{L}\\gamma_l x_{ij} t_{ij}\n\\] The remaining of the model is as in the book. We can penalize the estimated parameters for effect modification (\\(\\gamma\\)’s), using a Bayesian LASSO. We can do this using again the bipd package:\n\nipd2 <- with(ds2, ipdma.model.onestage(y = y, study = studyid, treat = treat,\n                                       X = cbind(w1, w2), \n                                       response = \"binomial\", \n                                       shrinkage = \"laplace\"), \n             type=\"random\", hy.prior = list(\"dunif\", 0, 1))\n\nipd2$model.JAGS\n\nfunction () \n{\n    for (i in 1:Np) {\n        y[i] ~ dbern(p[i])\n        logit(p[i]) <- alpha[studyid[i]] + inprod(beta[], X[i, \n            ]) + (1 - equals(treat[i], 1)) * inprod(gamma[], \n            X[i, ]) + d[studyid[i], treat[i]]\n    }\n    for (j in 1:Nstudies) {\n        d[j, 1] <- 0\n        d[j, 2] ~ dnorm(delta[2], tau)\n    }\n    sd ~ dnorm(0, 1)\n    T(0, )\n    tau <- pow(sd, -2)\n    delta[1] <- 0\n    delta[2] ~ dnorm(0, 0.001)\n    for (j in 1:Nstudies) {\n        alpha[j] ~ dnorm(0, 0.001)\n    }\n    for (k in 1:Ncovariate) {\n        beta[k] ~ dnorm(0, 0.001)\n    }\n    tt <- lambda\n    lambda <- pow(lambda.inv, -1)\n    lambda.inv ~ dunif(0, 5)\n    for (k in 1:Ncovariate) {\n        gamma[k] ~ ddexp(0, tt)\n    }\n}\n<environment: 0x000001ae7cfd22a8>\n\nsamples <- ipd.run(ipd2, n.chains = 2, n.iter = 20, \n                   pars.save = c(\"alpha\", \"beta\", \"delta\", \"sd\", \"gamma\"))\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 600\n   Unobserved stochastic nodes: 19\n   Total graph size: 6637\n\nInitializing model\n\nsummary(samples)\n\n\nIterations = 2001:2020\nThinning interval = 1 \nNumber of chains = 2 \nSample size per chain = 20 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n              Mean      SD Naive SE Time-series SE\nalpha[1] -0.984274 0.26349  0.04166        0.04992\nalpha[2] -1.573505 0.25292  0.03999        0.08070\nalpha[3] -1.279716 0.22995  0.03636        0.05196\nalpha[4] -1.383137 0.34623  0.05474        0.09106\nalpha[5] -1.147265 0.29182  0.04614        0.08169\nalpha[6] -0.953363 0.34103  0.05392        0.08123\nbeta[1]  -0.055307 0.09131  0.01444        0.02319\nbeta[2]   0.040268 0.11429  0.01807        0.02955\ndelta[1]  0.000000 0.00000  0.00000        0.00000\ndelta[2]  0.079293 0.30877  0.04882        0.05915\ngamma[1]  0.003803 0.09427  0.01491        0.01881\ngamma[2]  0.109287 0.16848  0.02664        0.04611\nsd        0.579989 0.30646  0.04846        0.04275\n\n2. Quantiles for each variable:\n\n            2.5%       25%       50%       75%   97.5%\nalpha[1] -1.4166 -1.172468 -1.008088 -0.819705 -0.5199\nalpha[2] -2.1215 -1.686202 -1.566791 -1.426509 -1.1048\nalpha[3] -1.7120 -1.425665 -1.279602 -1.114348 -0.8884\nalpha[4] -1.9871 -1.631581 -1.424444 -1.116214 -0.8401\nalpha[5] -1.5708 -1.376267 -1.134091 -0.983412 -0.4789\nalpha[6] -1.4547 -1.235113 -0.955681 -0.730722 -0.3138\nbeta[1]  -0.1734 -0.122405 -0.072395  0.008591  0.1591\nbeta[2]  -0.1737 -0.040189  0.036344  0.139622  0.2060\ndelta[1]  0.0000  0.000000  0.000000  0.000000  0.0000\ndelta[2] -0.4758 -0.167638  0.127279  0.298100  0.5646\ngamma[1] -0.1448 -0.053786 -0.006688  0.062349  0.1915\ngamma[2] -0.1321 -0.004547  0.063059  0.264443  0.4597\nsd        0.2543  0.371257  0.489620  0.684747  1.4118\n\nround(treatment.effect(ipd2, samples, newpatient = c(w1= 1.6, w2 = 1.3)), 2)\n\n0.025   0.5 0.975 \n 0.71  1.23  2.26"
  },
  {
    "objectID": "chapter_16.html#estimating-heterogeous-treatment-effects-in-network-meta-analysis",
    "href": "chapter_16.html#estimating-heterogeous-treatment-effects-in-network-meta-analysis",
    "title": "8  Prediction of individual treatment effect using data from multiple studies",
    "section": "8.2 Estimating heterogeous treatment effects in network meta-analysis",
    "text": "8.2 Estimating heterogeous treatment effects in network meta-analysis\n\n8.2.1 Example of a continuous outcome\n\n8.2.1.1 Setup\nWe use again the bipd package to simulate a dataset:\n\nds3 <- generate_ipdnma_example(type = \"continuous\")\nhead(ds3)\n\n  studyid treat         z1         z2  y\n1       1     1 -0.1875585 -0.9506523 11\n2       1     2  0.9494434  0.3039170  8\n3       1     1 -0.7032586 -1.1831367 11\n4       1     1 -0.9996807 -0.1456256 11\n5       1     2 -0.3241298  1.2025923 10\n6       1     1 -1.0800505 -0.2934566 11\n\n\nLet us look into the data a bit in more detail:\n\n\n\n\nTable 8.3:  The simulated dataset with a continuous outcome \n\n\n\n1(N=383)\n2(N=338)\n3(N=279)\nOverall(N=1000)\n\n\n\n\nz1\n\n\n\n\n\n\nMean (SD)\n0.00154 (1.05)\n0.0587 (1.01)\n0.107 (0.978)\n0.0503 (1.02)\n\n\nMedian [Min, Max]\n0.0142 [-2.96, 3.81]\n0.0304 [-2.90, 2.69]\n0.0843 [-2.34, 3.06]\n0.0521 [-2.96, 3.81]\n\n\nz2\n\n\n\n\n\n\nMean (SD)\n-0.0495 (0.948)\n0.0298 (0.958)\n0.0101 (1.06)\n-0.00605 (0.983)\n\n\nMedian [Min, Max]\n-0.0108 [-2.51, 2.69]\n0.0788 [-3.37, 3.37]\n0.00833 [-2.66, 3.36]\n0.0170 [-3.37, 3.37]\n\n\nstudyid\n\n\n\n\n\n\n1\n49 (12.8%)\n51 (15.1%)\n0 (0%)\n100 (10.0%)\n\n\n2\n51 (13.3%)\n49 (14.5%)\n0 (0%)\n100 (10.0%)\n\n\n3\n64 (16.7%)\n36 (10.7%)\n0 (0%)\n100 (10.0%)\n\n\n4\n48 (12.5%)\n0 (0%)\n52 (18.6%)\n100 (10.0%)\n\n\n5\n58 (15.1%)\n0 (0%)\n42 (15.1%)\n100 (10.0%)\n\n\n6\n0 (0%)\n58 (17.2%)\n42 (15.1%)\n100 (10.0%)\n\n\n7\n0 (0%)\n47 (13.9%)\n53 (19.0%)\n100 (10.0%)\n\n\n8\n32 (8.4%)\n31 (9.2%)\n37 (13.3%)\n100 (10.0%)\n\n\n9\n36 (9.4%)\n35 (10.4%)\n29 (10.4%)\n100 (10.0%)\n\n\n10\n45 (11.7%)\n31 (9.2%)\n24 (8.6%)\n100 (10.0%)\n\n\n\n\n\n\n\n\n\n8.2.1.2 Model fitting\nWe will use the model shown in Equation 16.8 in the book. In addition, we will use Bayesian LASSO to penalize the treatment-covariate interactions.\n\nipd3 <- with(ds3, ipdnma.model.onestage(y = y, study = studyid, treat = treat, \n                                        X = cbind(z1, z2), \n                                        response = \"normal\", \n                                        shrinkage = \"laplace\", \n                                        type = \"random\"))\nipd3$model.JAGS\n\nfunction () \n{\n    for (i in 1:Np) {\n        y[i] ~ dnorm(mu[i], sigma)\n        mu[i] <- alpha[studyid[i]] + inprod(beta[], X[i, ]) + \n            inprod(gamma[treat[i], ], X[i, ]) + d[studyid[i], \n            treatment.arm[i]]\n    }\n    sigma ~ dgamma(0.001, 0.001)\n    for (i in 1:Nstudies) {\n        w[i, 1] <- 0\n        d[i, 1] <- 0\n        for (k in 2:na[i]) {\n            d[i, k] ~ dnorm(mdelta[i, k], taudelta[i, k])\n            mdelta[i, k] <- delta[t[i, k]] - delta[t[i, 1]] + \n                sw[i, k]\n            taudelta[i, k] <- tau * 2 * (k - 1)/k\n            w[i, k] <- d[i, k] - delta[t[i, k]] + delta[t[i, \n                1]]\n            sw[i, k] <- sum(w[i, 1:(k - 1)])/(k - 1)\n        }\n    }\n    sd ~ dnorm(0, 1)\n    T(0, )\n    tau <- pow(sd, -2)\n    delta[1] <- 0\n    for (k in 2:Ntreat) {\n        delta[k] ~ dnorm(0, 0.001)\n    }\n    for (j in 1:Nstudies) {\n        alpha[j] ~ dnorm(0, 0.001)\n    }\n    for (k in 1:Ncovariate) {\n        beta[k] ~ dnorm(0, 0.001)\n    }\n    lambda[1] <- 0\n    lambda.inv[1] <- 0\n    for (m in 2:Ntreat) {\n        tt[m] <- lambda[m] * sigma\n        lambda[m] <- pow(lambda.inv[m], -1)\n        lambda.inv[m] ~ dunif(0, 5)\n    }\n    for (k in 1:Ncovariate) {\n        gamma[1, k] <- 0\n        for (m in 2:Ntreat) {\n            gamma[m, k] ~ ddexp(0, tt[m])\n        }\n    }\n}\n<environment: 0x000001ae04f236d8>\n\nsamples <- ipd.run(ipd3, n.chains = 2, n.iter = 20, \n                   pars.save = c(\"alpha\", \"beta\", \"delta\", \"sd\", \"gamma\"))\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 1000\n   Unobserved stochastic nodes: 35\n   Total graph size: 10141\n\nInitializing model\n\nsummary(samples)\n\n\nIterations = 2001:2020\nThinning interval = 1 \nNumber of chains = 2 \nSample size per chain = 20 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n               Mean      SD Naive SE Time-series SE\nalpha[1]   11.07946 0.05925 0.009368       0.014099\nalpha[2]    7.98839 0.04988 0.007886       0.011136\nalpha[3]   10.47679 0.04441 0.007023       0.004715\nalpha[4]    9.63764 0.05690 0.008997       0.016508\nalpha[5]   12.81475 0.04564 0.007216       0.007271\nalpha[6]   13.15277 0.04660 0.007368       0.007021\nalpha[7]    7.40044 0.05706 0.009023       0.010973\nalpha[8]   11.10532 0.05730 0.009060       0.013021\nalpha[9]   10.22007 0.04767 0.007538       0.011724\nalpha[10]   9.21398 0.04151 0.006563       0.006630\nbeta[1]     0.19130 0.01696 0.002682       0.004093\nbeta[2]     0.32711 0.01436 0.002271       0.003595\ndelta[1]    0.00000 0.00000 0.000000       0.000000\ndelta[2]   -3.03507 0.04345 0.006869       0.012138\ndelta[3]   -1.19926 0.05127 0.008106       0.015666\ngamma[1,1]  0.00000 0.00000 0.000000       0.000000\ngamma[2,1] -0.57132 0.02355 0.003723       0.008548\ngamma[3,1] -0.32373 0.02989 0.004727       0.008259\ngamma[1,2]  0.00000 0.00000 0.000000       0.000000\ngamma[2,2]  0.54261 0.02413 0.003815       0.004070\ngamma[3,2]  0.40604 0.02228 0.003523       0.005078\nsd          0.08247 0.03305 0.005225       0.003009\n\n2. Quantiles for each variable:\n\n               2.5%      25%      50%     75%   97.5%\nalpha[1]   10.96200 11.03785 11.07762 11.1232 11.1862\nalpha[2]    7.90994  7.95417  7.98816  8.0204  8.0697\nalpha[3]   10.38926 10.44718 10.47657 10.5131 10.5475\nalpha[4]    9.53259  9.59600  9.64600  9.6708  9.7473\nalpha[5]   12.73593 12.79370 12.81830 12.8292 12.9082\nalpha[6]   13.07713 13.12068 13.14804 13.1784 13.2309\nalpha[7]    7.31728  7.35745  7.39633  7.4410  7.5229\nalpha[8]   10.99883 11.07209 11.10682 11.1528 11.2015\nalpha[9]   10.13534 10.18977 10.21204 10.2398 10.3198\nalpha[10]   9.14199  9.18543  9.21731  9.2387  9.2953\nbeta[1]     0.16488  0.18180  0.18854  0.2041  0.2253\nbeta[2]     0.29995  0.31656  0.32369  0.3377  0.3522\ndelta[1]    0.00000  0.00000  0.00000  0.0000  0.0000\ndelta[2]   -3.11695 -3.06356 -3.03608 -3.0101 -2.9677\ndelta[3]   -1.28659 -1.23650 -1.20394 -1.1719 -1.0880\ngamma[1,1]  0.00000  0.00000  0.00000  0.0000  0.0000\ngamma[2,1] -0.61115 -0.58786 -0.57420 -0.5512 -0.5309\ngamma[3,1] -0.37790 -0.34807 -0.32233 -0.2990 -0.2773\ngamma[1,2]  0.00000  0.00000  0.00000  0.0000  0.0000\ngamma[2,2]  0.50675  0.52405  0.53893  0.5605  0.5854\ngamma[3,2]  0.37139  0.38803  0.40706  0.4216  0.4425\nsd          0.03021  0.05601  0.08447  0.1089  0.1307\n\n\nAs before, we can use the treatment.effect() function of bipd to estimate relative effects for new patients.\n\ntreatment.effect(ipd3, samples, newpatient= c(1,2))\n\n$`treatment 2`\n    0.025       0.5     0.975 \n-2.574488 -2.470314 -2.349447 \n\n$`treatment 3`\n     0.025        0.5      0.975 \n-0.7880932 -0.6744730 -0.5415160 \n\n\nThis gives us the relative effects for all treatments versus the reference. To obtain relative effects between active treatments we need some more coding:\n\nsamples.all=data.frame(rbind(samples[[1]], samples[[2]]))\nnewpatient= c(1,2)\nnewpatient <- (newpatient - ipd3$scale_mean)/ipd3$scale_sd\n\nmedian(\n  samples.all$delta.2.+samples.all$gamma.2.1.*\n    newpatient[1]+samples.all$gamma.2.2.*newpatient[2]\n-\n  (samples.all$delta.3.+samples.all$gamma.3.1.*newpatient[1]+\n     samples.all$gamma.3.2.*newpatient[2])\n)\n\n[1] -1.775043\n\nquantile(samples.all$delta.2.+samples.all$gamma.2.1.*\n           newpatient[1]+samples.all$gamma.2.2.*newpatient[2]\n         -(samples.all$delta.3.+samples.all$gamma.3.1.*newpatient[1]+\n             samples.all$gamma.3.2.*newpatient[2])\n         , probs = 0.025)\n\n     2.5% \n-1.921441 \n\nquantile(samples.all$delta.2.+samples.all$gamma.2.1.*\n           newpatient[1]+samples.all$gamma.2.2.*newpatient[2]\n         -(samples.all$delta.3.+samples.all$gamma.3.1.*newpatient[1]+\n             samples.all$gamma.3.2.*newpatient[2])\n         , probs = 0.975)\n\n    97.5% \n-1.677147 \n\n\n\n\n\n8.2.2 Modeling patient-level relative effects using randomized and observational evidence for a network of treatments\nWe will now follow Chapter 16.3.5 from the book. In this analysis we will not use penalization, and we will assume fixed effects. For an example with penalization and random effects, see part 2 of this vignettte.\n\n8.2.2.1 Setup\nWe generate a very simple dataset of three studies comparing three treatments. We will assume 2 RCTs and 1 non-randomized trial:\n\nds4 <- generate_ipdnma_example(type = \"continuous\")\nds4 <- ds4 %>% filter(studyid %in% c(1,4,10)) %>%\n  mutate(studyid = factor(studyid) %>%\n           recode_factor(\n             \"1\" = \"1\",\n             \"4\" = \"2\",\n             \"10\" = \"3\"),\n         design = ifelse(studyid == \"3\", \"nrs\", \"rct\"))\n\nThe sample size is as follows:\n\n\n          \n           s1 s2 s3\n  treat A: 51 50 36\n  treat B: 49  0 35\n  treat C:  0 50 29\n\n\n\n\n8.2.2.2 Model fitting\nWe will use the design-adjusted model, equation 16.9 in the book. We will fit a two-stage fixed effects meta-analysis and we will use a variance inflation factor. The code below is used to specify the analysis of each individual study. Briefly, in each study we adjust the treatment effect for the prognostic factors z1 and z2, as well as their interaction with treat.\n\nlibrary(rjags)\nfirst.stage <- \"\nmodel{\n\nfor (i in 1:N){\n    y[i] ~ dnorm(mu[i], tau)  \n    mu[i] <- a + inprod(b[], X[i,]) + inprod(c[,treat[i]], X[i,]) + d[treat[i]] \n}\nsigma ~ dunif(0, 5)\ntau <- pow(sigma, -2)\n\na ~ dnorm(0, 0.001)\n\nfor(k in 1:Ncovariate){\n    b[k] ~ dnorm(0,0.001)\n}\n\nfor(k in 1:Ncovariate){\n    c[k,1] <- 0\n}\n\ntauGamma <- pow(sdGamma,-1)\nsdGamma ~ dunif(0, 5)\n\nfor(k in 1:Ncovariate){\n    for(t in 2:Ntreat){\n        c[k,t] ~ ddexp(0, tauGamma)\n    }\n}\n\nd[1] <- 0\nfor(t in 2:Ntreat){\n    d[t] ~ dnorm(0, 0.001)\n}\n}\"\n\nSubsequently, we estimate the relative treatment effects in the first (randomized) study comparing treatments A and B:\n\n\n\n\nmodel1.spec <- textConnection(first.stage) \ndata1 <- with(ds4 %>% filter(studyid == 1), \n              list(y = y,\n                   N = length(y), \n                   X = cbind(z1,z2),  \n                   treat = treat,\n                   Ncovariate = 2, \n                   Ntreat = 2))\njags.m <- jags.model(model1.spec, data = data1, n.chains = 2, n.adapt = 500,\n                     quiet =  TRUE)\nparams <- c(\"d\", \"c\") \nsamps4.1 <- coda.samples(jags.m, params, n.iter = 50)\nsamps.all.s1 <- data.frame(as.matrix(samps4.1))\n\nsamps.all.s1 <- samps.all.s1[, c(\"c.1.2.\", \"c.2.2.\", \"d.2.\")]\ndelta.1 <- colMeans(samps.all.s1)\ncov.1 <- var(samps.all.s1)\n\n\n\n\nWe repeat the analysis for the second (randomized) study comparing treatments A and C:\n\nmodel1.spec <- textConnection(first.stage) \ndata2 <- with(ds4 %>% filter(studyid == 2), \n              list(y = y,\n                   N = length(y), \n                   X = cbind(z1,z2),  \n                   treat = ifelse(treat == 3, 2, treat),\n                   Ncovariate = 2, \n                   Ntreat = 2))\njags.m <- jags.model(model1.spec, data = data2, n.chains = 2, n.adapt = 100,\n                     quiet =  TRUE)\nparams <- c(\"d\", \"c\") \nsamps4.2 <- coda.samples(jags.m, params, n.iter = 50)\nsamps.all.s2 <- data.frame(as.matrix(samps4.2))\nsamps.all.s2 <- samps.all.s2[, c(\"c.1.2.\", \"c.2.2.\", \"d.2.\")]\ndelta.2 <- colMeans(samps.all.s2)\ncov.2 <- var(samps.all.s2)\n\n\n\n\nFinally, we analyze the third (non-randomized) study comparing treatments A, B, and C:\n\nmodel1.spec <- textConnection(first.stage) \ndata3 <- with(ds4 %>% filter(studyid == 3), \n              list(y = y,\n                   N = length(y), \n                   X = cbind(z1,z2),  \n                   treat = treat,\n                   Ncovariate = 2, \n                   Ntreat = 3))\njags.m <- jags.model(model1.spec, data = data3, n.chains = 2, n.adapt = 100,\n                     quiet = TRUE)\nparams <- c(\"d\", \"c\") \nsamps4.3 <- coda.samples(jags.m, params, n.iter = 50)\nsamps.all.s3 <- data.frame(as.matrix(samps4.3))\n\nsamps.all.s3 <- samps.all.s3[, c(\"c.1.2.\", \"c.2.2.\", \"d.2.\", \"c.1.3.\", \n                                 \"c.2.3.\", \"d.3.\")]\ndelta.3 <- colMeans(samps.all.s3)\ncov.3 <- var(samps.all.s3)\n\n\n\n\nThe corresponding treatment effect estimates are depicted below:\n\n\n\n\nTable 8.4:  Treatment effect estimates. \n \n  \n    study \n    B versus A \n    C versus A \n  \n \n\n  \n    study 1 \n    -2.887 (SE =  0.048 ) \n     \n  \n  \n    study 2 \n     \n    -1.105 (SE =  0.060 ) \n  \n  \n    study 3 \n    -2.991 (SE =  0.068 ) \n    -1.116 (SE =  0.070 ) \n  \n\n\n\n\n\n\nWe can now fit the second stage of the network meta-analysis. The corresponding JAGS model is specified below:\n\nsecond.stage <-\n\"model{\n  \n  #likelihood\n  y1 ~ dmnorm(Mu1, Omega1)\n  y2 ~ dmnorm(Mu2, Omega2)\n  y3 ~ dmnorm(Mu3, Omega3*W)\n\n  \n  Omega1 <- inverse(cov.1)\n  Omega2 <- inverse(cov.2)\n  Omega3 <- inverse(cov.3)\n\n  Mu1 <- c(gamma[,1], delta[2])\n  Mu2 <- c(gamma[,2], delta[3])  \n  Mu3 <- c(gamma[,1], delta[2],gamma[,2], delta[3])\n  \n  #parameters\n  for(i in 1:2){\n    gamma[i,1] ~ dnorm(0, 0.001)\n    gamma[i,2] ~ dnorm(0, 0.001)\n  }\n  \n  delta[1] <- 0\n  delta[2] ~ dnorm(0, 0.001)\n  delta[3] ~ dnorm(0, 0.001)\n  \n}\n\"\n\nWe can fit as follows:\n\nmodel1.spec <- textConnection(second.stage) \ndata3 <- list(y1 = delta.1, y2 = delta.2, y3 = delta.3, \n              cov.1 = cov.1, cov.2 = cov.2, cov.3 = cov.3, W = 0.5)\n\njags.m <- jags.model(model1.spec, data = data3, n.chains = 2, n.adapt = 50,\n                     quiet = TRUE)\nparams <- c(\"delta\", \"gamma\") \nsamps4.3 <- coda.samples(jags.m, params, n.iter = 50)\n\n\nsummary(samps4.3)\n\n\nIterations = 1:50\nThinning interval = 1 \nNumber of chains = 2 \nSample size per chain = 50 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n              Mean      SD Naive SE Time-series SE\ndelta[1]    0.0000 0.00000 0.000000       0.000000\ndelta[2]   -2.8822 0.04606 0.004606       0.005233\ndelta[3]   -1.0774 0.04728 0.004728       0.004745\ngamma[1,1] -0.8313 0.04143 0.004143       0.004752\ngamma[2,1]  0.8266 0.06591 0.006591       0.006609\ngamma[1,2] -0.5303 0.04990 0.004990       0.005007\ngamma[2,2]  0.3335 0.04220 0.004220       0.004929\n\n2. Quantiles for each variable:\n\n              2.5%     25%     50%     75%   97.5%\ndelta[1]    0.0000  0.0000  0.0000  0.0000  0.0000\ndelta[2]   -2.9681 -2.9167 -2.8800 -2.8571 -2.8068\ndelta[3]   -1.1638 -1.1073 -1.0814 -1.0508 -0.9773\ngamma[1,1] -0.9085 -0.8580 -0.8346 -0.8013 -0.7461\ngamma[2,1]  0.7126  0.7981  0.8309  0.8640  0.9095\ngamma[1,2] -0.6276 -0.5685 -0.5357 -0.4868 -0.4366\ngamma[2,2]  0.2688  0.3047  0.3293  0.3599  0.4234\n\n# calculate  treatment effects\nsamples.all=data.frame(rbind(samps4.3[[1]], samps4.3[[2]]))\nnewpatient= c(1,2)\n\nmedian(\n  samples.all$delta.2.+samples.all$gamma.1.1.*newpatient[1]+\n    samples.all$gamma.2.1.*newpatient[2]\n)\n\n[1] -2.056707\n\nquantile(samples.all$delta.2.+samples.all$gamma.1.1.*newpatient[1]+\n           samples.all$gamma.2.1.*newpatient[2]\n         , probs = 0.025)\n\n     2.5% \n-2.304999 \n\nquantile(samples.all$delta.2.+samples.all$gamma.1.1.*newpatient[1]+\n           samples.all$gamma.2.1.*newpatient[2]\n         , probs = 0.975)\n\n    97.5% \n-1.840682"
  },
  {
    "objectID": "chapter_16.html#version-info",
    "href": "chapter_16.html#version-info",
    "title": "8  Prediction of individual treatment effect using data from multiple studies",
    "section": "Version info",
    "text": "Version info\nThis chapter was rendered using the following version of R and its packages:\n\n\nR version 4.2.3 (2023-03-15 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=Dutch_Netherlands.utf8  LC_CTYPE=Dutch_Netherlands.utf8   \n[3] LC_MONETARY=Dutch_Netherlands.utf8 LC_NUMERIC=C                      \n[5] LC_TIME=Dutch_Netherlands.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] kableExtra_1.3.4 rjags_4-14       coda_0.19-4      ggplot2_3.4.2   \n[5] dplyr_1.1.2      table1_1.4.3     tableone_0.13.2  bipd_0.3        \n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.0  xfun_0.39         mitools_2.4       splines_4.2.3    \n [5] lattice_0.21-8    colorspace_2.1-0  vctrs_0.6.2       generics_0.1.3   \n [9] viridisLite_0.4.2 htmltools_0.5.5   yaml_2.3.7        utf8_1.2.3       \n[13] survival_3.5-5    rlang_1.1.1       pillar_1.9.0      glue_1.6.2       \n[17] withr_2.5.0       DBI_1.1.3         lifecycle_1.0.3   stringr_1.5.0    \n[21] munsell_0.5.0     gtable_0.3.3      rvest_1.0.3       htmlwidgets_1.6.2\n[25] mvtnorm_1.1-3     codetools_0.2-19  evaluate_0.21     knitr_1.43       \n[29] fastmap_1.1.1     fansi_1.0.4       highr_0.10        scales_1.2.1     \n[33] webshot_0.5.4     jsonlite_1.8.4    systemfonts_1.0.4 digest_0.6.31    \n[37] stringi_1.7.12    survey_4.2-1      grid_4.2.3        cli_3.6.1        \n[41] tools_4.2.3       magrittr_2.0.3    tibble_3.2.1      Formula_1.2-5    \n[45] pkgconfig_2.0.3   Matrix_1.5-4.1    xml2_1.3.4        rmarkdown_2.22   \n[49] svglite_2.1.1     httr_1.4.6        rstudioapi_0.14   R6_2.5.1         \n[53] compiler_4.2.3"
  },
  {
    "objectID": "chapter_16.html#references",
    "href": "chapter_16.html#references",
    "title": "8  Prediction of individual treatment effect using data from multiple studies",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "chapter_12.html#references",
    "href": "chapter_12.html#references",
    "title": "7  Dealing with irregular and informative visits",
    "section": "References",
    "text": "References\n\n\n\n\nDebray, Thomas PA, Gabrielle Simoneau, Massimiliano Copetti, Robert W Platt, Changyu Shen, Fabio Pellegrini, and Carl de Moor. 2023. “Methods for Comparative Effectiveness Based on Time to Confirmed Disability Progression with Irregular Observations in Multiple Sclerosis.” Statistical Methods in Medical Research, June, 096228022311720. https://doi.org/10.1177/09622802231172032."
  }
]